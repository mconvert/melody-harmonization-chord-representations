{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, GRU, concatenate\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Custom library for the project\n",
    "import sys\n",
    "sys.path.insert(0, '../../../src')\n",
    "import harmoutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = harmoutil.load_pickled_data(\"../../../data/refined_data.pkl\") # lists of (chord label, melody seqs) by sections\n",
    "augmented_data = harmoutil.transpose_and_augment_data(raw_data)\n",
    "data = [harmoutil.to_sevenths(section) for section in augmented_data]\n",
    "data = [harmoutil.melody_to_octave_range(section) for section in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique notes (13): [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Unique chords (193): ['A', 'A+', 'A+7', 'A+j7', 'A-', 'A-6', 'A-7', 'A-j7', 'A6', 'A7', 'Ab', 'Ab+', 'Ab+7', 'Ab+j7', 'Ab-', 'Ab-6', 'Ab-7', 'Ab-j7', 'Ab6', 'Ab7', 'Abj7', 'Abm7b5', 'Abo', 'Abo7', 'Absus', 'Absus7', 'Aj7', 'Am7b5', 'Ao', 'Ao7', 'Asus', 'Asus7', 'B', 'B+', 'B+7', 'B+j7', 'B-', 'B-6', 'B-7', 'B-j7', 'B6', 'B7', 'Bb', 'Bb+', 'Bb+7', 'Bb+j7', 'Bb-', 'Bb-6', 'Bb-7', 'Bb-j7', 'Bb6', 'Bb7', 'Bbj7', 'Bbm7b5', 'Bbo', 'Bbo7', 'Bbsus', 'Bbsus7', 'Bj7', 'Bm7b5', 'Bo', 'Bo7', 'Bsus', 'Bsus7', 'C', 'C+', 'C+7', 'C+j7', 'C-', 'C-6', 'C-7', 'C-j7', 'C6', 'C7', 'Cj7', 'Cm7b5', 'Co', 'Co7', 'Csus', 'Csus7', 'D', 'D+', 'D+7', 'D+j7', 'D-', 'D-6', 'D-7', 'D-j7', 'D6', 'D7', 'Db', 'Db+', 'Db+7', 'Db+j7', 'Db-', 'Db-6', 'Db-7', 'Db-j7', 'Db6', 'Db7', 'Dbj7', 'Dbm7b5', 'Dbo', 'Dbo7', 'Dbsus', 'Dbsus7', 'Dj7', 'Dm7b5', 'Do', 'Do7', 'Dsus', 'Dsus7', 'E', 'E+', 'E+7', 'E+j7', 'E-', 'E-6', 'E-7', 'E-j7', 'E6', 'E7', 'Eb', 'Eb+', 'Eb+7', 'Eb+j7', 'Eb-', 'Eb-6', 'Eb-7', 'Eb-j7', 'Eb6', 'Eb7', 'Ebj7', 'Ebm7b5', 'Ebo', 'Ebo7', 'Ebsus', 'Ebsus7', 'Ej7', 'Em7b5', 'Eo', 'Eo7', 'Esus', 'Esus7', 'F', 'F+', 'F+7', 'F+j7', 'F-', 'F-6', 'F-7', 'F-j7', 'F6', 'F7', 'Fj7', 'Fm7b5', 'Fo', 'Fo7', 'Fsus', 'Fsus7', 'G', 'G+', 'G+7', 'G+j7', 'G-', 'G-6', 'G-7', 'G-j7', 'G6', 'G7', 'Gb', 'Gb+', 'Gb+7', 'Gb+j7', 'Gb-', 'Gb-6', 'Gb-7', 'Gb-j7', 'Gb6', 'Gb7', 'Gbj7', 'Gbm7b5', 'Gbo', 'Gbo7', 'Gbsus', 'Gbsus7', 'Gj7', 'Gm7b5', 'Go', 'Go7', 'Gsus', 'Gsus7', 'NC']\n"
     ]
    }
   ],
   "source": [
    "# Isolate meaningful data\n",
    "def get_notes_by_chord(beats):\n",
    "    return [note for beat in beats for note in beat]\n",
    "\n",
    "def get_chords_by_section(section):\n",
    "    return [chord_info[0] for chord_info in section]\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in data]\n",
    "chords = [chord_info[0] for section in data for chord_info in section]\n",
    "unique_chords = sorted(list(set(chords)))\n",
    "\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "notes = [note for chord_notes in notes_by_chords for note in chord_notes]\n",
    "unique_notes = sorted(list(set(notes)))\n",
    "\n",
    "print(\"Unique notes ({}): {}\".format(len(unique_notes), unique_notes))\n",
    "print(\"Unique chords ({}): {}\".format(len(unique_chords), unique_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody note to integer mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, '<pad>': 13, -1: 12}\n",
      "\n",
      "Integer to melody note mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: -1, 13: '<pad>'}\n",
      "\n",
      "Chord label to integer mapping:\n",
      " {'Db-': 94, 'C': 64, 'D-6': 85, 'B7': 41, 'E6': 120, 'Bo7': 61, 'Gbsus': 184, 'B-': 36, 'Dbo': 102, 'Fj7': 154, 'A7': 9, 'Ab+7': 12, 'Bb': 42, 'C-6': 69, 'Absus': 24, 'C+7': 66, 'Bbj7': 52, 'Ab6': 18, 'D+7': 82, 'D+': 81, 'Bb7': 51, 'Bo': 60, 'Cj7': 74, 'Bb+7': 44, 'Gb-': 174, 'Ab-6': 15, 'Ebo7': 135, 'A-j7': 7, 'F+': 145, 'Db6': 98, 'G7': 169, 'A+7': 2, 'Dsus': 110, 'A': 0, 'Db+7': 92, 'Gb-6': 175, 'Eb-j7': 129, 'Gb6': 178, 'F': 144, 'F-6': 149, 'G+7': 162, 'Ab-7': 16, 'Ab+': 11, 'Bj7': 58, 'Bsus': 62, 'Bb+': 43, 'Bb-7': 48, 'Do7': 109, 'D6': 88, 'G-6': 165, 'Cm7b5': 75, 'D-j7': 87, 'C+j7': 67, 'Dbj7': 100, 'C+': 65, 'Gbj7': 180, 'Dbsus7': 105, 'Abo7': 23, 'Gb': 170, 'C-': 68, 'Db': 90, 'F-': 148, 'Bb-j7': 49, 'Ebj7': 132, 'E-j7': 119, 'Gm7b5': 187, 'G': 160, 'Ab-j7': 17, 'Esus7': 143, 'Db+j7': 93, 'Eb+7': 124, 'Eb-7': 128, 'Dbm7b5': 101, 'Dj7': 106, 'C6': 72, 'B-6': 37, 'Eb7': 131, 'B': 32, 'F+j7': 147, 'Gsus7': 191, 'Gbo': 182, 'Bb6': 50, 'Fo7': 157, 'G-7': 166, 'Ebsus': 136, 'F-7': 150, 'Ao': 28, 'Asus': 30, 'Ao7': 29, 'Db+': 91, 'E-6': 117, 'Bb+j7': 45, 'Do': 108, 'Csus': 78, 'Bbm7b5': 53, 'G-j7': 167, 'Eb+': 123, 'Co': 76, 'Eb-6': 127, 'F6': 152, 'Dsus7': 111, 'Dm7b5': 107, 'Fsus7': 159, 'Db7': 99, 'E-7': 118, 'B+7': 34, 'Bsus7': 63, 'Go': 188, 'E+7': 114, 'G+j7': 163, 'Am7b5': 27, 'Abj7': 20, 'A+j7': 3, 'Eb': 122, 'Gb+': 171, 'B-7': 38, 'D7': 89, 'E+': 113, 'Gsus': 190, 'Go7': 189, 'Abo': 22, 'G6': 168, 'Fm7b5': 155, 'Eb+j7': 125, 'G-': 164, 'Bbsus7': 57, 'A+': 1, 'B-j7': 39, 'E': 112, 'G+': 161, 'Bbsus': 56, 'Gb-j7': 177, 'F-j7': 151, 'Eo7': 141, 'Gbo7': 183, 'Co7': 77, 'A-7': 6, 'Eb-': 126, 'Bbo': 54, 'Gbsus7': 185, 'Em7b5': 139, 'F7': 153, 'C7': 73, 'C-j7': 71, 'Ebsus7': 137, 'Abm7b5': 21, 'Gb+j7': 173, 'Absus7': 25, '<bos>': 193, 'Gb+7': 172, 'Ab7': 19, 'E-': 116, 'Gj7': 186, 'Ab': 10, 'Gb7': 179, 'Ab-': 14, 'Fsus': 158, 'D': 80, 'Gb-7': 176, 'D-7': 86, 'A-': 4, 'Ebm7b5': 133, 'Bb-': 46, 'Dbsus': 104, 'A-6': 5, 'NC': 192, 'Bbo7': 55, 'E+j7': 115, 'E7': 121, 'Fo': 156, 'Csus7': 79, 'Eb6': 130, 'Gbm7b5': 181, 'Db-7': 96, 'Dbo7': 103, 'B+j7': 35, 'Eo': 140, 'B6': 40, 'Bm7b5': 59, 'Db-j7': 97, 'Bb-6': 47, 'Ab+j7': 13, 'Esus': 142, 'D+j7': 83, 'Ej7': 138, 'Ebo': 134, 'Db-6': 95, 'C-7': 70, 'B+': 33, 'A6': 8, 'Asus7': 31, 'F+7': 146, 'D-': 84, 'Aj7': 26}\n",
      "\n",
      "Integer to chord label mapping:\n",
      " {0: 'A', 1: 'A+', 2: 'A+7', 3: 'A+j7', 4: 'A-', 5: 'A-6', 6: 'A-7', 7: 'A-j7', 8: 'A6', 9: 'A7', 10: 'Ab', 11: 'Ab+', 12: 'Ab+7', 13: 'Ab+j7', 14: 'Ab-', 15: 'Ab-6', 16: 'Ab-7', 17: 'Ab-j7', 18: 'Ab6', 19: 'Ab7', 20: 'Abj7', 21: 'Abm7b5', 22: 'Abo', 23: 'Abo7', 24: 'Absus', 25: 'Absus7', 26: 'Aj7', 27: 'Am7b5', 28: 'Ao', 29: 'Ao7', 30: 'Asus', 31: 'Asus7', 32: 'B', 33: 'B+', 34: 'B+7', 35: 'B+j7', 36: 'B-', 37: 'B-6', 38: 'B-7', 39: 'B-j7', 40: 'B6', 41: 'B7', 42: 'Bb', 43: 'Bb+', 44: 'Bb+7', 45: 'Bb+j7', 46: 'Bb-', 47: 'Bb-6', 48: 'Bb-7', 49: 'Bb-j7', 50: 'Bb6', 51: 'Bb7', 52: 'Bbj7', 53: 'Bbm7b5', 54: 'Bbo', 55: 'Bbo7', 56: 'Bbsus', 57: 'Bbsus7', 58: 'Bj7', 59: 'Bm7b5', 60: 'Bo', 61: 'Bo7', 62: 'Bsus', 63: 'Bsus7', 64: 'C', 65: 'C+', 66: 'C+7', 67: 'C+j7', 68: 'C-', 69: 'C-6', 70: 'C-7', 71: 'C-j7', 72: 'C6', 73: 'C7', 74: 'Cj7', 75: 'Cm7b5', 76: 'Co', 77: 'Co7', 78: 'Csus', 79: 'Csus7', 80: 'D', 81: 'D+', 82: 'D+7', 83: 'D+j7', 84: 'D-', 85: 'D-6', 86: 'D-7', 87: 'D-j7', 88: 'D6', 89: 'D7', 90: 'Db', 91: 'Db+', 92: 'Db+7', 93: 'Db+j7', 94: 'Db-', 95: 'Db-6', 96: 'Db-7', 97: 'Db-j7', 98: 'Db6', 99: 'Db7', 100: 'Dbj7', 101: 'Dbm7b5', 102: 'Dbo', 103: 'Dbo7', 104: 'Dbsus', 105: 'Dbsus7', 106: 'Dj7', 107: 'Dm7b5', 108: 'Do', 109: 'Do7', 110: 'Dsus', 111: 'Dsus7', 112: 'E', 113: 'E+', 114: 'E+7', 115: 'E+j7', 116: 'E-', 117: 'E-6', 118: 'E-7', 119: 'E-j7', 120: 'E6', 121: 'E7', 122: 'Eb', 123: 'Eb+', 124: 'Eb+7', 125: 'Eb+j7', 126: 'Eb-', 127: 'Eb-6', 128: 'Eb-7', 129: 'Eb-j7', 130: 'Eb6', 131: 'Eb7', 132: 'Ebj7', 133: 'Ebm7b5', 134: 'Ebo', 135: 'Ebo7', 136: 'Ebsus', 137: 'Ebsus7', 138: 'Ej7', 139: 'Em7b5', 140: 'Eo', 141: 'Eo7', 142: 'Esus', 143: 'Esus7', 144: 'F', 145: 'F+', 146: 'F+7', 147: 'F+j7', 148: 'F-', 149: 'F-6', 150: 'F-7', 151: 'F-j7', 152: 'F6', 153: 'F7', 154: 'Fj7', 155: 'Fm7b5', 156: 'Fo', 157: 'Fo7', 158: 'Fsus', 159: 'Fsus7', 160: 'G', 161: 'G+', 162: 'G+7', 163: 'G+j7', 164: 'G-', 165: 'G-6', 166: 'G-7', 167: 'G-j7', 168: 'G6', 169: 'G7', 170: 'Gb', 171: 'Gb+', 172: 'Gb+7', 173: 'Gb+j7', 174: 'Gb-', 175: 'Gb-6', 176: 'Gb-7', 177: 'Gb-j7', 178: 'Gb6', 179: 'Gb7', 180: 'Gbj7', 181: 'Gbm7b5', 182: 'Gbo', 183: 'Gbo7', 184: 'Gbsus', 185: 'Gbsus7', 186: 'Gj7', 187: 'Gm7b5', 188: 'Go', 189: 'Go7', 190: 'Gsus', 191: 'Gsus7', 192: 'NC', 193: '<bos>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create categorical data mappings\n",
    "note_to_int = dict([(c, i) for i, c in enumerate(unique_notes[1:])])\n",
    "note_to_int[-1] = len(note_to_int)\n",
    "note_to_int['<pad>'] = len(note_to_int)\n",
    "\n",
    "int_to_note = dict([(k, v) for v, k in note_to_int.items()])\n",
    "\n",
    "chord_to_int = dict([(c, i) for i, c in enumerate(unique_chords)])\n",
    "chord_to_int['<bos>'] = len(chord_to_int)\n",
    "\n",
    "int_to_chord = dict([(k, v) for v, k in chord_to_int.items()])\n",
    "\n",
    "print(\"Melody note to integer mapping:\\n {}\\n\".format(note_to_int))\n",
    "print(\"Integer to melody note mapping:\\n {}\\n\".format(int_to_note))\n",
    "print(\"Chord label to integer mapping:\\n {}\\n\".format(chord_to_int))\n",
    "print(\"Integer to chord label mapping:\\n {}\\n\".format(int_to_chord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections: 28836\n",
      "\n",
      "Number of sections: 28416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refine data that will actually be used\n",
    "def check_if_augmented_major(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"+j7\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_if_NC(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"NC\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "section_data = [section for section in data if not check_if_augmented_major(section)]\n",
    "print(\"Number of sections: {}\\n\".format(len(section_data)))\n",
    "\n",
    "section_data = [section for section in section_data if not check_if_NC(section)]\n",
    "print(\"Number of sections: {}\\n\".format(len(section_data)))\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in section_data]\n",
    "chords_data = [chord_info[0] for section in section_data for chord_info in section]\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in section_data for chord_info in section]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 327708\n",
      "Number of distinct melody notes: 14\n",
      "Number of distinct chord labels: 194\n",
      "Maximum length of melody sequences for one chord: 115\n",
      "Number of past chords given as input: 7\n"
     ]
    }
   ],
   "source": [
    "# Define numerical variables\n",
    "\n",
    "n_samples = len(chords_data)\n",
    "n_chords = len(chord_to_int)\n",
    "n_notes = len(note_to_int)\n",
    "max_mel_len = max([len(mel) for mel in notes_by_chords])\n",
    "chord_context_len = 7\n",
    "\n",
    "# print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Number of distinct melody notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct chord labels: {}\".format(n_chords))\n",
    "print(\"Maximum length of melody sequences for one chord: {}\".format(max_mel_len))\n",
    "print(\"Number of past chords given as input: {}\".format(chord_context_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input melody sequence: [8, 3, 6, '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample input chord sequence: ['<bos>', '<bos>', 'E6', 'Db7', 'Gb-7', 'B7', 'E']\n",
      "\n",
      "Sample target chord: Db-7\n",
      "\n",
      "Input melody: 327708, Input chords: 327708, Target chords: 327708\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensor data\n",
    "def pad_melody(melody, max_len):\n",
    "    return melody + (max_len-len(melody))*['<pad>']\n",
    "\n",
    "def build_input_chord_sequences(chord_seq, context_len):\n",
    "    padded_sequence = context_len*['<bos>'] + chord_seq\n",
    "    formatted_sequences = [padded_sequence[i:i+context_len+1] for i in range(len(chord_seq))]\n",
    "    return formatted_sequences\n",
    "\n",
    "# Melody\n",
    "input_melody_data = [pad_melody(melody, max_mel_len) for melody in notes_by_chords]\n",
    "print(\"Sample input melody sequence: {}\\n\".format(input_melody_data[5]))\n",
    "\n",
    "# Chords\n",
    "formatted_chords_data = []\n",
    "for section_chords in chords_by_sections:\n",
    "    formatted_chords_data += build_input_chord_sequences(section_chords, chord_context_len)\n",
    "    \n",
    "input_chords_data = [ch[:-1] for ch in formatted_chords_data]\n",
    "target_chords_data = [ch[-1] for ch in formatted_chords_data]\n",
    "print(\"Sample input chord sequence: {}\\n\".format(input_chords_data[5]))\n",
    "print(\"Sample target chord: {}\\n\".format(target_chords_data[5]))\n",
    "\n",
    "print(\"Input melody: {}, Input chords: {}, Target chords: {}\".format(len(input_melody_data), len(input_chords_data), len(target_chords_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tensors\n",
    "\n",
    "X_melody = np.zeros((n_samples, max_mel_len, n_notes), dtype='float32')\n",
    "X_labels = np.zeros((n_samples, chord_context_len, n_chords), dtype='float32')\n",
    "Y_labels = np.zeros((n_samples, n_chords), dtype='float32')\n",
    "\n",
    "for i, (input_mel, input_ch, target_ch) in enumerate(zip(input_melody_data, input_chords_data, target_chords_data)):\n",
    "    Y_labels[i, chord_to_int[target_ch]] = 1\n",
    "    for j, chord in enumerate(input_ch):\n",
    "        X_labels[i, j, chord_to_int[chord]] = 1\n",
    "        \n",
    "    for j, note in enumerate(input_mel):\n",
    "        X_melody[i, j, note_to_int[note]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 80%-10%-10% train-valid-test\n",
    "\n",
    "seed = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "\n",
    "for train_index, aux_index in sss.split(X_labels, Y_labels):\n",
    "    X_melody_train, X_melody_aux = X_melody[train_index], X_melody[aux_index]\n",
    "    X_labels_train, X_labels_aux = X_labels[train_index], X_labels[aux_index]\n",
    "    Y_labels_train, Y_labels_aux = Y_labels[train_index], Y_labels[aux_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "\n",
    "for valid_index, test_index in sss.split(X_labels_aux, Y_labels_aux):\n",
    "    X_melody_valid, X_melody_test = X_melody_aux[valid_index], X_melody_aux[test_index]\n",
    "    X_labels_valid, X_labels_test = X_labels_aux[valid_index], X_labels_aux[test_index]\n",
    "    Y_labels_valid, Y_labels_test = Y_labels_aux[valid_index], Y_labels_aux[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 115, 14)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 7, 194)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 115, 128)      54912       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_4 (GRU)                      (None, 7, 128)        124032      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 115, 128)      98688       gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_5 (GRU)                      (None, 7, 128)        98688       gru_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_3 (GRU)                      (None, 128)           98688       gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_6 (GRU)                      (None, 128)           98688       gru_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           gru_3[0][0]                      \n",
      "                                                                   gru_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           32896       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 194)           25026       dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 648,130\n",
      "Trainable params: 648,130\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 276.00 556.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-552 272,-552 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140382709846088 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140382709846088</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 125,-547.5 125,-511.5 0,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-525.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140382709846368 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140382709846368</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-438.5 30.5,-474.5 114.5,-474.5 114.5,-438.5 30.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72.5\" y=\"-452.8\">gru_1: GRU</text>\n",
       "</g>\n",
       "<!-- 140382709846088&#45;&gt;140382709846368 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140382709846088-&gt;140382709846368</title>\n",
       "<path d=\"M64.9207,-511.313C66.0508,-503.289 67.4229,-493.547 68.6874,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.1726,-484.919 70.1016,-474.529 65.241,-483.943 72.1726,-484.919\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377882590680 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140377882590680</title>\n",
       "<polygon fill=\"none\" points=\"143,-511.5 143,-547.5 268,-547.5 268,-511.5 143,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-525.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140377847124600 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140377847124600</title>\n",
       "<polygon fill=\"none\" points=\"152.5,-438.5 152.5,-474.5 236.5,-474.5 236.5,-438.5 152.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-452.8\">gru_4: GRU</text>\n",
       "</g>\n",
       "<!-- 140377882590680&#45;&gt;140377847124600 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140377882590680-&gt;140377847124600</title>\n",
       "<path d=\"M202.837,-511.313C201.594,-503.289 200.085,-493.547 198.694,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.128,-483.875 197.138,-474.529 195.211,-484.947 202.128,-483.875\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140382709848440 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140382709848440</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-365.5 40.5,-401.5 124.5,-401.5 124.5,-365.5 40.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-379.8\">gru_2: GRU</text>\n",
       "</g>\n",
       "<!-- 140382709846368&#45;&gt;140382709848440 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140382709846368-&gt;140382709848440</title>\n",
       "<path d=\"M74.9207,-438.313C76.0508,-430.289 77.4229,-420.547 78.6874,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"82.1726,-411.919 80.1016,-401.529 75.241,-410.943 82.1726,-411.919\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377845641456 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140377845641456</title>\n",
       "<polygon fill=\"none\" points=\"150.5,-365.5 150.5,-401.5 234.5,-401.5 234.5,-365.5 150.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-379.8\">gru_5: GRU</text>\n",
       "</g>\n",
       "<!-- 140377847124600&#45;&gt;140377845641456 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140377847124600-&gt;140377845641456</title>\n",
       "<path d=\"M194.016,-438.313C193.79,-430.289 193.515,-420.547 193.263,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"196.76,-411.426 192.98,-401.529 189.763,-411.623 196.76,-411.426\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140382709846984 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140382709846984</title>\n",
       "<polygon fill=\"none\" points=\"41.5,-292.5 41.5,-328.5 125.5,-328.5 125.5,-292.5 41.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83.5\" y=\"-306.8\">gru_3: GRU</text>\n",
       "</g>\n",
       "<!-- 140382709848440&#45;&gt;140382709846984 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140382709848440-&gt;140382709846984</title>\n",
       "<path d=\"M82.7421,-365.313C82.8551,-357.289 82.9923,-347.547 83.1187,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.6189,-338.577 83.2602,-328.529 79.6196,-338.479 86.6189,-338.577\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377845127096 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140377845127096</title>\n",
       "<polygon fill=\"none\" points=\"145.5,-292.5 145.5,-328.5 229.5,-328.5 229.5,-292.5 145.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.5\" y=\"-306.8\">gru_6: GRU</text>\n",
       "</g>\n",
       "<!-- 140377845641456&#45;&gt;140377845127096 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140377845641456-&gt;140377845127096</title>\n",
       "<path d=\"M191.29,-365.313C190.725,-357.289 190.039,-347.547 189.406,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"192.893,-338.258 188.699,-328.529 185.91,-338.75 192.893,-338.258\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377843489200 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140377843489200</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-219.5 51.5,-255.5 219.5,-255.5 219.5,-219.5 51.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-233.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140382709846984&#45;&gt;140377843489200 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140382709846984-&gt;140377843489200</title>\n",
       "<path d=\"M96.0877,-292.313C102.347,-283.766 110.035,-273.269 116.954,-263.823\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"119.943,-265.665 123.028,-255.529 114.296,-261.528 119.943,-265.665\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377845127096&#45;&gt;140377843489200 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140377845127096-&gt;140377843489200</title>\n",
       "<path d=\"M174.912,-292.313C168.653,-283.766 160.965,-273.269 154.046,-263.823\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"156.704,-261.528 147.972,-255.529 151.057,-265.665 156.704,-261.528\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140376944749648 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140376944749648</title>\n",
       "<polygon fill=\"none\" points=\"84.5,-146.5 84.5,-182.5 186.5,-182.5 186.5,-146.5 84.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140377843489200&#45;&gt;140376944749648 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140377843489200-&gt;140376944749648</title>\n",
       "<path d=\"M135.5,-219.313C135.5,-211.289 135.5,-201.547 135.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"139,-192.529 135.5,-182.529 132,-192.529 139,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140377843936952 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140377843936952</title>\n",
       "<polygon fill=\"none\" points=\"84.5,-73.5 84.5,-109.5 186.5,-109.5 186.5,-73.5 84.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140376944749648&#45;&gt;140377843936952 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140376944749648-&gt;140377843936952</title>\n",
       "<path d=\"M135.5,-146.313C135.5,-138.289 135.5,-128.547 135.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"139,-119.529 135.5,-109.529 132,-119.529 139,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140376943635536 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140376943635536</title>\n",
       "<polygon fill=\"none\" points=\"84.5,-0.5 84.5,-36.5 186.5,-36.5 186.5,-0.5 84.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140377843936952&#45;&gt;140376943635536 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140377843936952-&gt;140376943635536</title>\n",
       "<path d=\"M135.5,-73.3129C135.5,-65.2895 135.5,-55.5475 135.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"139,-46.5288 135.5,-36.5288 132,-46.5289 139,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define neural net architecture\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "melody_input = Input(shape=(max_mel_len, n_notes))\n",
    "melody_gru1 = GRU(latent_dim, return_sequences=True)(melody_input)\n",
    "melody_gru2 = GRU(latent_dim, return_sequences=True)(melody_gru1)\n",
    "melody_gru3 = GRU(latent_dim)(melody_gru2)\n",
    "\n",
    "label_input = Input(shape=(chord_context_len, n_chords))\n",
    "label_gru1 = GRU(latent_dim, return_sequences=True)(label_input)\n",
    "label_gru2 = GRU(latent_dim, return_sequences=True)(label_gru1)\n",
    "label_gru3 = GRU(latent_dim)(label_gru2)\n",
    "\n",
    "concat = concatenate([melody_gru3, label_gru3])\n",
    "\n",
    "label_hidden1 = Dense(latent_dim, activation='relu')(concat)\n",
    "label_hidden2 = Dense(latent_dim, activation='relu')(label_hidden1)\n",
    "label_dense = Dense(n_chords, activation='softmax')(label_hidden2)\n",
    "\n",
    "model = Model([melody_input, label_input], label_dense)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce Save-Best-Performance callbacks\n",
    "filepath = \"Label_depth3.h5\"\n",
    "bp = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262166 samples, validate on 32771 samples\n",
      "Epoch 1/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 3.0648 - acc: 0.3054Epoch 00000: val_acc improved from -inf to 0.41427, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1196s - loss: 3.0647 - acc: 0.3054 - val_loss: 2.5460 - val_acc: 0.4143\n",
      "Epoch 2/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 2.3408 - acc: 0.4456Epoch 00001: val_acc improved from 0.41427 to 0.46874, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1195s - loss: 2.3408 - acc: 0.4456 - val_loss: 2.2327 - val_acc: 0.4687\n",
      "Epoch 3/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 2.0836 - acc: 0.4989Epoch 00002: val_acc improved from 0.46874 to 0.50377, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1194s - loss: 2.0836 - acc: 0.4989 - val_loss: 2.0531 - val_acc: 0.5038\n",
      "Epoch 4/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.9012 - acc: 0.5422Epoch 00003: val_acc improved from 0.50377 to 0.54448, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1188s - loss: 1.9012 - acc: 0.5422 - val_loss: 1.9106 - val_acc: 0.5445\n",
      "Epoch 5/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.7562 - acc: 0.5761Epoch 00004: val_acc improved from 0.54448 to 0.56922, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1190s - loss: 1.7562 - acc: 0.5761 - val_loss: 1.8163 - val_acc: 0.5692\n",
      "Epoch 6/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.6388 - acc: 0.6020Epoch 00005: val_acc improved from 0.56922 to 0.58997, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1191s - loss: 1.6388 - acc: 0.6020 - val_loss: 1.7243 - val_acc: 0.5900\n",
      "Epoch 7/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.5384 - acc: 0.6253Epoch 00006: val_acc improved from 0.58997 to 0.61033, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1187s - loss: 1.5384 - acc: 0.6253 - val_loss: 1.6569 - val_acc: 0.6103\n",
      "Epoch 8/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.4572 - acc: 0.6439Epoch 00007: val_acc improved from 0.61033 to 0.62424, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1189s - loss: 1.4572 - acc: 0.6439 - val_loss: 1.5971 - val_acc: 0.6242\n",
      "Epoch 9/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.3894 - acc: 0.6581Epoch 00008: val_acc improved from 0.62424 to 0.63196, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1190s - loss: 1.3894 - acc: 0.6581 - val_loss: 1.5597 - val_acc: 0.6320\n",
      "Epoch 10/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.3342 - acc: 0.6691Epoch 00009: val_acc improved from 0.63196 to 0.64093, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1201s - loss: 1.3342 - acc: 0.6691 - val_loss: 1.5242 - val_acc: 0.6409\n",
      "Epoch 11/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.2862 - acc: 0.6804Epoch 00010: val_acc improved from 0.64093 to 0.64981, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 1.2862 - acc: 0.6804 - val_loss: 1.4836 - val_acc: 0.6498\n",
      "Epoch 12/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.2453 - acc: 0.6883Epoch 00011: val_acc improved from 0.64981 to 0.65692, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 1.2453 - acc: 0.6883 - val_loss: 1.4781 - val_acc: 0.6569\n",
      "Epoch 13/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.2113 - acc: 0.6957Epoch 00012: val_acc improved from 0.65692 to 0.66174, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1195s - loss: 1.2112 - acc: 0.6957 - val_loss: 1.4572 - val_acc: 0.6617\n",
      "Epoch 14/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.1819 - acc: 0.7016Epoch 00013: val_acc improved from 0.66174 to 0.66888, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1194s - loss: 1.1818 - acc: 0.7016 - val_loss: 1.4280 - val_acc: 0.6689\n",
      "Epoch 15/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.1567 - acc: 0.7062Epoch 00014: val_acc improved from 0.66888 to 0.67236, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1195s - loss: 1.1567 - acc: 0.7062 - val_loss: 1.4101 - val_acc: 0.6724\n",
      "Epoch 16/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.1334 - acc: 0.7103Epoch 00015: val_acc improved from 0.67236 to 0.67502, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1199s - loss: 1.1334 - acc: 0.7103 - val_loss: 1.4013 - val_acc: 0.6750\n",
      "Epoch 17/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.1156 - acc: 0.7142Epoch 00016: val_acc improved from 0.67502 to 0.67648, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 1.1156 - acc: 0.7142 - val_loss: 1.3974 - val_acc: 0.6765\n",
      "Epoch 18/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0963 - acc: 0.7188Epoch 00017: val_acc improved from 0.67648 to 0.67770, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 1.0964 - acc: 0.7188 - val_loss: 1.3832 - val_acc: 0.6777\n",
      "Epoch 19/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0817 - acc: 0.7219Epoch 00018: val_acc improved from 0.67770 to 0.68255, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 1.0818 - acc: 0.7218 - val_loss: 1.3770 - val_acc: 0.6826\n",
      "Epoch 20/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.7242Epoch 00019: val_acc did not improve\n",
      "262166/262166 [==============================] - 1196s - loss: 1.0677 - acc: 0.7242 - val_loss: 1.3765 - val_acc: 0.6826\n",
      "Epoch 21/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0558 - acc: 0.7267Epoch 00020: val_acc improved from 0.68255 to 0.68277, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1196s - loss: 1.0558 - acc: 0.7267 - val_loss: 1.3763 - val_acc: 0.6828\n",
      "Epoch 22/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0446 - acc: 0.7283Epoch 00021: val_acc improved from 0.68277 to 0.69037, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 1.0447 - acc: 0.7283 - val_loss: 1.3514 - val_acc: 0.6904\n",
      "Epoch 23/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0362 - acc: 0.7299Epoch 00022: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 1.0362 - acc: 0.7299 - val_loss: 1.3669 - val_acc: 0.6859\n",
      "Epoch 24/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0264 - acc: 0.7319Epoch 00023: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 1.0264 - acc: 0.7319 - val_loss: 1.3519 - val_acc: 0.6879\n",
      "Epoch 25/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0180 - acc: 0.7336Epoch 00024: val_acc did not improve\n",
      "262166/262166 [==============================] - 1201s - loss: 1.0180 - acc: 0.7336 - val_loss: 1.3521 - val_acc: 0.6902\n",
      "Epoch 26/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0126 - acc: 0.7352Epoch 00025: val_acc improved from 0.69037 to 0.69391, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1199s - loss: 1.0126 - acc: 0.7352 - val_loss: 1.3508 - val_acc: 0.6939\n",
      "Epoch 27/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0033 - acc: 0.7369Epoch 00026: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 1.0033 - acc: 0.7369 - val_loss: 1.3512 - val_acc: 0.6913\n",
      "Epoch 28/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 1.0029 - acc: 0.7359Epoch 00027: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 1.0029 - acc: 0.7359 - val_loss: 1.3411 - val_acc: 0.6929\n",
      "Epoch 29/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9930 - acc: 0.7383Epoch 00028: val_acc improved from 0.69391 to 0.69659, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1205s - loss: 0.9930 - acc: 0.7383 - val_loss: 1.3379 - val_acc: 0.6966\n",
      "Epoch 30/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9916 - acc: 0.7376Epoch 00029: val_acc improved from 0.69659 to 0.69900, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1204s - loss: 0.9916 - acc: 0.7376 - val_loss: 1.3323 - val_acc: 0.6990\n",
      "Epoch 31/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9835 - acc: 0.7398Epoch 00030: val_acc improved from 0.69900 to 0.69995, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9836 - acc: 0.7398 - val_loss: 1.3278 - val_acc: 0.6999\n",
      "Epoch 32/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9802 - acc: 0.7411Epoch 00031: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9802 - acc: 0.7411 - val_loss: 1.3288 - val_acc: 0.6988\n",
      "Epoch 33/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9770 - acc: 0.7411Epoch 00032: val_acc improved from 0.69995 to 0.70221, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9770 - acc: 0.7411 - val_loss: 1.3278 - val_acc: 0.7022\n",
      "Epoch 34/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9732 - acc: 0.7416Epoch 00033: val_acc improved from 0.70221 to 0.70239, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1194s - loss: 0.9732 - acc: 0.7416 - val_loss: 1.3228 - val_acc: 0.7024\n",
      "Epoch 35/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.7421Epoch 00034: val_acc did not improve\n",
      "262166/262166 [==============================] - 1194s - loss: 0.9714 - acc: 0.7421 - val_loss: 1.3162 - val_acc: 0.7008\n",
      "Epoch 36/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9650 - acc: 0.7434Epoch 00035: val_acc did not improve\n",
      "262166/262166 [==============================] - 1194s - loss: 0.9650 - acc: 0.7434 - val_loss: 1.3210 - val_acc: 0.7020\n",
      "Epoch 37/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.7442Epoch 00036: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.9616 - acc: 0.7442 - val_loss: 1.3274 - val_acc: 0.7013\n",
      "Epoch 38/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9592 - acc: 0.7444Epoch 00037: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.9592 - acc: 0.7443 - val_loss: 1.3211 - val_acc: 0.6989\n",
      "Epoch 39/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9576 - acc: 0.7450Epoch 00038: val_acc improved from 0.70239 to 0.70340, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.9576 - acc: 0.7450 - val_loss: 1.3216 - val_acc: 0.7034\n",
      "Epoch 40/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.7461Epoch 00039: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9544 - acc: 0.7460 - val_loss: 1.3198 - val_acc: 0.7000\n",
      "Epoch 41/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9553 - acc: 0.7450Epoch 00040: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9552 - acc: 0.7450 - val_loss: 1.3194 - val_acc: 0.7023\n",
      "Epoch 42/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9526 - acc: 0.7454Epoch 00041: val_acc improved from 0.70340 to 0.70587, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1195s - loss: 0.9526 - acc: 0.7454 - val_loss: 1.3102 - val_acc: 0.7059\n",
      "Epoch 43/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9591 - acc: 0.7447Epoch 00042: val_acc did not improve\n",
      "262166/262166 [==============================] - 1195s - loss: 0.9591 - acc: 0.7447 - val_loss: 1.3212 - val_acc: 0.7023\n",
      "Epoch 44/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9480 - acc: 0.7470Epoch 00043: val_acc did not improve\n",
      "262166/262166 [==============================] - 1194s - loss: 0.9480 - acc: 0.7470 - val_loss: 1.3227 - val_acc: 0.7030\n",
      "Epoch 45/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9450 - acc: 0.7475Epoch 00044: val_acc improved from 0.70587 to 0.70733, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1194s - loss: 0.9450 - acc: 0.7475 - val_loss: 1.3080 - val_acc: 0.7073\n",
      "Epoch 46/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9442 - acc: 0.7473Epoch 00045: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9441 - acc: 0.7473 - val_loss: 1.3116 - val_acc: 0.7028\n",
      "Epoch 47/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9411 - acc: 0.7475Epoch 00046: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9411 - acc: 0.7475 - val_loss: 1.3208 - val_acc: 0.7051\n",
      "Epoch 48/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9394 - acc: 0.7481Epoch 00047: val_acc improved from 0.70733 to 0.70828, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1195s - loss: 0.9394 - acc: 0.7481 - val_loss: 1.3091 - val_acc: 0.7083\n",
      "Epoch 49/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.7491Epoch 00048: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9373 - acc: 0.7491 - val_loss: 1.3108 - val_acc: 0.7072\n",
      "Epoch 50/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7492Epoch 00049: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9354 - acc: 0.7492 - val_loss: 1.3118 - val_acc: 0.7064\n",
      "Epoch 51/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7493Epoch 00050: val_acc improved from 0.70828 to 0.70880, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1196s - loss: 0.9354 - acc: 0.7493 - val_loss: 1.3200 - val_acc: 0.7088\n",
      "Epoch 52/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.7491Epoch 00051: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.9330 - acc: 0.7491 - val_loss: 1.3053 - val_acc: 0.7085\n",
      "Epoch 53/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9326 - acc: 0.7499Epoch 00052: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.9326 - acc: 0.7499 - val_loss: 1.3119 - val_acc: 0.7053\n",
      "Epoch 54/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7499Epoch 00053: val_acc did not improve\n",
      "262166/262166 [==============================] - 1196s - loss: 0.9317 - acc: 0.7499 - val_loss: 1.3034 - val_acc: 0.7050\n",
      "Epoch 55/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9284 - acc: 0.7505Epoch 00054: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9284 - acc: 0.7505 - val_loss: 1.3044 - val_acc: 0.7062\n",
      "Epoch 56/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9282 - acc: 0.7506Epoch 00055: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9283 - acc: 0.7506 - val_loss: 1.3069 - val_acc: 0.7075\n",
      "Epoch 57/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9281 - acc: 0.7505Epoch 00056: val_acc improved from 0.70880 to 0.71154, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9281 - acc: 0.7505 - val_loss: 1.2929 - val_acc: 0.7115\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.7504Epoch 00057: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9276 - acc: 0.7504 - val_loss: 1.3065 - val_acc: 0.7100\n",
      "Epoch 59/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.7505Epoch 00058: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9254 - acc: 0.7505 - val_loss: 1.3095 - val_acc: 0.7081\n",
      "Epoch 60/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9244 - acc: 0.7512Epoch 00059: val_acc improved from 0.71154 to 0.71170, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9245 - acc: 0.7512 - val_loss: 1.3077 - val_acc: 0.7117\n",
      "Epoch 61/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9209 - acc: 0.7521Epoch 00060: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.9209 - acc: 0.7521 - val_loss: 1.3065 - val_acc: 0.7086\n",
      "Epoch 62/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9191 - acc: 0.7524Epoch 00061: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.9191 - acc: 0.7524 - val_loss: 1.3109 - val_acc: 0.7090\n",
      "Epoch 63/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9226 - acc: 0.7510Epoch 00062: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.9226 - acc: 0.7510 - val_loss: 1.3108 - val_acc: 0.7102\n",
      "Epoch 64/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.7528Epoch 00063: val_acc did not improve\n",
      "262166/262166 [==============================] - 1190s - loss: 0.9168 - acc: 0.7528 - val_loss: 1.3176 - val_acc: 0.7082\n",
      "Epoch 65/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9207 - acc: 0.7508Epoch 00064: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9206 - acc: 0.7508 - val_loss: 1.3233 - val_acc: 0.7071\n",
      "Epoch 66/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9169 - acc: 0.7530Epoch 00065: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 0.9170 - acc: 0.7530 - val_loss: 1.3075 - val_acc: 0.7097\n",
      "Epoch 67/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9177 - acc: 0.7529Epoch 00066: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9177 - acc: 0.7529 - val_loss: 1.3030 - val_acc: 0.7113\n",
      "Epoch 68/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.7532Epoch 00067: val_acc improved from 0.71170 to 0.71234, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9149 - acc: 0.7532 - val_loss: 1.3027 - val_acc: 0.7123\n",
      "Epoch 69/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9131 - acc: 0.7537Epoch 00068: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9131 - acc: 0.7537 - val_loss: 1.3057 - val_acc: 0.7090\n",
      "Epoch 70/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9422 - acc: 0.7467Epoch 00069: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9423 - acc: 0.7467 - val_loss: 1.3396 - val_acc: 0.7019\n",
      "Epoch 71/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9331 - acc: 0.7488Epoch 00070: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9331 - acc: 0.7488 - val_loss: 1.3129 - val_acc: 0.7060\n",
      "Epoch 72/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9186 - acc: 0.7524Epoch 00071: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9186 - acc: 0.7524 - val_loss: 1.3158 - val_acc: 0.7076\n",
      "Epoch 73/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.7484Epoch 00072: val_acc did not improve\n",
      "262166/262166 [==============================] - 1188s - loss: 0.9330 - acc: 0.7484 - val_loss: 1.3229 - val_acc: 0.7093\n",
      "Epoch 74/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.7472Epoch 00073: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9397 - acc: 0.7472 - val_loss: 1.3278 - val_acc: 0.7067\n",
      "Epoch 75/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9240 - acc: 0.7521Epoch 00074: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9240 - acc: 0.7521 - val_loss: 1.3274 - val_acc: 0.7090\n",
      "Epoch 76/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9185 - acc: 0.7525Epoch 00075: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.9185 - acc: 0.7525 - val_loss: 1.3210 - val_acc: 0.7106\n",
      "Epoch 77/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9174 - acc: 0.7532Epoch 00076: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.9174 - acc: 0.7532 - val_loss: 1.3313 - val_acc: 0.7075\n",
      "Epoch 78/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9224 - acc: 0.7514Epoch 00077: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.9225 - acc: 0.7514 - val_loss: 1.3208 - val_acc: 0.7086\n",
      "Epoch 79/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9115 - acc: 0.7545Epoch 00078: val_acc did not improve\n",
      "262166/262166 [==============================] - 1190s - loss: 0.9115 - acc: 0.7545 - val_loss: 1.3103 - val_acc: 0.7098\n",
      "Epoch 80/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.9023 - acc: 0.7550Epoch 00079: val_acc improved from 0.71234 to 0.71347, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1189s - loss: 0.9022 - acc: 0.7550 - val_loss: 1.3027 - val_acc: 0.7135\n",
      "Epoch 81/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8959 - acc: 0.7569Epoch 00080: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.8959 - acc: 0.7569 - val_loss: 1.3024 - val_acc: 0.7120\n",
      "Epoch 82/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8838 - acc: 0.7579Epoch 00081: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8838 - acc: 0.7579 - val_loss: 1.2862 - val_acc: 0.7124\n",
      "Epoch 83/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7559Epoch 00082: val_acc improved from 0.71347 to 0.71408, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1199s - loss: 0.8907 - acc: 0.7559 - val_loss: 1.2840 - val_acc: 0.7141\n",
      "Epoch 84/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8812 - acc: 0.7576Epoch 00083: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.8813 - acc: 0.7576 - val_loss: 1.2904 - val_acc: 0.7131\n",
      "Epoch 85/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8838 - acc: 0.7563Epoch 00084: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.8838 - acc: 0.7563 - val_loss: 1.2992 - val_acc: 0.7131\n",
      "Epoch 86/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8805 - acc: 0.7572Epoch 00085: val_acc did not improve\n",
      "262166/262166 [==============================] - 1191s - loss: 0.8806 - acc: 0.7572 - val_loss: 1.2929 - val_acc: 0.7126\n",
      "Epoch 87/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.7588Epoch 00086: val_acc improved from 0.71408 to 0.71591, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1190s - loss: 0.8708 - acc: 0.7588 - val_loss: 1.2820 - val_acc: 0.7159\n",
      "Epoch 88/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8731 - acc: 0.7584Epoch 00087: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.8730 - acc: 0.7584 - val_loss: 1.2711 - val_acc: 0.7152\n",
      "Epoch 89/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8675 - acc: 0.7598Epoch 00088: val_acc improved from 0.71591 to 0.71826, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1193s - loss: 0.8675 - acc: 0.7598 - val_loss: 1.2694 - val_acc: 0.7183\n",
      "Epoch 90/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8536 - acc: 0.7627Epoch 00089: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.8537 - acc: 0.7627 - val_loss: 1.2681 - val_acc: 0.7142\n",
      "Epoch 91/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8447 - acc: 0.7647Epoch 00090: val_acc did not improve\n",
      "262166/262166 [==============================] - 1194s - loss: 0.8447 - acc: 0.7647 - val_loss: 1.2797 - val_acc: 0.7177\n",
      "Epoch 92/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8355 - acc: 0.7669Epoch 00091: val_acc improved from 0.71826 to 0.72000, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1201s - loss: 0.8355 - acc: 0.7669 - val_loss: 1.2606 - val_acc: 0.7200\n",
      "Epoch 93/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7666Epoch 00092: val_acc improved from 0.72000 to 0.72219, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.8327 - acc: 0.7666 - val_loss: 1.2598 - val_acc: 0.7222\n",
      "Epoch 94/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8296 - acc: 0.7669Epoch 00093: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8296 - acc: 0.7669 - val_loss: 1.2502 - val_acc: 0.7211\n",
      "Epoch 95/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8173 - acc: 0.7708Epoch 00094: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8173 - acc: 0.7708 - val_loss: 1.2641 - val_acc: 0.7200\n",
      "Epoch 96/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8046 - acc: 0.7739Epoch 00095: val_acc improved from 0.72219 to 0.72650, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8046 - acc: 0.7739 - val_loss: 1.2390 - val_acc: 0.7265\n",
      "Epoch 97/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8039 - acc: 0.7738Epoch 00096: val_acc did not improve\n",
      "262166/262166 [==============================] - 1191s - loss: 0.8039 - acc: 0.7738 - val_loss: 1.2367 - val_acc: 0.7249\n",
      "Epoch 98/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7958 - acc: 0.7756Epoch 00097: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.7958 - acc: 0.7756 - val_loss: 1.2752 - val_acc: 0.7155\n",
      "Epoch 99/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7533Epoch 00098: val_acc did not improve\n",
      "262166/262166 [==============================] - 1189s - loss: 0.8967 - acc: 0.7533 - val_loss: 1.2862 - val_acc: 0.7149\n",
      "Epoch 100/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7595Epoch 00099: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.8678 - acc: 0.7595 - val_loss: 1.2810 - val_acc: 0.7185\n",
      "Epoch 101/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7645Epoch 00100: val_acc did not improve\n",
      "262166/262166 [==============================] - 1194s - loss: 0.8477 - acc: 0.7645 - val_loss: 1.2546 - val_acc: 0.7216\n",
      "Epoch 102/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7668Epoch 00101: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.8341 - acc: 0.7668 - val_loss: 1.2737 - val_acc: 0.7198\n",
      "Epoch 103/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7681Epoch 00102: val_acc did not improve\n",
      "262166/262166 [==============================] - 1188s - loss: 0.8271 - acc: 0.7681 - val_loss: 1.2506 - val_acc: 0.7238\n",
      "Epoch 104/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8171 - acc: 0.7708Epoch 00103: val_acc did not improve\n",
      "262166/262166 [==============================] - 1192s - loss: 0.8171 - acc: 0.7708 - val_loss: 1.2367 - val_acc: 0.7242\n",
      "Epoch 105/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8080 - acc: 0.7727Epoch 00104: val_acc improved from 0.72650 to 0.72961, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8080 - acc: 0.7726 - val_loss: 1.2195 - val_acc: 0.7296\n",
      "Epoch 106/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.7763Epoch 00105: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 0.7960 - acc: 0.7763 - val_loss: 1.2333 - val_acc: 0.7274\n",
      "Epoch 107/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.7731Epoch 00106: val_acc improved from 0.72961 to 0.72997, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.8096 - acc: 0.7731 - val_loss: 1.2298 - val_acc: 0.7300\n",
      "Epoch 108/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7774Epoch 00107: val_acc did not improve\n",
      "262166/262166 [==============================] - 1195s - loss: 0.7929 - acc: 0.7774 - val_loss: 1.2303 - val_acc: 0.7278\n",
      "Epoch 109/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.7804Epoch 00108: val_acc did not improve\n",
      "262166/262166 [==============================] - 1190s - loss: 0.7827 - acc: 0.7804 - val_loss: 1.2398 - val_acc: 0.7286\n",
      "Epoch 110/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7869 - acc: 0.7782Epoch 00109: val_acc improved from 0.72997 to 0.73263, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1204s - loss: 0.7869 - acc: 0.7783 - val_loss: 1.2100 - val_acc: 0.7326\n",
      "Epoch 111/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7686 - acc: 0.7837Epoch 00110: val_acc improved from 0.73263 to 0.73351, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1204s - loss: 0.7686 - acc: 0.7837 - val_loss: 1.2138 - val_acc: 0.7335\n",
      "Epoch 112/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7628 - acc: 0.7849Epoch 00111: val_acc improved from 0.73351 to 0.73550, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.7628 - acc: 0.7849 - val_loss: 1.2264 - val_acc: 0.7355\n",
      "Epoch 113/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7872Epoch 00112: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.7522 - acc: 0.7872 - val_loss: 1.2316 - val_acc: 0.7330\n",
      "Epoch 114/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7821 - acc: 0.7797Epoch 00113: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.7821 - acc: 0.7797 - val_loss: 1.2162 - val_acc: 0.7331\n",
      "Epoch 115/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7468 - acc: 0.7890Epoch 00114: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.7468 - acc: 0.7890 - val_loss: 1.2284 - val_acc: 0.7336\n",
      "Epoch 116/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7908Epoch 00115: val_acc improved from 0.73550 to 0.73605, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1203s - loss: 0.7394 - acc: 0.7909 - val_loss: 1.2218 - val_acc: 0.7360\n",
      "Epoch 117/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.7922Epoch 00116: val_acc improved from 0.73605 to 0.73629, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1204s - loss: 0.7346 - acc: 0.7922 - val_loss: 1.2242 - val_acc: 0.7363\n",
      "Epoch 118/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7930Epoch 00117: val_acc improved from 0.73629 to 0.73699, saving model to Label_depth3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262166/262166 [==============================] - 1201s - loss: 0.7323 - acc: 0.7929 - val_loss: 1.2186 - val_acc: 0.7370\n",
      "Epoch 119/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7222 - acc: 0.7960Epoch 00118: val_acc improved from 0.73699 to 0.74410, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1199s - loss: 0.7222 - acc: 0.7960 - val_loss: 1.2087 - val_acc: 0.7441\n",
      "Epoch 120/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.8011Epoch 00119: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.7046 - acc: 0.8011 - val_loss: 1.2060 - val_acc: 0.7407\n",
      "Epoch 121/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7028 - acc: 0.8004Epoch 00120: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.7029 - acc: 0.8004 - val_loss: 1.2237 - val_acc: 0.7402\n",
      "Epoch 122/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7971Epoch 00121: val_acc did not improve\n",
      "262166/262166 [==============================] - 1198s - loss: 0.7165 - acc: 0.7971 - val_loss: 1.2167 - val_acc: 0.7398\n",
      "Epoch 123/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.8062Epoch 00122: val_acc improved from 0.74410 to 0.74511, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.6869 - acc: 0.8062 - val_loss: 1.2247 - val_acc: 0.7451\n",
      "Epoch 124/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.8054Epoch 00123: val_acc improved from 0.74511 to 0.74520, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.6861 - acc: 0.8054 - val_loss: 1.2056 - val_acc: 0.7452\n",
      "Epoch 125/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.8115Epoch 00124: val_acc did not improve\n",
      "262166/262166 [==============================] - 1203s - loss: 0.6655 - acc: 0.8115 - val_loss: 1.2171 - val_acc: 0.7443\n",
      "Epoch 126/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.8116Epoch 00125: val_acc improved from 0.74520 to 0.74645, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1202s - loss: 0.6638 - acc: 0.8116 - val_loss: 1.2172 - val_acc: 0.7465\n",
      "Epoch 127/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.8171Epoch 00126: val_acc improved from 0.74645 to 0.74856, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1201s - loss: 0.6461 - acc: 0.8171 - val_loss: 1.2237 - val_acc: 0.7486\n",
      "Epoch 128/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.8146Epoch 00127: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 0.6505 - acc: 0.8146 - val_loss: 1.2190 - val_acc: 0.7481\n",
      "Epoch 129/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6337 - acc: 0.8196Epoch 00128: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 0.6337 - acc: 0.8196 - val_loss: 1.2317 - val_acc: 0.7473\n",
      "Epoch 130/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6183 - acc: 0.8243Epoch 00129: val_acc improved from 0.74856 to 0.75280, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.6183 - acc: 0.8243 - val_loss: 1.2103 - val_acc: 0.7528\n",
      "Epoch 131/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.8255Epoch 00130: val_acc improved from 0.75280 to 0.75368, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.6127 - acc: 0.8254 - val_loss: 1.2134 - val_acc: 0.7537\n",
      "Epoch 132/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.8276Epoch 00131: val_acc improved from 0.75368 to 0.75387, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.6059 - acc: 0.8276 - val_loss: 1.2189 - val_acc: 0.7539\n",
      "Epoch 133/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8298Epoch 00132: val_acc did not improve\n",
      "262166/262166 [==============================] - 1200s - loss: 0.5943 - acc: 0.8298 - val_loss: 1.2342 - val_acc: 0.7505\n",
      "Epoch 134/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.6118 - acc: 0.8250Epoch 00133: val_acc did not improve\n",
      "262166/262166 [==============================] - 1201s - loss: 0.6118 - acc: 0.8250 - val_loss: 1.2315 - val_acc: 0.7496\n",
      "Epoch 135/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.8297Epoch 00134: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.5949 - acc: 0.8297 - val_loss: 1.2379 - val_acc: 0.7479\n",
      "Epoch 136/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5875 - acc: 0.8324Epoch 00135: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.5875 - acc: 0.8324 - val_loss: 1.2486 - val_acc: 0.7508\n",
      "Epoch 137/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8356Epoch 00136: val_acc did not improve\n",
      "262166/262166 [==============================] - 1190s - loss: 0.5740 - acc: 0.8356 - val_loss: 1.2529 - val_acc: 0.7515\n",
      "Epoch 138/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.8305Epoch 00137: val_acc improved from 0.75387 to 0.75643, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1191s - loss: 0.5915 - acc: 0.8305 - val_loss: 1.2363 - val_acc: 0.7564\n",
      "Epoch 139/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8414Epoch 00138: val_acc improved from 0.75643 to 0.75756, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1198s - loss: 0.5556 - acc: 0.8414 - val_loss: 1.2336 - val_acc: 0.7576\n",
      "Epoch 140/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8429Epoch 00139: val_acc improved from 0.75756 to 0.75777, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1200s - loss: 0.5492 - acc: 0.8429 - val_loss: 1.2331 - val_acc: 0.7578\n",
      "Epoch 141/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8389Epoch 00140: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.5631 - acc: 0.8389 - val_loss: 1.2531 - val_acc: 0.7532\n",
      "Epoch 142/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8477Epoch 00141: val_acc improved from 0.75777 to 0.75872, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1196s - loss: 0.5325 - acc: 0.8477 - val_loss: 1.2480 - val_acc: 0.7587\n",
      "Epoch 143/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8422Epoch 00142: val_acc did not improve\n",
      "262166/262166 [==============================] - 1193s - loss: 0.5512 - acc: 0.8422 - val_loss: 1.2520 - val_acc: 0.7577\n",
      "Epoch 144/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8509Epoch 00143: val_acc improved from 0.75872 to 0.76317, saving model to Label_depth3.h5\n",
      "262166/262166 [==============================] - 1193s - loss: 0.5202 - acc: 0.8510 - val_loss: 1.2450 - val_acc: 0.7632\n",
      "Epoch 145/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8526Epoch 00144: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.5153 - acc: 0.8526 - val_loss: 1.2746 - val_acc: 0.7575\n",
      "Epoch 146/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8456Epoch 00145: val_acc did not improve\n",
      "262166/262166 [==============================] - 1197s - loss: 0.5389 - acc: 0.8456 - val_loss: 1.2767 - val_acc: 0.7546\n",
      "Epoch 147/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.8319Epoch 00146: val_acc did not improve\n",
      "262166/262166 [==============================] - 1196s - loss: 0.5871 - acc: 0.8319 - val_loss: 1.2871 - val_acc: 0.7473\n",
      "Epoch 148/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8471Epoch 00147: val_acc did not improve\n",
      "262166/262166 [==============================] - 1199s - loss: 0.5334 - acc: 0.8471 - val_loss: 1.2555 - val_acc: 0.7609\n",
      "Epoch 149/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.4973 - acc: 0.8579Epoch 00148: val_acc did not improve\n",
      "262166/262166 [==============================] - 1202s - loss: 0.4973 - acc: 0.8579 - val_loss: 1.2595 - val_acc: 0.7620\n",
      "Epoch 150/150\n",
      "262144/262166 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8564Epoch 00149: val_acc did not improve\n",
      "262166/262166 [==============================] - 1201s - loss: 0.5025 - acc: 0.8564 - val_loss: 1.2734 - val_acc: 0.7604\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit([X_melody_train, X_labels_train], Y_labels_train, epochs=epochs, validation_data=([X_melody_valid, X_labels_valid], Y_labels_valid,), batch_size=batch_size, callbacks=[bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fadc47d8dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFkCAYAAADWs8tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac45dda6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model learning\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Training history of root model\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best performance model\n",
    "best_model = load_model(\"Label_depth3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord label prediction: 0.7629\n",
      "Kappa score of combined chord label prediction: 0.7569\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of labels\n",
    "\n",
    "# Predict chords from each test sample melody\n",
    "Y_labels_pred = best_model.predict([X_melody_test, X_labels_test])\n",
    "\n",
    "# Compute accuracy and kappa score\n",
    "print(\"Categorical accuracy of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_accuracy_score(Y_labels_test, Y_labels_pred)))\n",
    "print(\"Kappa score of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_kappa_score(Y_labels_test, Y_labels_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord pitch prediction: 0.9156\n",
      "TP: 111884 TN: 248174 FP: 16718 FN: 16476\n",
      "Kappa score of combined chord pitch prediction: 0.8081\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of pitches\n",
    "\n",
    "def label_to_pitch_tensors(predictions):\n",
    "    predicted_chords = [int_to_chord[np.argmax(ch)] for ch in predictions]\n",
    "    pitch_chords = [harmoutil.chord_to_notes(label) for label in predicted_chords]\n",
    "    \n",
    "    Y_pitches = np.zeros((predictions.shape[0], 12), dtype='float32')\n",
    "    for i, chord_pitches in enumerate(pitch_chords):\n",
    "        for j, pitch_presence in enumerate(chord_pitches):\n",
    "            Y_pitches[i, j] = pitch_presence\n",
    "\n",
    "    return Y_pitches\n",
    "    \n",
    "    \n",
    "Y_labels_pred_pitch = label_to_pitch_tensors(Y_labels_pred)\n",
    "Y_labels_test_pitch = label_to_pitch_tensors(Y_labels_test)\n",
    "\n",
    "print(\"Categorical accuracy of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_accuracy_score(Y_labels_test_pitch, Y_labels_pred_pitch)))\n",
    "print(\"Kappa score of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_kappa_score(Y_labels_test_pitch, Y_labels_pred_pitch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
