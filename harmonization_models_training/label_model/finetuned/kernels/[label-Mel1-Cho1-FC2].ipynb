{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import harmoutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, concatenate\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Raw data---\n",
      "Number of sections: 2408\n",
      "Sample section: [('Bb6', [[58.0], [58.0]]), ('G7', [[-1.0], [-1.0]]), ('C-7', [[-1.0], [-1.0]]), ('F7', [[-1.0], [-1.0]]), ('Bb', [[-1.0], [-1.0]]), ('G-7', [[50.0], [57.0, 60.0]]), ('C-7', [[58.0, 55.0], [58.0]]), ('F7', [[61.0], [60.0, 58.0]]), ('F-7', [[60.0], [58.0]]), ('Bb7', [[56.0, 60.0], [59.0, 57.0]]), ('Eb7', [[58.0, 54.0], [55.0, 58.0]]), ('Ab7', [[61.0, 56.0], [61.0, 62.0]]), ('D-7', [[58.0, 60.0], [55.0, 58.0]]), ('G7', [[58.0], [-1.0]]), ('C-7', [[-1.0], [-1.0]]), ('F7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Transpose and augment data---\n",
      "Number of sections after data augmentation: 28884\n",
      "Sample section: [('E6', [[52.0], [52.0]]), ('Db7', [[-1.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]]), ('E', [[-1.0], [-1.0]]), ('Db-7', [[44.0], [51.0, 54.0]]), ('Gb-7', [[52.0, 49.0], [52.0]]), ('B7', [[55.0], [54.0, 52.0]]), ('B-7', [[54.0], [52.0]]), ('E7', [[50.0, 54.0], [53.0, 51.0]]), ('A7', [[52.0, 48.0], [49.0, 52.0]]), ('D7', [[55.0, 50.0], [55.0, 56.0]]), ('Ab-7', [[52.0, 54.0], [49.0, 52.0]]), ('Db7', [[52.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Truncate chords to sevenths---\n",
      "Number of sections: 28884\n",
      "Sample section: [('E6', [[52.0], [52.0]]), ('Db7', [[-1.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]]), ('E', [[-1.0], [-1.0]]), ('Db-7', [[44.0], [51.0, 54.0]]), ('Gb-7', [[52.0, 49.0], [52.0]]), ('B7', [[55.0], [54.0, 52.0]]), ('B-7', [[54.0], [52.0]]), ('E7', [[50.0, 54.0], [53.0, 51.0]]), ('A7', [[52.0, 48.0], [49.0, 52.0]]), ('D7', [[55.0, 50.0], [55.0, 56.0]]), ('Ab-7', [[52.0, 54.0], [49.0, 52.0]]), ('Db7', [[52.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Convert melody to integers---\n",
      "Number of sections: 28884\n",
      "Sample section: [('E6', [[4], [4]]), ('Db7', [[-1], [-1]]), ('Gb-7', [[-1], [-1]]), ('B7', [[-1], [-1]]), ('E', [[-1], [-1]]), ('Db-7', [[8], [3, 6]]), ('Gb-7', [[4, 1], [4]]), ('B7', [[7], [6, 4]]), ('B-7', [[6], [4]]), ('E7', [[2, 6], [5, 3]]), ('A7', [[4, 0], [1, 4]]), ('D7', [[7, 2], [7, 8]]), ('Ab-7', [[4, 6], [1, 4]]), ('Db7', [[4], [-1]]), ('Gb-7', [[-1], [-1]]), ('B7', [[-1], [-1]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = harmoutil.load_pickled_data(\"../data/refined_data.pkl\") # lists of (chord label, melody seqs) by sections\n",
    "print(\"---Raw data---\")\n",
    "print(\"Number of sections: {}\".format(len(raw_data)))\n",
    "print(\"Sample section: {}\\n\".format(raw_data[0]))\n",
    "augmented_data = harmoutil.transpose_and_augment_data(raw_data)\n",
    "print(\"---Transpose and augment data---\")\n",
    "print(\"Number of sections after data augmentation: {}\".format(len(augmented_data)))\n",
    "print(\"Sample section: {}\\n\".format(augmented_data[0]))\n",
    "data = [harmoutil.to_sevenths(section) for section in augmented_data]\n",
    "print(\"---Truncate chords to sevenths---\")\n",
    "print(\"Number of sections: {}\".format(len(data)))\n",
    "print(\"Sample section: {}\\n\".format(data[0]))\n",
    "data = [harmoutil.melody_to_octave_range(section) for section in data]\n",
    "print(\"---Convert melody to integers---\")\n",
    "print(\"Number of sections: {}\".format(len(data)))\n",
    "print(\"Sample section: {}\\n\".format(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Remove sections with augmented major chord---\n",
      "Number of sections: 28836\n",
      "\n",
      "Number of sections: 28836 | Sample section chords: ['E6', 'Db7', 'Gb-7', 'B7', 'E', 'Db-7', 'Gb-7', 'B7', 'B-7', 'E7', 'A7', 'D7', 'Ab-7', 'Db7', 'Gb-7', 'B7']\n",
      "Number of chords: 333480 | Sample chord: E6\n",
      "Number of melodies 333480 | Sample melody: [4, 4]\n",
      "Number of melody notes in the data: 2195328 | Sample melody note: 4\n"
     ]
    }
   ],
   "source": [
    "# Create individual chord and melody element lists \n",
    "def get_notes_by_chord(beats):\n",
    "    return [note for beat in beats for note in beat]\n",
    "\n",
    "def get_chords_by_section(section):\n",
    "    return [chord_info[0] for chord_info in section]\n",
    "\n",
    "def check_if_augmented_major(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"+j7\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "data = [section for section in data if not check_if_augmented_major(section)]\n",
    "print(\"---Remove sections with augmented major chord---\")\n",
    "print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in data]\n",
    "chords = [chord_info[0] for section in data for chord_info in section]\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "notes = [note for chord_notes in notes_by_chords for note in chord_notes]\n",
    "# print(sum([len(section) for section in chords_by_sections]))\n",
    "print(\"Number of sections: {} | Sample section chords: {}\".format(len(chords_by_sections), chords_by_sections[0]))\n",
    "print(\"Number of chords: {} | Sample chord: {}\".format(len(chords), chords[0]))\n",
    "print(\"Number of melodies {} | Sample melody: {}\".format(len(notes_by_chords), notes_by_chords[0]))\n",
    "print(\"Number of melody notes in the data: {} | Sample melody note: {}\".format(len(notes), notes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody note to integer mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, '<pad>': 13, -1: 12}\n",
      "\n",
      "Integer to melody note mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: -1, 13: '<pad>'}\n",
      "\n",
      "Chord label to integer mapping:\n",
      " {'Bb-': 42, 'Eb-6': 118, 'Ebm7b5': 124, 'Ab-j7': 15, 'Bb6': 46, 'Asus': 28, 'Fj7': 144, 'G': 150, 'Eo': 131, 'Aj7': 24, 'Dbsus': 97, 'Db+7': 86, 'A': 0, 'G+': 151, 'E-7': 110, 'Bm7b5': 55, 'Ebo': 125, 'Bo': 56, 'Gb6': 166, 'C-7': 65, 'Bbo': 50, 'D6': 82, 'Bb7': 47, 'B+': 31, 'D': 75, 'Bb-j7': 45, 'Gb7': 167, 'G7': 158, 'Gsus': 178, 'Bb-7': 44, 'Db-7': 89, 'Ao7': 27, 'Ao': 26, 'Gb-6': 163, 'G-6': 154, 'Dsus7': 104, 'F+7': 137, 'F-7': 140, 'D-6': 79, 'Cm7b5': 70, 'Gbm7b5': 169, 'Bsus7': 59, 'E-6': 109, 'Esus7': 134, 'B-7': 35, 'Eb+': 115, 'Gbo7': 171, 'Dbj7': 93, 'Bb+': 40, 'Bj7': 54, 'Am7b5': 25, 'Gj7': 174, 'Ebsus7': 128, 'Abo7': 21, 'Ab-6': 13, 'Abo': 20, 'B': 30, 'Bb-6': 43, 'Absus7': 23, 'C+7': 62, 'Db-6': 88, 'Fo7': 147, 'Gm7b5': 175, 'Co7': 72, 'Ab': 9, 'Abj7': 18, 'Gsus7': 179, 'Gbj7': 168, 'Bbo7': 51, 'Gb-': 162, 'B-': 33, 'Gb+': 160, 'Ab7': 17, 'Bo7': 57, 'E+': 106, 'Db+': 85, 'F7': 143, 'Ebj7': 123, 'G6': 157, 'C7': 68, 'E-j7': 111, 'Bbj7': 48, 'A+7': 2, 'D+7': 77, 'Ej7': 129, 'Go7': 177, 'A+': 1, 'G+7': 152, 'Eb-': 117, 'C': 60, 'Dsus': 103, 'E6': 112, '<bos>': 181, 'E7': 113, 'Dbm7b5': 94, 'Fsus': 148, 'Bb': 39, 'F-6': 139, 'Ab+7': 11, 'Ebo7': 126, 'C6': 67, 'Cj7': 69, 'Fm7b5': 145, 'Dj7': 99, 'Eb-7': 119, 'Em7b5': 130, 'Eo7': 132, 'Csus': 73, 'Asus7': 29, 'Bsus': 58, 'Db7': 92, 'Gb-7': 164, 'Do': 101, 'Ebsus': 127, 'Ab-7': 14, 'Ab-': 12, 'Eb-j7': 120, 'Db': 84, 'A-j7': 6, 'Fsus7': 149, 'F-': 138, 'C-': 63, 'Do7': 102, 'F6': 142, 'D7': 83, 'Dbo7': 96, 'Db-j7': 90, 'C+': 61, 'Dm7b5': 100, 'Dbsus7': 98, 'Bb+7': 41, 'B7': 38, 'A-7': 5, 'Absus': 22, 'Gbo': 170, 'F': 135, 'A-': 3, 'Eb+7': 116, 'Co': 71, 'E': 105, 'Gbsus7': 173, 'Db-': 87, 'Go': 176, 'C-j7': 66, 'B-6': 34, 'Ab6': 16, 'NC': 180, 'Dbo': 95, 'B+7': 32, 'A7': 8, 'G-': 153, 'Gb+7': 161, 'Db6': 91, 'F+': 136, 'Ab+': 10, 'C-6': 64, 'E+7': 107, 'Bbsus': 52, 'Esus': 133, 'Eb7': 122, 'D-7': 80, 'D-j7': 81, 'Gb-j7': 165, 'F-j7': 141, 'A6': 7, 'Bbsus7': 53, 'G-j7': 156, 'E-': 108, 'Gbsus': 172, 'D-': 78, 'Gb': 159, 'Eb6': 121, 'G-7': 155, 'A-6': 4, 'Eb': 114, 'Csus7': 74, 'Abm7b5': 19, 'D+': 76, 'Fo': 146, 'B6': 37, 'B-j7': 36, 'Bbm7b5': 49}\n",
      "\n",
      "Integer to chord label mapping:\n",
      " {0: 'A', 1: 'A+', 2: 'A+7', 3: 'A-', 4: 'A-6', 5: 'A-7', 6: 'A-j7', 7: 'A6', 8: 'A7', 9: 'Ab', 10: 'Ab+', 11: 'Ab+7', 12: 'Ab-', 13: 'Ab-6', 14: 'Ab-7', 15: 'Ab-j7', 16: 'Ab6', 17: 'Ab7', 18: 'Abj7', 19: 'Abm7b5', 20: 'Abo', 21: 'Abo7', 22: 'Absus', 23: 'Absus7', 24: 'Aj7', 25: 'Am7b5', 26: 'Ao', 27: 'Ao7', 28: 'Asus', 29: 'Asus7', 30: 'B', 31: 'B+', 32: 'B+7', 33: 'B-', 34: 'B-6', 35: 'B-7', 36: 'B-j7', 37: 'B6', 38: 'B7', 39: 'Bb', 40: 'Bb+', 41: 'Bb+7', 42: 'Bb-', 43: 'Bb-6', 44: 'Bb-7', 45: 'Bb-j7', 46: 'Bb6', 47: 'Bb7', 48: 'Bbj7', 49: 'Bbm7b5', 50: 'Bbo', 51: 'Bbo7', 52: 'Bbsus', 53: 'Bbsus7', 54: 'Bj7', 55: 'Bm7b5', 56: 'Bo', 57: 'Bo7', 58: 'Bsus', 59: 'Bsus7', 60: 'C', 61: 'C+', 62: 'C+7', 63: 'C-', 64: 'C-6', 65: 'C-7', 66: 'C-j7', 67: 'C6', 68: 'C7', 69: 'Cj7', 70: 'Cm7b5', 71: 'Co', 72: 'Co7', 73: 'Csus', 74: 'Csus7', 75: 'D', 76: 'D+', 77: 'D+7', 78: 'D-', 79: 'D-6', 80: 'D-7', 81: 'D-j7', 82: 'D6', 83: 'D7', 84: 'Db', 85: 'Db+', 86: 'Db+7', 87: 'Db-', 88: 'Db-6', 89: 'Db-7', 90: 'Db-j7', 91: 'Db6', 92: 'Db7', 93: 'Dbj7', 94: 'Dbm7b5', 95: 'Dbo', 96: 'Dbo7', 97: 'Dbsus', 98: 'Dbsus7', 99: 'Dj7', 100: 'Dm7b5', 101: 'Do', 102: 'Do7', 103: 'Dsus', 104: 'Dsus7', 105: 'E', 106: 'E+', 107: 'E+7', 108: 'E-', 109: 'E-6', 110: 'E-7', 111: 'E-j7', 112: 'E6', 113: 'E7', 114: 'Eb', 115: 'Eb+', 116: 'Eb+7', 117: 'Eb-', 118: 'Eb-6', 119: 'Eb-7', 120: 'Eb-j7', 121: 'Eb6', 122: 'Eb7', 123: 'Ebj7', 124: 'Ebm7b5', 125: 'Ebo', 126: 'Ebo7', 127: 'Ebsus', 128: 'Ebsus7', 129: 'Ej7', 130: 'Em7b5', 131: 'Eo', 132: 'Eo7', 133: 'Esus', 134: 'Esus7', 135: 'F', 136: 'F+', 137: 'F+7', 138: 'F-', 139: 'F-6', 140: 'F-7', 141: 'F-j7', 142: 'F6', 143: 'F7', 144: 'Fj7', 145: 'Fm7b5', 146: 'Fo', 147: 'Fo7', 148: 'Fsus', 149: 'Fsus7', 150: 'G', 151: 'G+', 152: 'G+7', 153: 'G-', 154: 'G-6', 155: 'G-7', 156: 'G-j7', 157: 'G6', 158: 'G7', 159: 'Gb', 160: 'Gb+', 161: 'Gb+7', 162: 'Gb-', 163: 'Gb-6', 164: 'Gb-7', 165: 'Gb-j7', 166: 'Gb6', 167: 'Gb7', 168: 'Gbj7', 169: 'Gbm7b5', 170: 'Gbo', 171: 'Gbo7', 172: 'Gbsus', 173: 'Gbsus7', 174: 'Gj7', 175: 'Gm7b5', 176: 'Go', 177: 'Go7', 178: 'Gsus', 179: 'Gsus7', 180: 'NC', 181: '<bos>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create categorical data mappings\n",
    "\n",
    "# Note to integer index\n",
    "note_to_int = dict([(c, i) for i, c in enumerate(sorted(list(set(notes)))[1:])])\n",
    "note_to_int[-1] = len(note_to_int)\n",
    "note_to_int['<pad>'] = len(note_to_int)\n",
    "print(\"Melody note to integer mapping:\\n {}\\n\".format(note_to_int))\n",
    "\n",
    "# Integer to note\n",
    "int_to_note = dict([(k, v) for v, k in note_to_int.items()])\n",
    "print(\"Integer to melody note mapping:\\n {}\\n\".format(int_to_note))\n",
    "\n",
    "# Chord to integer index\n",
    "chord_to_int = dict([(c, i) for i, c in enumerate(sorted(list(set(chords))))])\n",
    "chord_to_int['<bos>'] = len(chord_to_int)\n",
    "print(\"Chord label to integer mapping:\\n {}\\n\".format(chord_to_int))\n",
    "\n",
    "\n",
    "# Integer to chord index\n",
    "int_to_chord = dict([(k, v) for v, k in chord_to_int.items()])\n",
    "print(\"Integer to chord label mapping:\\n {}\\n\".format(int_to_chord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 333480\n",
      "Number of distinct melody notes: 14\n",
      "Number of distinct chord labels: 182\n",
      "Maximum melody sequence length: 135\n",
      "Fixed context chord sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "# Define numerical variables\n",
    "\n",
    "n_samples = len(chords)\n",
    "n_notes = len(note_to_int)\n",
    "n_chords = len(chord_to_int)\n",
    "max_melody_len = max([len(mel_seq) for mel_seq in notes_by_chords])\n",
    "chord_context_len = 7\n",
    "\n",
    "print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Number of distinct melody notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct chord labels: {}\".format(n_chords))\n",
    "print(\"Maximum melody sequence length: {}\".format(max_melody_len))\n",
    "print(\"Fixed context chord sequence length: {}\".format(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input melody sequence: [8, 3, 6, '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample input chord sequence: ['<bos>', '<bos>', 'E6', 'Db7', 'Gb-7', 'B7', 'E']\n",
      "\n",
      "Sample target chord: Db-7\n",
      "\n",
      "Input melody: 333480, Input chords: 333480, Target chords: 333480\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensor data\n",
    "\n",
    "def pad_melody(melody, max_len):\n",
    "    return melody + (max_len-len(melody))*['<pad>']\n",
    "\n",
    "def build_input_chord_sequences(chord_seq, context_len):\n",
    "    padded_sequence = context_len*['<bos>'] + chord_seq\n",
    "    formatted_sequences = [padded_sequence[i:i+context_len+1] for i in range(len(chord_seq))]\n",
    "    return formatted_sequences\n",
    "\n",
    "# Melody\n",
    "input_melody_data = [pad_melody(melody, max_melody_len) for melody in notes_by_chords]\n",
    "print(\"Sample input melody sequence: {}\\n\".format(input_melody_data[5]))\n",
    "\n",
    "\n",
    "# Chords\n",
    "formatted_chords_data = []\n",
    "for section_chords in chords_by_sections:\n",
    "    formatted_chords_data += build_input_chord_sequences(section_chords, chord_context_len)\n",
    "\n",
    "input_chords_data = [ch[:-1] for ch in formatted_chords_data]\n",
    "target_chords_data = [ch[-1] for ch in formatted_chords_data]\n",
    "print(\"Sample input chord sequence: {}\\n\".format(input_chords_data[5]))\n",
    "print(\"Sample target chord: {}\\n\".format(target_chords_data[5]))\n",
    "\n",
    "print(\"Input melody: {}, Input chords: {}, Target chords: {}\".format(len(input_melody_data), len(input_chords_data), len(target_chords_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tensors\n",
    "\n",
    "X_melody = np.zeros((n_samples, max_melody_len, n_notes), dtype='float32')\n",
    "X_chords = np.zeros((n_samples, chord_context_len, n_chords), dtype='float32')\n",
    "Y = np.zeros((n_samples, n_chords), dtype='float32')\n",
    "\n",
    "for i, (input_mel, input_ch, target_ch) in enumerate(zip(input_melody_data, input_chords_data, target_chords_data)):\n",
    "    Y[i, chord_to_int[target_ch]] = 1\n",
    "    for j, chord in enumerate(input_ch):\n",
    "        X_chords[i, j, chord_to_int[chord]] = 1\n",
    "        \n",
    "    for j, note in enumerate(input_mel):\n",
    "        X_melody[i, j, note_to_int[note]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333480, 135, 14) = (n_samples, max_melody_len, n_notes)\n",
      "(True, 45019800, 45019800.0, 44982130.0, 45019800)\n"
     ]
    }
   ],
   "source": [
    "# Test melody tensor\n",
    "\n",
    "def test_samples_axis(melody_tensor):\n",
    "    count = 0\n",
    "    sample_axis_sums = melody_tensor.sum(axis=2)\n",
    "    for entry in sample_axis_sums.ravel():\n",
    "        count += 1\n",
    "        if not (entry == 1):\n",
    "            return (False, count)\n",
    "    return (True, count, np.sum(sample_axis_sums), np.sum(melody_tensor), np.sum(melody_tensor, dtype=np.int32))\n",
    "\n",
    "# Test n_samples axis i.e. axis 1. If there are any \"holes\" (non-1) entry, it's a problem\n",
    "print(\"{} = (n_samples, max_melody_len, n_notes)\".format(X_melody.shape))\n",
    "print(test_samples_axis(X_melody))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into 80%-10%-10% train-valid-test\n",
    "\n",
    "seed = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "\n",
    "for train_index, aux_index in sss.split(X_chords, Y):\n",
    "    X_melody_train, X_melody_aux = X_melody[train_index], X_melody[aux_index]\n",
    "    X_chords_train, X_chords_aux = X_chords[train_index], X_chords[aux_index]\n",
    "    Y_train, Y_aux = Y[train_index], Y[aux_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "\n",
    "for valid_index, test_index in sss.split(X_chords_aux, Y_aux):\n",
    "    X_melody_valid, X_melody_test = X_melody[valid_index], X_melody[test_index]\n",
    "    X_chords_valid, X_chords_test = X_chords[valid_index], X_chords[test_index]\n",
    "    Y_valid, Y_test = Y[valid_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 135, 14)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 7, 182)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           54912       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 128)           119424      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           gru_1[0][0]                      \n",
      "                                                                   gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 182)           46774       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 182)           33306       dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 254,416\n",
      "Trainable params: 254,416\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 276.00 337.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 272,-333 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140392927853088 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140392927853088</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 125,-328.5 125,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-306.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140392927852864 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140392927852864</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-219.5 30.5,-255.5 114.5,-255.5 114.5,-219.5 30.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72.5\" y=\"-233.8\">gru_1: GRU</text>\n",
       "</g>\n",
       "<!-- 140392927853088&#45;&gt;140392927852864 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140392927853088-&gt;140392927852864</title>\n",
       "<path d=\"M64.9207,-292.313C66.0508,-284.289 67.4229,-274.547 68.6874,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.1726,-265.919 70.1016,-255.529 65.241,-264.943 72.1726,-265.919\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140392927853032 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140392927853032</title>\n",
       "<polygon fill=\"none\" points=\"143,-292.5 143,-328.5 268,-328.5 268,-292.5 143,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-306.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140393153868632 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140393153868632</title>\n",
       "<polygon fill=\"none\" points=\"152.5,-219.5 152.5,-255.5 236.5,-255.5 236.5,-219.5 152.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-233.8\">gru_2: GRU</text>\n",
       "</g>\n",
       "<!-- 140392927853032&#45;&gt;140393153868632 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140392927853032-&gt;140393153868632</title>\n",
       "<path d=\"M202.837,-292.313C201.594,-284.289 200.085,-274.547 198.694,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.128,-264.875 197.138,-255.529 195.211,-265.947 202.128,-264.875\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140393153868576 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140393153868576</title>\n",
       "<polygon fill=\"none\" points=\"49.5,-146.5 49.5,-182.5 217.5,-182.5 217.5,-146.5 49.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-160.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140392927852864&#45;&gt;140393153868576 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140392927852864-&gt;140393153868576</title>\n",
       "<path d=\"M87.2664,-219.313C94.7591,-210.592 103.996,-199.84 112.239,-190.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"115.008,-192.395 118.87,-182.529 109.698,-187.833 115.008,-192.395\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140393153868632&#45;&gt;140393153868576 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140393153868632-&gt;140393153868576</title>\n",
       "<path d=\"M179.734,-219.313C172.241,-210.592 163.004,-199.84 154.761,-190.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"157.302,-187.833 148.13,-182.529 151.992,-192.395 157.302,-187.833\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140387768547648 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140387768547648</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-73.5 82.5,-109.5 184.5,-109.5 184.5,-73.5 82.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140393153868576&#45;&gt;140387768547648 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140393153868576-&gt;140387768547648</title>\n",
       "<path d=\"M133.5,-146.313C133.5,-138.289 133.5,-128.547 133.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-119.529 133.5,-109.529 130,-119.529 137,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140393153869808 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140393153869808</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-0.5 82.5,-36.5 184.5,-36.5 184.5,-0.5 82.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140387768547648&#45;&gt;140393153869808 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140387768547648-&gt;140393153869808</title>\n",
       "<path d=\"M133.5,-73.3129C133.5,-65.2895 133.5,-55.5475 133.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-46.5288 133.5,-36.5288 130,-46.5289 137,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define neural net architecture\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "melody_input = Input(shape=(max_melody_len, n_notes))\n",
    "melody_gru = GRU(latent_dim)(melody_input)\n",
    "\n",
    "chords_input = Input(shape=(chord_context_len, n_chords))\n",
    "chords_gru = GRU(latent_dim)(chords_input)\n",
    "\n",
    "concat = concatenate([melody_gru, chords_gru])\n",
    "\n",
    "chord_hidden1 = Dense(n_chords, activation='relu')(concat)\n",
    "chord_dense = Dense(n_chords, activation='softmax')(chord_hidden1)\n",
    "\n",
    "model = Model([melody_input, chords_input], chord_dense)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce Early-Stopping and Save-Best-Performance callbacks\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min')\n",
    "filepath = \"../models/label-Mel1-Cho1-FC2_150ep.h5\"\n",
    "bp = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266784 samples, validate on 33348 samples\n",
      "Epoch 1/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.9453 - acc: 0.3419Epoch 00000: val_acc improved from -inf to 0.42299, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 2.9452 - acc: 0.3420 - val_loss: 2.5170 - val_acc: 0.4230\n",
      "Epoch 2/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.3936 - acc: 0.4358Epoch 00001: val_acc improved from 0.42299 to 0.46680, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 2.3936 - acc: 0.4358 - val_loss: 2.2925 - val_acc: 0.4668\n",
      "Epoch 3/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.1885 - acc: 0.4738Epoch 00002: val_acc improved from 0.46680 to 0.49844, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 2.1886 - acc: 0.4738 - val_loss: 2.1576 - val_acc: 0.4984\n",
      "Epoch 4/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.0387 - acc: 0.5082Epoch 00003: val_acc improved from 0.49844 to 0.51634, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 2.0387 - acc: 0.5082 - val_loss: 2.0627 - val_acc: 0.5163\n",
      "Epoch 5/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.9128 - acc: 0.5380Epoch 00004: val_acc improved from 0.51634 to 0.54075, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.9129 - acc: 0.5380 - val_loss: 1.9857 - val_acc: 0.5408\n",
      "Epoch 6/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.8075 - acc: 0.5617Epoch 00005: val_acc improved from 0.54075 to 0.56021, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.8075 - acc: 0.5617 - val_loss: 1.9025 - val_acc: 0.5602\n",
      "Epoch 7/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.7126 - acc: 0.5833Epoch 00006: val_acc improved from 0.56021 to 0.56837, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 1.7126 - acc: 0.5833 - val_loss: 1.8627 - val_acc: 0.5684\n",
      "Epoch 8/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.6317 - acc: 0.6027Epoch 00007: val_acc improved from 0.56837 to 0.58168, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.6316 - acc: 0.6027 - val_loss: 1.8069 - val_acc: 0.5817\n",
      "Epoch 9/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.5595 - acc: 0.6200Epoch 00008: val_acc improved from 0.58168 to 0.59428, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.5595 - acc: 0.6200 - val_loss: 1.7634 - val_acc: 0.5943\n",
      "Epoch 10/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.4984 - acc: 0.6343Epoch 00009: val_acc improved from 0.59428 to 0.59851, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.4984 - acc: 0.6343 - val_loss: 1.7424 - val_acc: 0.5985\n",
      "Epoch 11/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.4437 - acc: 0.6469Epoch 00010: val_acc improved from 0.59851 to 0.61026, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.4437 - acc: 0.6469 - val_loss: 1.7016 - val_acc: 0.6103\n",
      "Epoch 12/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.3952 - acc: 0.6581Epoch 00011: val_acc improved from 0.61026 to 0.61674, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.3952 - acc: 0.6581 - val_loss: 1.6730 - val_acc: 0.6167\n",
      "Epoch 13/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.3527 - acc: 0.6673Epoch 00012: val_acc improved from 0.61674 to 0.62106, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.3527 - acc: 0.6673 - val_loss: 1.6562 - val_acc: 0.6211\n",
      "Epoch 14/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.3153 - acc: 0.6763Epoch 00013: val_acc improved from 0.62106 to 0.62789, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.3153 - acc: 0.6763 - val_loss: 1.6354 - val_acc: 0.6279\n",
      "Epoch 15/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2817 - acc: 0.6829Epoch 00014: val_acc improved from 0.62789 to 0.63080, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 438s - loss: 1.2818 - acc: 0.6829 - val_loss: 1.6180 - val_acc: 0.6308\n",
      "Epoch 16/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2526 - acc: 0.6885Epoch 00015: val_acc improved from 0.63080 to 0.63251, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 437s - loss: 1.2526 - acc: 0.6885 - val_loss: 1.6022 - val_acc: 0.6325\n",
      "Epoch 17/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2254 - acc: 0.6936Epoch 00016: val_acc improved from 0.63251 to 0.64418, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.2254 - acc: 0.6936 - val_loss: 1.5842 - val_acc: 0.6442\n",
      "Epoch 18/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2019 - acc: 0.6996Epoch 00017: val_acc improved from 0.64418 to 0.64532, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.2019 - acc: 0.6995 - val_loss: 1.5710 - val_acc: 0.6453\n",
      "Epoch 19/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1808 - acc: 0.7038Epoch 00018: val_acc improved from 0.64532 to 0.65632, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.1808 - acc: 0.7038 - val_loss: 1.5544 - val_acc: 0.6563\n",
      "Epoch 20/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1596 - acc: 0.7072Epoch 00019: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 1.1596 - acc: 0.7072 - val_loss: 1.5505 - val_acc: 0.6481\n",
      "Epoch 21/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1403 - acc: 0.7123Epoch 00020: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 1.1403 - acc: 0.7123 - val_loss: 1.5405 - val_acc: 0.6549\n",
      "Epoch 22/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1243 - acc: 0.7155Epoch 00021: val_acc improved from 0.65632 to 0.65944, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 437s - loss: 1.1242 - acc: 0.7155 - val_loss: 1.5260 - val_acc: 0.6594\n",
      "Epoch 23/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1092 - acc: 0.7183Epoch 00022: val_acc improved from 0.65944 to 0.66067, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 1.1092 - acc: 0.7183 - val_loss: 1.5227 - val_acc: 0.6607\n",
      "Epoch 24/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0943 - acc: 0.7211Epoch 00023: val_acc improved from 0.66067 to 0.66769, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 1.0943 - acc: 0.7211 - val_loss: 1.5117 - val_acc: 0.6677\n",
      "Epoch 25/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0820 - acc: 0.7242Epoch 00024: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 1.0821 - acc: 0.7242 - val_loss: 1.5071 - val_acc: 0.6660\n",
      "Epoch 26/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.7264Epoch 00025: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266784/266784 [==============================] - 434s - loss: 1.0707 - acc: 0.7264 - val_loss: 1.5061 - val_acc: 0.6675\n",
      "Epoch 27/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0599 - acc: 0.7278Epoch 00026: val_acc improved from 0.66769 to 0.66808, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.0599 - acc: 0.7278 - val_loss: 1.4992 - val_acc: 0.6681\n",
      "Epoch 28/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0494 - acc: 0.7299Epoch 00027: val_acc improved from 0.66808 to 0.67227, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.0494 - acc: 0.7299 - val_loss: 1.4881 - val_acc: 0.6723\n",
      "Epoch 29/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0411 - acc: 0.7313Epoch 00028: val_acc improved from 0.67227 to 0.67536, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.0411 - acc: 0.7313 - val_loss: 1.4896 - val_acc: 0.6754\n",
      "Epoch 30/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.7334Epoch 00029: val_acc improved from 0.67536 to 0.67746, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 1.0317 - acc: 0.7334 - val_loss: 1.4885 - val_acc: 0.6775\n",
      "Epoch 31/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0238 - acc: 0.7348Epoch 00030: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 1.0239 - acc: 0.7348 - val_loss: 1.4807 - val_acc: 0.6770\n",
      "Epoch 32/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0167 - acc: 0.7369Epoch 00031: val_acc improved from 0.67746 to 0.67860, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.0166 - acc: 0.7369 - val_loss: 1.4793 - val_acc: 0.6786\n",
      "Epoch 33/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0097 - acc: 0.7373Epoch 00032: val_acc improved from 0.67860 to 0.67938, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.0097 - acc: 0.7373 - val_loss: 1.4797 - val_acc: 0.6794\n",
      "Epoch 34/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0024 - acc: 0.7391Epoch 00033: val_acc improved from 0.67938 to 0.67947, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.0024 - acc: 0.7391 - val_loss: 1.4722 - val_acc: 0.6795\n",
      "Epoch 35/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9972 - acc: 0.7409Epoch 00034: val_acc improved from 0.67947 to 0.68178, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9972 - acc: 0.7409 - val_loss: 1.4708 - val_acc: 0.6818\n",
      "Epoch 36/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9915 - acc: 0.7413Epoch 00035: val_acc improved from 0.68178 to 0.68319, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9915 - acc: 0.7413 - val_loss: 1.4700 - val_acc: 0.6832\n",
      "Epoch 37/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9852 - acc: 0.7424Epoch 00036: val_acc improved from 0.68319 to 0.68529, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9852 - acc: 0.7424 - val_loss: 1.4710 - val_acc: 0.6853\n",
      "Epoch 38/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9802 - acc: 0.7432Epoch 00037: val_acc improved from 0.68529 to 0.68586, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9803 - acc: 0.7432 - val_loss: 1.4606 - val_acc: 0.6859\n",
      "Epoch 39/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.7442Epoch 00038: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9766 - acc: 0.7442 - val_loss: 1.4577 - val_acc: 0.6855\n",
      "Epoch 40/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.7452Epoch 00039: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9713 - acc: 0.7452 - val_loss: 1.4557 - val_acc: 0.6835\n",
      "Epoch 41/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9685 - acc: 0.7450Epoch 00040: val_acc improved from 0.68586 to 0.68664, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9685 - acc: 0.7450 - val_loss: 1.4599 - val_acc: 0.6866\n",
      "Epoch 42/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9638 - acc: 0.7460Epoch 00041: val_acc improved from 0.68664 to 0.68736, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 0.9637 - acc: 0.7460 - val_loss: 1.4626 - val_acc: 0.6874\n",
      "Epoch 43/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9604 - acc: 0.7464Epoch 00042: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9604 - acc: 0.7464 - val_loss: 1.4665 - val_acc: 0.6869\n",
      "Epoch 44/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.7478Epoch 00043: val_acc improved from 0.68736 to 0.69072, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9569 - acc: 0.7478 - val_loss: 1.4554 - val_acc: 0.6907\n",
      "Epoch 45/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9530 - acc: 0.7490Epoch 00044: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9531 - acc: 0.7490 - val_loss: 1.4531 - val_acc: 0.6904\n",
      "Epoch 46/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.7484Epoch 00045: val_acc improved from 0.69072 to 0.69506, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 0.9508 - acc: 0.7484 - val_loss: 1.4496 - val_acc: 0.6951\n",
      "Epoch 47/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.7497Epoch 00046: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9472 - acc: 0.7497 - val_loss: 1.4573 - val_acc: 0.6913\n",
      "Epoch 48/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9449 - acc: 0.7494Epoch 00047: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9450 - acc: 0.7494 - val_loss: 1.4496 - val_acc: 0.6925\n",
      "Epoch 49/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9426 - acc: 0.7505Epoch 00048: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9425 - acc: 0.7506 - val_loss: 1.4575 - val_acc: 0.6901\n",
      "Epoch 50/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9405 - acc: 0.7507Epoch 00049: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9405 - acc: 0.7507 - val_loss: 1.4526 - val_acc: 0.6939\n",
      "Epoch 51/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9381 - acc: 0.7512Epoch 00050: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9381 - acc: 0.7512 - val_loss: 1.4468 - val_acc: 0.6946\n",
      "Epoch 52/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9470 - acc: 0.7487Epoch 00051: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9471 - acc: 0.7487 - val_loss: 1.4614 - val_acc: 0.6945\n",
      "Epoch 53/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9369 - acc: 0.7518Epoch 00052: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9369 - acc: 0.7519 - val_loss: 1.4477 - val_acc: 0.6911\n",
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.7530Epoch 00053: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9320 - acc: 0.7530 - val_loss: 1.4636 - val_acc: 0.6914\n",
      "Epoch 55/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9311 - acc: 0.7533Epoch 00054: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9311 - acc: 0.7533 - val_loss: 1.4548 - val_acc: 0.6938\n",
      "Epoch 56/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.7531Epoch 00055: val_acc improved from 0.69506 to 0.69815, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 0.9276 - acc: 0.7531 - val_loss: 1.4457 - val_acc: 0.6982\n",
      "Epoch 57/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.7542Epoch 00056: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9242 - acc: 0.7542 - val_loss: 1.4583 - val_acc: 0.6952\n",
      "Epoch 58/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9222 - acc: 0.7543Epoch 00057: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9222 - acc: 0.7543 - val_loss: 1.4575 - val_acc: 0.6944\n",
      "Epoch 59/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7546Epoch 00058: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9204 - acc: 0.7546 - val_loss: 1.4641 - val_acc: 0.6949\n",
      "Epoch 60/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9186 - acc: 0.7559Epoch 00059: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9186 - acc: 0.7559 - val_loss: 1.4558 - val_acc: 0.6960\n",
      "Epoch 61/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9169 - acc: 0.7563Epoch 00060: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9169 - acc: 0.7563 - val_loss: 1.4510 - val_acc: 0.6951\n",
      "Epoch 62/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9159 - acc: 0.7559Epoch 00061: val_acc improved from 0.69815 to 0.70046, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 0.9160 - acc: 0.7559 - val_loss: 1.4495 - val_acc: 0.7005\n",
      "Epoch 63/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9151 - acc: 0.7568Epoch 00062: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9150 - acc: 0.7568 - val_loss: 1.4605 - val_acc: 0.6957\n",
      "Epoch 64/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9124 - acc: 0.7567Epoch 00063: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9124 - acc: 0.7567 - val_loss: 1.4570 - val_acc: 0.6954\n",
      "Epoch 65/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9116 - acc: 0.7574Epoch 00064: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9116 - acc: 0.7574 - val_loss: 1.4568 - val_acc: 0.6980\n",
      "Epoch 66/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9125 - acc: 0.7563Epoch 00065: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9126 - acc: 0.7563 - val_loss: 1.4550 - val_acc: 0.6976\n",
      "Epoch 67/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9115 - acc: 0.7565Epoch 00066: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9115 - acc: 0.7565 - val_loss: 1.4609 - val_acc: 0.6947\n",
      "Epoch 68/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9096 - acc: 0.7573Epoch 00067: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9096 - acc: 0.7573 - val_loss: 1.4636 - val_acc: 0.6953\n",
      "Epoch 69/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9099 - acc: 0.7568Epoch 00068: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9099 - acc: 0.7568 - val_loss: 1.4604 - val_acc: 0.6972\n",
      "Epoch 70/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9088 - acc: 0.7573Epoch 00069: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9088 - acc: 0.7573 - val_loss: 1.4564 - val_acc: 0.6964\n",
      "Epoch 71/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9080 - acc: 0.7577Epoch 00070: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9080 - acc: 0.7577 - val_loss: 1.4520 - val_acc: 0.6996\n",
      "Epoch 72/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9060 - acc: 0.7575Epoch 00071: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9060 - acc: 0.7575 - val_loss: 1.4561 - val_acc: 0.6966\n",
      "Epoch 73/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9062 - acc: 0.7576Epoch 00072: val_acc improved from 0.70046 to 0.70241, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 431s - loss: 0.9062 - acc: 0.7576 - val_loss: 1.4485 - val_acc: 0.7024\n",
      "Epoch 74/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9063 - acc: 0.7575Epoch 00073: val_acc did not improve\n",
      "266784/266784 [==============================] - 431s - loss: 0.9064 - acc: 0.7575 - val_loss: 1.4439 - val_acc: 0.7018\n",
      "Epoch 75/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9048 - acc: 0.7579Epoch 00074: val_acc did not improve\n",
      "266784/266784 [==============================] - 431s - loss: 0.9048 - acc: 0.7579 - val_loss: 1.4494 - val_acc: 0.6970\n",
      "Epoch 76/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7579Epoch 00075: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.9046 - acc: 0.7579 - val_loss: 1.4548 - val_acc: 0.6982\n",
      "Epoch 77/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9051 - acc: 0.7573Epoch 00076: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.9050 - acc: 0.7573 - val_loss: 1.4509 - val_acc: 0.6985\n",
      "Epoch 78/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9023 - acc: 0.7584Epoch 00077: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9023 - acc: 0.7584 - val_loss: 1.4510 - val_acc: 0.6990\n",
      "Epoch 79/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9025 - acc: 0.7585Epoch 00078: val_acc improved from 0.70241 to 0.70286, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 0.9025 - acc: 0.7585 - val_loss: 1.4485 - val_acc: 0.7029\n",
      "Epoch 80/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7584Epoch 00079: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.9012 - acc: 0.7584 - val_loss: 1.4498 - val_acc: 0.6999\n",
      "Epoch 81/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7586Epoch 00080: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9008 - acc: 0.7586 - val_loss: 1.4553 - val_acc: 0.6994\n",
      "Epoch 82/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8990 - acc: 0.7591Epoch 00081: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8990 - acc: 0.7591 - val_loss: 1.4516 - val_acc: 0.6992\n",
      "Epoch 83/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8998 - acc: 0.7584Epoch 00082: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8998 - acc: 0.7584 - val_loss: 1.4535 - val_acc: 0.7013\n",
      "Epoch 84/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.7586Epoch 00083: val_acc improved from 0.70286 to 0.70583, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266784/266784 [==============================] - 433s - loss: 0.8988 - acc: 0.7586 - val_loss: 1.4474 - val_acc: 0.7058\n",
      "Epoch 85/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8986 - acc: 0.7593Epoch 00084: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8986 - acc: 0.7593 - val_loss: 1.4527 - val_acc: 0.6995\n",
      "Epoch 86/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8973 - acc: 0.7590Epoch 00085: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8973 - acc: 0.7590 - val_loss: 1.4578 - val_acc: 0.6970\n",
      "Epoch 87/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8972 - acc: 0.7592Epoch 00086: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8973 - acc: 0.7592 - val_loss: 1.4508 - val_acc: 0.7035\n",
      "Epoch 88/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8952 - acc: 0.7597Epoch 00087: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8953 - acc: 0.7597 - val_loss: 1.4586 - val_acc: 0.7006\n",
      "Epoch 89/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8964 - acc: 0.7594Epoch 00088: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8964 - acc: 0.7594 - val_loss: 1.4599 - val_acc: 0.6960\n",
      "Epoch 90/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8946 - acc: 0.7598Epoch 00089: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8946 - acc: 0.7598 - val_loss: 1.4571 - val_acc: 0.6983\n",
      "Epoch 91/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8949 - acc: 0.7597Epoch 00090: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8949 - acc: 0.7597 - val_loss: 1.4570 - val_acc: 0.7040\n",
      "Epoch 92/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8939 - acc: 0.7597Epoch 00091: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8940 - acc: 0.7597 - val_loss: 1.4589 - val_acc: 0.6963\n",
      "Epoch 93/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7596Epoch 00092: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8930 - acc: 0.7596 - val_loss: 1.4665 - val_acc: 0.7005\n",
      "Epoch 94/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8917 - acc: 0.7601Epoch 00093: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8918 - acc: 0.7601 - val_loss: 1.4521 - val_acc: 0.7037\n",
      "Epoch 95/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7600Epoch 00094: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8930 - acc: 0.7600 - val_loss: 1.4583 - val_acc: 0.7020\n",
      "Epoch 96/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7600Epoch 00095: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8924 - acc: 0.7600 - val_loss: 1.4574 - val_acc: 0.7011\n",
      "Epoch 97/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8903 - acc: 0.7607Epoch 00096: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8903 - acc: 0.7607 - val_loss: 1.4574 - val_acc: 0.7015\n",
      "Epoch 98/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8910 - acc: 0.7601Epoch 00097: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8911 - acc: 0.7601 - val_loss: 1.4590 - val_acc: 0.7000\n",
      "Epoch 99/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7598Epoch 00098: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8912 - acc: 0.7598 - val_loss: 1.4625 - val_acc: 0.6996\n",
      "Epoch 100/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8903 - acc: 0.7603Epoch 00099: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8903 - acc: 0.7603 - val_loss: 1.4594 - val_acc: 0.7013\n",
      "Epoch 101/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8898 - acc: 0.7604Epoch 00100: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8898 - acc: 0.7604 - val_loss: 1.4450 - val_acc: 0.7044\n",
      "Epoch 102/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8888 - acc: 0.7605Epoch 00101: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8888 - acc: 0.7605 - val_loss: 1.4600 - val_acc: 0.6996\n",
      "Epoch 103/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.7604Epoch 00102: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8895 - acc: 0.7604 - val_loss: 1.4574 - val_acc: 0.6952\n",
      "Epoch 104/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.7606Epoch 00103: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8879 - acc: 0.7606 - val_loss: 1.4647 - val_acc: 0.6993\n",
      "Epoch 105/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7601Epoch 00104: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8880 - acc: 0.7601 - val_loss: 1.4618 - val_acc: 0.7003\n",
      "Epoch 106/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8869 - acc: 0.7611Epoch 00105: val_acc did not improve\n",
      "266784/266784 [==============================] - 428s - loss: 0.8869 - acc: 0.7611 - val_loss: 1.4612 - val_acc: 0.7030\n",
      "Epoch 107/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8872 - acc: 0.7611Epoch 00106: val_acc did not improve\n",
      "266784/266784 [==============================] - 429s - loss: 0.8871 - acc: 0.7611 - val_loss: 1.4601 - val_acc: 0.6998\n",
      "Epoch 108/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8856 - acc: 0.7614Epoch 00107: val_acc did not improve\n",
      "266784/266784 [==============================] - 429s - loss: 0.8855 - acc: 0.7614 - val_loss: 1.4602 - val_acc: 0.7005\n",
      "Epoch 109/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8862 - acc: 0.7607Epoch 00108: val_acc did not improve\n",
      "266784/266784 [==============================] - 430s - loss: 0.8862 - acc: 0.7606 - val_loss: 1.4688 - val_acc: 0.6986\n",
      "Epoch 110/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8862 - acc: 0.7613Epoch 00109: val_acc did not improve\n",
      "266784/266784 [==============================] - 430s - loss: 0.8862 - acc: 0.7613 - val_loss: 1.4497 - val_acc: 0.7035\n",
      "Epoch 111/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8846 - acc: 0.7608Epoch 00110: val_acc did not improve\n",
      "266784/266784 [==============================] - 431s - loss: 0.8846 - acc: 0.7608 - val_loss: 1.4604 - val_acc: 0.7002\n",
      "Epoch 112/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8853 - acc: 0.7607Epoch 00111: val_acc did not improve\n",
      "266784/266784 [==============================] - 431s - loss: 0.8853 - acc: 0.7607 - val_loss: 1.4608 - val_acc: 0.7011\n",
      "Epoch 113/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8843 - acc: 0.7610Epoch 00112: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8843 - acc: 0.7610 - val_loss: 1.4579 - val_acc: 0.7037\n",
      "Epoch 114/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.7620Epoch 00113: val_acc improved from 0.70583 to 0.70634, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 441s - loss: 0.8835 - acc: 0.7620 - val_loss: 1.4477 - val_acc: 0.7063\n",
      "Epoch 115/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7610Epoch 00114: val_acc did not improve\n",
      "266784/266784 [==============================] - 442s - loss: 0.8846 - acc: 0.7610 - val_loss: 1.4544 - val_acc: 0.6986\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8844 - acc: 0.7613Epoch 00115: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8844 - acc: 0.7613 - val_loss: 1.4548 - val_acc: 0.7033\n",
      "Epoch 117/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8837 - acc: 0.7617Epoch 00116: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8837 - acc: 0.7617 - val_loss: 1.4555 - val_acc: 0.7041\n",
      "Epoch 118/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8859 - acc: 0.7608Epoch 00117: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8859 - acc: 0.7608 - val_loss: 1.4599 - val_acc: 0.7015\n",
      "Epoch 119/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8830 - acc: 0.7620Epoch 00118: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8830 - acc: 0.7620 - val_loss: 1.4533 - val_acc: 0.7027\n",
      "Epoch 120/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8831 - acc: 0.7617Epoch 00119: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8831 - acc: 0.7616 - val_loss: 1.4649 - val_acc: 0.7020\n",
      "Epoch 121/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8826 - acc: 0.7614Epoch 00120: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8825 - acc: 0.7614 - val_loss: 1.4644 - val_acc: 0.7016\n",
      "Epoch 122/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8817 - acc: 0.7614Epoch 00121: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8816 - acc: 0.7614 - val_loss: 1.4574 - val_acc: 0.7016\n",
      "Epoch 123/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7613Epoch 00122: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8841 - acc: 0.7613 - val_loss: 1.4587 - val_acc: 0.7038\n",
      "Epoch 124/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.7612Epoch 00123: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8875 - acc: 0.7612 - val_loss: 1.4658 - val_acc: 0.7018\n",
      "Epoch 125/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8836 - acc: 0.7620Epoch 00124: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8836 - acc: 0.7621 - val_loss: 1.4657 - val_acc: 0.7002\n",
      "Epoch 126/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7621Epoch 00125: val_acc improved from 0.70634 to 0.70649, saving model to ../models/label-Mel1-Cho1-FC2_150ep.h5\n",
      "266784/266784 [==============================] - 441s - loss: 0.8814 - acc: 0.7621 - val_loss: 1.4556 - val_acc: 0.7065\n",
      "Epoch 127/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7621Epoch 00126: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8814 - acc: 0.7621 - val_loss: 1.4612 - val_acc: 0.7014\n",
      "Epoch 128/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8806 - acc: 0.7622Epoch 00127: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8806 - acc: 0.7622 - val_loss: 1.4561 - val_acc: 0.7019\n",
      "Epoch 129/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.7621Epoch 00128: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8792 - acc: 0.7621 - val_loss: 1.4560 - val_acc: 0.7045\n",
      "Epoch 130/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7631Epoch 00129: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8782 - acc: 0.7631 - val_loss: 1.4686 - val_acc: 0.7037\n",
      "Epoch 131/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7630Epoch 00130: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8783 - acc: 0.7630 - val_loss: 1.4648 - val_acc: 0.7012\n",
      "Epoch 132/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.7629Epoch 00131: val_acc did not improve\n",
      "266784/266784 [==============================] - 442s - loss: 0.8792 - acc: 0.7629 - val_loss: 1.4578 - val_acc: 0.7032\n",
      "Epoch 133/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8830 - acc: 0.7620Epoch 00132: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8830 - acc: 0.7620 - val_loss: 1.4646 - val_acc: 0.7012\n",
      "Epoch 134/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8815 - acc: 0.7620Epoch 00133: val_acc did not improve\n",
      "266784/266784 [==============================] - 442s - loss: 0.8815 - acc: 0.7620 - val_loss: 1.4536 - val_acc: 0.7017\n",
      "Epoch 135/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8798 - acc: 0.7620Epoch 00134: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8798 - acc: 0.7620 - val_loss: 1.4591 - val_acc: 0.7005\n",
      "Epoch 136/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.7623Epoch 00135: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8791 - acc: 0.7623 - val_loss: 1.4601 - val_acc: 0.7028\n",
      "Epoch 137/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.7618Epoch 00136: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8792 - acc: 0.7618 - val_loss: 1.4628 - val_acc: 0.7048\n",
      "Epoch 138/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8891 - acc: 0.7598Epoch 00137: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8891 - acc: 0.7598 - val_loss: 1.4634 - val_acc: 0.6976\n",
      "Epoch 139/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.7622Epoch 00138: val_acc did not improve\n",
      "266784/266784 [==============================] - 437s - loss: 0.8799 - acc: 0.7622 - val_loss: 1.4565 - val_acc: 0.7017\n",
      "Epoch 140/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8768 - acc: 0.7628Epoch 00139: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8768 - acc: 0.7628 - val_loss: 1.4538 - val_acc: 0.7045\n",
      "Epoch 141/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7633Epoch 00140: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8732 - acc: 0.7633 - val_loss: 1.4519 - val_acc: 0.7050\n",
      "Epoch 142/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8733 - acc: 0.7635Epoch 00141: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8732 - acc: 0.7635 - val_loss: 1.4605 - val_acc: 0.7041\n",
      "Epoch 143/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8713 - acc: 0.7641Epoch 00142: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8713 - acc: 0.7641 - val_loss: 1.4608 - val_acc: 0.7030\n",
      "Epoch 144/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8695 - acc: 0.7642Epoch 00143: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8696 - acc: 0.7642 - val_loss: 1.4598 - val_acc: 0.7011\n",
      "Epoch 145/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.7638Epoch 00144: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8703 - acc: 0.7638 - val_loss: 1.4555 - val_acc: 0.7013\n",
      "Epoch 146/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8873 - acc: 0.7608Epoch 00145: val_acc did not improve\n",
      "266784/266784 [==============================] - 439s - loss: 0.8873 - acc: 0.7608 - val_loss: 1.4639 - val_acc: 0.7039\n",
      "Epoch 147/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7637Epoch 00146: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266784/266784 [==============================] - 441s - loss: 0.8744 - acc: 0.7637 - val_loss: 1.4565 - val_acc: 0.7036\n",
      "Epoch 148/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8709 - acc: 0.7642Epoch 00147: val_acc did not improve\n",
      "266784/266784 [==============================] - 440s - loss: 0.8709 - acc: 0.7642 - val_loss: 1.4576 - val_acc: 0.7018\n",
      "Epoch 149/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7653Epoch 00148: val_acc did not improve\n",
      "266784/266784 [==============================] - 442s - loss: 0.8678 - acc: 0.7652 - val_loss: 1.4543 - val_acc: 0.7032\n",
      "Epoch 150/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8661 - acc: 0.7654Epoch 00149: val_acc did not improve\n",
      "266784/266784 [==============================] - 441s - loss: 0.8661 - acc: 0.7654 - val_loss: 1.4682 - val_acc: 0.7027\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit([X_melody_train, X_chords_train], Y_train, epochs=epochs, validation_data=([X_melody_valid, X_chords_valid], Y_valid,), batch_size=batch_size, callbacks=[bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae92f93860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFkCAYAAADWs8tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0245a80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model learning\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Training history of root model\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best performance model\n",
    "model = load_model(\"../models/label-Mel1-Cho1-FC2_150ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord label prediction: 0.6990\n",
      "Kappa score of combined chord label prediction: 0.6911\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of labels\n",
    "\n",
    "# Predict chords from each test sample melody\n",
    "Y_pred = model.predict([X_melody_test, X_chords_test])\n",
    "\n",
    "# Compute accuracy and kappa score\n",
    "print(\"Categorical accuracy of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Kappa score of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_kappa_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord pitch prediction: 0.8728\n",
      "TP: 105082 TN: 244191 FP: 25859 FN: 25044\n",
      "Kappa score of combined chord pitch prediction: 0.7106\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of pitches\n",
    "\n",
    "def label_to_pitch_tensors(predictions):\n",
    "    predicted_chords = [int_to_chord[np.argmax(ch)] for ch in predictions]\n",
    "    pitch_chords = [harmoutil.chord_to_notes(label) for label in predicted_chords]\n",
    "    \n",
    "    Y_pitches = np.zeros((predictions.shape[0], 12), dtype='float32')\n",
    "    for i, chord_pitches in enumerate(pitch_chords):\n",
    "        for j, pitch_presence in enumerate(chord_pitches):\n",
    "            Y_pitches[i, j] = pitch_presence\n",
    "\n",
    "    return Y_pitches\n",
    "    \n",
    "    \n",
    "Y_pred_pitch = label_to_pitch_tensors(Y_pred)\n",
    "Y_test_pitch = label_to_pitch_tensors(Y_test)\n",
    "\n",
    "print(\"Categorical accuracy of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_accuracy_score(Y_test_pitch, Y_pred_pitch)))\n",
    "print(\"Kappa score of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_kappa_score(Y_test_pitch, Y_pred_pitch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"F-score: {0:.4f}\".format(harmoutil.compute_binary_fscore(Y_test_pitch, Y_pred_pitch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
