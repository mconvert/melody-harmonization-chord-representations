{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import harmoutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, concatenate\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Raw data---\n",
      "Number of sections: 2408\n",
      "Sample section: [('Bb6', [[58.0], [58.0]]), ('G7', [[-1.0], [-1.0]]), ('C-7', [[-1.0], [-1.0]]), ('F7', [[-1.0], [-1.0]]), ('Bb', [[-1.0], [-1.0]]), ('G-7', [[50.0], [57.0, 60.0]]), ('C-7', [[58.0, 55.0], [58.0]]), ('F7', [[61.0], [60.0, 58.0]]), ('F-7', [[60.0], [58.0]]), ('Bb7', [[56.0, 60.0], [59.0, 57.0]]), ('Eb7', [[58.0, 54.0], [55.0, 58.0]]), ('Ab7', [[61.0, 56.0], [61.0, 62.0]]), ('D-7', [[58.0, 60.0], [55.0, 58.0]]), ('G7', [[58.0], [-1.0]]), ('C-7', [[-1.0], [-1.0]]), ('F7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Transpose and augment data---\n",
      "Number of sections after data augmentation: 28884\n",
      "Sample section: [('E6', [[52.0], [52.0]]), ('Db7', [[-1.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]]), ('E', [[-1.0], [-1.0]]), ('Db-7', [[44.0], [51.0, 54.0]]), ('Gb-7', [[52.0, 49.0], [52.0]]), ('B7', [[55.0], [54.0, 52.0]]), ('B-7', [[54.0], [52.0]]), ('E7', [[50.0, 54.0], [53.0, 51.0]]), ('A7', [[52.0, 48.0], [49.0, 52.0]]), ('D7', [[55.0, 50.0], [55.0, 56.0]]), ('Ab-7', [[52.0, 54.0], [49.0, 52.0]]), ('Db7', [[52.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Truncate chords to sevenths---\n",
      "Number of sections: 28884\n",
      "Sample section: [('E6', [[52.0], [52.0]]), ('Db7', [[-1.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]]), ('E', [[-1.0], [-1.0]]), ('Db-7', [[44.0], [51.0, 54.0]]), ('Gb-7', [[52.0, 49.0], [52.0]]), ('B7', [[55.0], [54.0, 52.0]]), ('B-7', [[54.0], [52.0]]), ('E7', [[50.0, 54.0], [53.0, 51.0]]), ('A7', [[52.0, 48.0], [49.0, 52.0]]), ('D7', [[55.0, 50.0], [55.0, 56.0]]), ('Ab-7', [[52.0, 54.0], [49.0, 52.0]]), ('Db7', [[52.0], [-1.0]]), ('Gb-7', [[-1.0], [-1.0]]), ('B7', [[-1.0], [-1.0]])]\n",
      "\n",
      "---Convert melody to integers---\n",
      "Number of sections: 28884\n",
      "Sample section: [('E6', [[4], [4]]), ('Db7', [[-1], [-1]]), ('Gb-7', [[-1], [-1]]), ('B7', [[-1], [-1]]), ('E', [[-1], [-1]]), ('Db-7', [[8], [3, 6]]), ('Gb-7', [[4, 1], [4]]), ('B7', [[7], [6, 4]]), ('B-7', [[6], [4]]), ('E7', [[2, 6], [5, 3]]), ('A7', [[4, 0], [1, 4]]), ('D7', [[7, 2], [7, 8]]), ('Ab-7', [[4, 6], [1, 4]]), ('Db7', [[4], [-1]]), ('Gb-7', [[-1], [-1]]), ('B7', [[-1], [-1]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = harmoutil.load_pickled_data(\"../data/refined_data.pkl\") # lists of (chord label, melody seqs) by sections\n",
    "print(\"---Raw data---\")\n",
    "print(\"Number of sections: {}\".format(len(raw_data)))\n",
    "print(\"Sample section: {}\\n\".format(raw_data[0]))\n",
    "augmented_data = harmoutil.transpose_and_augment_data(raw_data)\n",
    "print(\"---Transpose and augment data---\")\n",
    "print(\"Number of sections after data augmentation: {}\".format(len(augmented_data)))\n",
    "print(\"Sample section: {}\\n\".format(augmented_data[0]))\n",
    "data = [harmoutil.to_sevenths(section) for section in augmented_data]\n",
    "print(\"---Truncate chords to sevenths---\")\n",
    "print(\"Number of sections: {}\".format(len(data)))\n",
    "print(\"Sample section: {}\\n\".format(data[0]))\n",
    "data = [harmoutil.melody_to_octave_range(section) for section in data]\n",
    "print(\"---Convert melody to integers---\")\n",
    "print(\"Number of sections: {}\".format(len(data)))\n",
    "print(\"Sample section: {}\\n\".format(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Remove sections with augmented major chord---\n",
      "Number of sections: 28836\n",
      "\n",
      "Number of sections: 28836 | Sample section chords: ['E6', 'Db7', 'Gb-7', 'B7', 'E', 'Db-7', 'Gb-7', 'B7', 'B-7', 'E7', 'A7', 'D7', 'Ab-7', 'Db7', 'Gb-7', 'B7']\n",
      "Number of chords: 333480 | Sample chord: E6\n",
      "Number of melodies 333480 | Sample melody: [4, 4]\n",
      "Number of melody notes in the data: 2195328 | Sample melody note: 4\n"
     ]
    }
   ],
   "source": [
    "# Create individual chord and melody element lists \n",
    "def get_notes_by_chord(beats):\n",
    "    return [note for beat in beats for note in beat]\n",
    "\n",
    "def get_chords_by_section(section):\n",
    "    return [chord_info[0] for chord_info in section]\n",
    "\n",
    "def check_if_augmented_major(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"+j7\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "data = [section for section in data if not check_if_augmented_major(section)]\n",
    "print(\"---Remove sections with augmented major chord---\")\n",
    "print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in data]\n",
    "chords = [chord_info[0] for section in data for chord_info in section]\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "notes = [note for chord_notes in notes_by_chords for note in chord_notes]\n",
    "# print(sum([len(section) for section in chords_by_sections]))\n",
    "print(\"Number of sections: {} | Sample section chords: {}\".format(len(chords_by_sections), chords_by_sections[0]))\n",
    "print(\"Number of chords: {} | Sample chord: {}\".format(len(chords), chords[0]))\n",
    "print(\"Number of melodies {} | Sample melody: {}\".format(len(notes_by_chords), notes_by_chords[0]))\n",
    "print(\"Number of melody notes in the data: {} | Sample melody note: {}\".format(len(notes), notes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody note to integer mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, '<pad>': 13, -1: 12}\n",
      "\n",
      "Integer to melody note mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: -1, 13: '<pad>'}\n",
      "\n",
      "Chord label to integer mapping:\n",
      " {'D': 75, 'Dbm7b5': 94, 'Db-': 87, 'Bbsus7': 53, 'Eb+7': 116, 'B-7': 35, 'D-': 78, 'Dbo': 95, 'Gb+7': 161, 'F7': 143, 'Db': 84, 'Eb-7': 119, 'Dsus': 103, 'Bb7': 47, 'A-7': 5, 'A+7': 2, 'Csus7': 74, 'Eb-': 117, 'A-6': 4, '<bos>': 181, 'G-j7': 156, 'Db-7': 89, 'Bb': 39, 'G-7': 155, 'Eo': 131, 'Go': 176, 'Gbsus': 172, 'A7': 8, 'Bb-j7': 45, 'Ebo': 125, 'A-': 3, 'Ebo7': 126, 'Bbo7': 51, 'Gbsus7': 173, 'Gb-': 162, 'Cj7': 69, 'B': 30, 'Gbo': 170, 'G-': 153, 'Ebsus': 127, 'D+7': 77, 'Absus7': 23, 'D+': 76, 'Asus7': 29, 'B+7': 32, 'Ab-j7': 15, 'Do7': 102, 'Ab+': 10, 'F-7': 140, 'D7': 83, 'Eo7': 132, 'D-6': 79, 'Gbm7b5': 169, 'B7': 38, 'A+': 1, 'E+7': 107, 'E-j7': 111, 'G+7': 152, 'Db+': 85, 'Ab-7': 14, 'Fm7b5': 145, 'D-7': 80, 'Aj7': 24, 'Csus': 73, 'Co7': 72, 'Bb6': 46, 'Asus': 28, 'Ao7': 27, 'Abo7': 21, 'F+7': 137, 'Bbm7b5': 49, 'A-j7': 6, 'Bbo': 50, 'Db6': 91, 'G': 150, 'Gb7': 167, 'Gbo7': 171, 'Eb+': 115, 'Ab+7': 11, 'Gm7b5': 175, 'Gb-6': 163, 'Dbj7': 93, 'Dm7b5': 100, 'B-': 33, 'C-7': 65, 'C': 60, 'Gb+': 160, 'D-j7': 81, 'G6': 157, 'Eb': 114, 'Ab-': 12, 'Dbo7': 96, 'Ab': 9, 'B-6': 34, 'Gsus': 178, 'Bo7': 57, 'Eb6': 121, 'Gb-j7': 165, 'Abj7': 18, 'C-6': 64, 'Ao': 26, 'F+': 136, 'NC': 180, 'E+': 106, 'B6': 37, 'Bo': 56, 'Ej7': 129, 'Db+7': 86, 'D6': 82, 'F6': 142, 'C+7': 62, 'C-': 63, 'Bb+': 40, 'B-j7': 36, 'Ebm7b5': 124, 'Abo': 20, 'Fsus': 148, 'Ab7': 17, 'F-': 138, 'Gbj7': 168, 'Fo7': 147, 'E-7': 110, 'Eb7': 122, 'Bbsus': 52, 'Am7b5': 25, 'Fo': 146, 'B+': 31, 'Db-6': 88, 'Esus7': 134, 'Bb-6': 43, 'Abm7b5': 19, 'Dj7': 99, 'C7': 68, 'E6': 112, 'E-6': 109, 'E-': 108, 'G7': 158, 'A6': 7, 'Gj7': 174, 'Gsus7': 179, 'Gb': 159, 'Absus': 22, 'F-j7': 141, 'Db-j7': 90, 'F': 135, 'G-6': 154, 'Bb-7': 44, 'Bbj7': 48, 'Esus': 133, 'Em7b5': 130, 'G+': 151, 'Db7': 92, 'Dbsus7': 98, 'Ebsus7': 128, 'E7': 113, 'A': 0, 'Fsus7': 149, 'Co': 71, 'Bsus': 58, 'Bm7b5': 55, 'Bb+7': 41, 'Do': 101, 'C6': 67, 'Cm7b5': 70, 'F-6': 139, 'Go7': 177, 'Bj7': 54, 'C-j7': 66, 'Gb-7': 164, 'Dsus7': 104, 'Eb-6': 118, 'Bb-': 42, 'Gb6': 166, 'C+': 61, 'Bsus7': 59, 'Ab-6': 13, 'Ab6': 16, 'Dbsus': 97, 'E': 105, 'Eb-j7': 120, 'Ebj7': 123, 'Fj7': 144}\n",
      "\n",
      "Integer to chord label mapping:\n",
      " {0: 'A', 1: 'A+', 2: 'A+7', 3: 'A-', 4: 'A-6', 5: 'A-7', 6: 'A-j7', 7: 'A6', 8: 'A7', 9: 'Ab', 10: 'Ab+', 11: 'Ab+7', 12: 'Ab-', 13: 'Ab-6', 14: 'Ab-7', 15: 'Ab-j7', 16: 'Ab6', 17: 'Ab7', 18: 'Abj7', 19: 'Abm7b5', 20: 'Abo', 21: 'Abo7', 22: 'Absus', 23: 'Absus7', 24: 'Aj7', 25: 'Am7b5', 26: 'Ao', 27: 'Ao7', 28: 'Asus', 29: 'Asus7', 30: 'B', 31: 'B+', 32: 'B+7', 33: 'B-', 34: 'B-6', 35: 'B-7', 36: 'B-j7', 37: 'B6', 38: 'B7', 39: 'Bb', 40: 'Bb+', 41: 'Bb+7', 42: 'Bb-', 43: 'Bb-6', 44: 'Bb-7', 45: 'Bb-j7', 46: 'Bb6', 47: 'Bb7', 48: 'Bbj7', 49: 'Bbm7b5', 50: 'Bbo', 51: 'Bbo7', 52: 'Bbsus', 53: 'Bbsus7', 54: 'Bj7', 55: 'Bm7b5', 56: 'Bo', 57: 'Bo7', 58: 'Bsus', 59: 'Bsus7', 60: 'C', 61: 'C+', 62: 'C+7', 63: 'C-', 64: 'C-6', 65: 'C-7', 66: 'C-j7', 67: 'C6', 68: 'C7', 69: 'Cj7', 70: 'Cm7b5', 71: 'Co', 72: 'Co7', 73: 'Csus', 74: 'Csus7', 75: 'D', 76: 'D+', 77: 'D+7', 78: 'D-', 79: 'D-6', 80: 'D-7', 81: 'D-j7', 82: 'D6', 83: 'D7', 84: 'Db', 85: 'Db+', 86: 'Db+7', 87: 'Db-', 88: 'Db-6', 89: 'Db-7', 90: 'Db-j7', 91: 'Db6', 92: 'Db7', 93: 'Dbj7', 94: 'Dbm7b5', 95: 'Dbo', 96: 'Dbo7', 97: 'Dbsus', 98: 'Dbsus7', 99: 'Dj7', 100: 'Dm7b5', 101: 'Do', 102: 'Do7', 103: 'Dsus', 104: 'Dsus7', 105: 'E', 106: 'E+', 107: 'E+7', 108: 'E-', 109: 'E-6', 110: 'E-7', 111: 'E-j7', 112: 'E6', 113: 'E7', 114: 'Eb', 115: 'Eb+', 116: 'Eb+7', 117: 'Eb-', 118: 'Eb-6', 119: 'Eb-7', 120: 'Eb-j7', 121: 'Eb6', 122: 'Eb7', 123: 'Ebj7', 124: 'Ebm7b5', 125: 'Ebo', 126: 'Ebo7', 127: 'Ebsus', 128: 'Ebsus7', 129: 'Ej7', 130: 'Em7b5', 131: 'Eo', 132: 'Eo7', 133: 'Esus', 134: 'Esus7', 135: 'F', 136: 'F+', 137: 'F+7', 138: 'F-', 139: 'F-6', 140: 'F-7', 141: 'F-j7', 142: 'F6', 143: 'F7', 144: 'Fj7', 145: 'Fm7b5', 146: 'Fo', 147: 'Fo7', 148: 'Fsus', 149: 'Fsus7', 150: 'G', 151: 'G+', 152: 'G+7', 153: 'G-', 154: 'G-6', 155: 'G-7', 156: 'G-j7', 157: 'G6', 158: 'G7', 159: 'Gb', 160: 'Gb+', 161: 'Gb+7', 162: 'Gb-', 163: 'Gb-6', 164: 'Gb-7', 165: 'Gb-j7', 166: 'Gb6', 167: 'Gb7', 168: 'Gbj7', 169: 'Gbm7b5', 170: 'Gbo', 171: 'Gbo7', 172: 'Gbsus', 173: 'Gbsus7', 174: 'Gj7', 175: 'Gm7b5', 176: 'Go', 177: 'Go7', 178: 'Gsus', 179: 'Gsus7', 180: 'NC', 181: '<bos>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create categorical data mappings\n",
    "\n",
    "# Note to integer index\n",
    "note_to_int = dict([(c, i) for i, c in enumerate(sorted(list(set(notes)))[1:])])\n",
    "note_to_int[-1] = len(note_to_int)\n",
    "note_to_int['<pad>'] = len(note_to_int)\n",
    "print(\"Melody note to integer mapping:\\n {}\\n\".format(note_to_int))\n",
    "\n",
    "# Integer to note\n",
    "int_to_note = dict([(k, v) for v, k in note_to_int.items()])\n",
    "print(\"Integer to melody note mapping:\\n {}\\n\".format(int_to_note))\n",
    "\n",
    "# Chord to integer index\n",
    "chord_to_int = dict([(c, i) for i, c in enumerate(sorted(list(set(chords))))])\n",
    "chord_to_int['<bos>'] = len(chord_to_int)\n",
    "print(\"Chord label to integer mapping:\\n {}\\n\".format(chord_to_int))\n",
    "\n",
    "\n",
    "# Integer to chord index\n",
    "int_to_chord = dict([(k, v) for v, k in chord_to_int.items()])\n",
    "print(\"Integer to chord label mapping:\\n {}\\n\".format(int_to_chord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 333480\n",
      "Number of distinct melody notes: 14\n",
      "Number of distinct chord labels: 182\n",
      "Maximum melody sequence length: 135\n",
      "Fixed context chord sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "# Define numerical variables\n",
    "\n",
    "n_samples = len(chords)\n",
    "n_notes = len(note_to_int)\n",
    "n_chords = len(chord_to_int)\n",
    "max_melody_len = max([len(mel_seq) for mel_seq in notes_by_chords])\n",
    "chord_context_len = 7\n",
    "\n",
    "print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Number of distinct melody notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct chord labels: {}\".format(n_chords))\n",
    "print(\"Maximum melody sequence length: {}\".format(max_melody_len))\n",
    "print(\"Fixed context chord sequence length: {}\".format(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input melody sequence: [8, 3, 6, '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample input chord sequence: ['<bos>', '<bos>', 'E6', 'Db7', 'Gb-7', 'B7', 'E']\n",
      "\n",
      "Sample target chord: Db-7\n",
      "\n",
      "Input melody: 333480, Input chords: 333480, Target chords: 333480\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensor data\n",
    "\n",
    "def pad_melody(melody, max_len):\n",
    "    return melody + (max_len-len(melody))*['<pad>']\n",
    "\n",
    "def build_input_chord_sequences(chord_seq, context_len):\n",
    "    padded_sequence = context_len*['<bos>'] + chord_seq\n",
    "    formatted_sequences = [padded_sequence[i:i+context_len+1] for i in range(len(chord_seq))]\n",
    "    return formatted_sequences\n",
    "\n",
    "# Melody\n",
    "input_melody_data = [pad_melody(melody, max_melody_len) for melody in notes_by_chords]\n",
    "print(\"Sample input melody sequence: {}\\n\".format(input_melody_data[5]))\n",
    "\n",
    "\n",
    "# Chords\n",
    "formatted_chords_data = []\n",
    "for section_chords in chords_by_sections:\n",
    "    formatted_chords_data += build_input_chord_sequences(section_chords, chord_context_len)\n",
    "\n",
    "input_chords_data = [ch[:-1] for ch in formatted_chords_data]\n",
    "target_chords_data = [ch[-1] for ch in formatted_chords_data]\n",
    "print(\"Sample input chord sequence: {}\\n\".format(input_chords_data[5]))\n",
    "print(\"Sample target chord: {}\\n\".format(target_chords_data[5]))\n",
    "\n",
    "print(\"Input melody: {}, Input chords: {}, Target chords: {}\".format(len(input_melody_data), len(input_chords_data), len(target_chords_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tensors\n",
    "\n",
    "X_melody = np.zeros((n_samples, max_melody_len, n_notes), dtype='float32')\n",
    "X_chords = np.zeros((n_samples, chord_context_len, n_chords), dtype='float32')\n",
    "Y = np.zeros((n_samples, n_chords), dtype='float32')\n",
    "\n",
    "for i, (input_mel, input_ch, target_ch) in enumerate(zip(input_melody_data, input_chords_data, target_chords_data)):\n",
    "    Y[i, chord_to_int[target_ch]] = 1\n",
    "    for j, chord in enumerate(input_ch):\n",
    "        X_chords[i, j, chord_to_int[chord]] = 1\n",
    "        \n",
    "    for j, note in enumerate(input_mel):\n",
    "        X_melody[i, j, note_to_int[note]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333480, 135, 14) = (n_samples, max_melody_len, n_notes)\n",
      "(True, 45019800, 45019800.0, 44982130.0, 45019800)\n"
     ]
    }
   ],
   "source": [
    "# Test melody tensor\n",
    "\n",
    "def test_samples_axis(melody_tensor):\n",
    "    count = 0\n",
    "    sample_axis_sums = melody_tensor.sum(axis=2)\n",
    "    for entry in sample_axis_sums.ravel():\n",
    "        count += 1\n",
    "        if not (entry == 1):\n",
    "            return (False, count)\n",
    "    return (True, count, np.sum(sample_axis_sums), np.sum(melody_tensor), np.sum(melody_tensor, dtype=np.int32))\n",
    "\n",
    "# Test n_samples axis i.e. axis 1. If there are any \"holes\" (non-1) entry, it's a problem\n",
    "print(\"{} = (n_samples, max_melody_len, n_notes)\".format(X_melody.shape))\n",
    "print(test_samples_axis(X_melody))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into 80%-10%-10% train-valid-test\n",
    "\n",
    "seed = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "\n",
    "for train_index, aux_index in sss.split(X_chords, Y):\n",
    "    X_melody_train, X_melody_aux = X_melody[train_index], X_melody[aux_index]\n",
    "    X_chords_train, X_chords_aux = X_chords[train_index], X_chords[aux_index]\n",
    "    Y_train, Y_aux = Y[train_index], Y[aux_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "\n",
    "for valid_index, test_index in sss.split(X_chords_aux, Y_aux):\n",
    "    X_melody_valid, X_melody_test = X_melody[valid_index], X_melody[test_index]\n",
    "    X_chords_valid, X_chords_test = X_chords[valid_index], X_chords[test_index]\n",
    "    Y_valid, Y_test = Y[valid_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 135, 14)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 7, 182)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           54912       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 128)           119424      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           gru_1[0][0]                      \n",
      "                                                                   gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 182)           46774       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 182)           33306       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 182)           33306       dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 287,722\n",
      "Trainable params: 287,722\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 276.00 410.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 272,-406 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140005079247392 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140005079247392</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 125,-401.5 125,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-379.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140005211770272 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140005211770272</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-292.5 30.5,-328.5 114.5,-328.5 114.5,-292.5 30.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72.5\" y=\"-306.8\">gru_1: GRU</text>\n",
       "</g>\n",
       "<!-- 140005079247392&#45;&gt;140005211770272 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140005079247392-&gt;140005211770272</title>\n",
       "<path d=\"M64.9207,-365.313C66.0508,-357.289 67.4229,-347.547 68.6874,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.1726,-338.919 70.1016,-328.529 65.241,-337.943 72.1726,-338.919\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140005211770720 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140005211770720</title>\n",
       "<polygon fill=\"none\" points=\"143,-365.5 143,-401.5 268,-401.5 268,-365.5 143,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-379.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140005437913352 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140005437913352</title>\n",
       "<polygon fill=\"none\" points=\"152.5,-292.5 152.5,-328.5 236.5,-328.5 236.5,-292.5 152.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-306.8\">gru_2: GRU</text>\n",
       "</g>\n",
       "<!-- 140005211770720&#45;&gt;140005437913352 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140005211770720-&gt;140005437913352</title>\n",
       "<path d=\"M202.837,-365.313C201.594,-357.289 200.085,-347.547 198.694,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.128,-337.875 197.138,-328.529 195.211,-338.947 202.128,-337.875\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140005437913016 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140005437913016</title>\n",
       "<polygon fill=\"none\" points=\"49.5,-219.5 49.5,-255.5 217.5,-255.5 217.5,-219.5 49.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-233.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140005211770272&#45;&gt;140005437913016 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140005211770272-&gt;140005437913016</title>\n",
       "<path d=\"M87.2664,-292.313C94.7591,-283.592 103.996,-272.84 112.239,-263.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"115.008,-265.395 118.87,-255.529 109.698,-260.833 115.008,-265.395\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140005437913352&#45;&gt;140005437913016 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140005437913352-&gt;140005437913016</title>\n",
       "<path d=\"M179.734,-292.313C172.241,-283.592 163.004,-272.84 154.761,-263.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"157.302,-260.833 148.13,-255.529 151.992,-265.395 157.302,-260.833\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140000052591584 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140000052591584</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-146.5 82.5,-182.5 184.5,-182.5 184.5,-146.5 82.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140005437913016&#45;&gt;140000052591584 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140005437913016-&gt;140000052591584</title>\n",
       "<path d=\"M133.5,-219.313C133.5,-211.289 133.5,-201.547 133.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-192.529 133.5,-182.529 130,-192.529 137,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140005437912960 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140005437912960</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-73.5 82.5,-109.5 184.5,-109.5 184.5,-73.5 82.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140000052591584&#45;&gt;140005437912960 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140000052591584-&gt;140005437912960</title>\n",
       "<path d=\"M133.5,-146.313C133.5,-138.289 133.5,-128.547 133.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-119.529 133.5,-109.529 130,-119.529 137,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140000051905648 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140000051905648</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-0.5 82.5,-36.5 184.5,-36.5 184.5,-0.5 82.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140005437912960&#45;&gt;140000051905648 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140005437912960-&gt;140000051905648</title>\n",
       "<path d=\"M133.5,-73.3129C133.5,-65.2895 133.5,-55.5475 133.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-46.5288 133.5,-36.5288 130,-46.5289 137,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define neural net architecture\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "melody_input = Input(shape=(max_melody_len, n_notes))\n",
    "melody_gru = GRU(latent_dim)(melody_input)\n",
    "\n",
    "chords_input = Input(shape=(chord_context_len, n_chords))\n",
    "chords_gru = GRU(latent_dim)(chords_input)\n",
    "\n",
    "concat = concatenate([melody_gru, chords_gru])\n",
    "\n",
    "chord_hidden1 = Dense(n_chords, activation='relu')(concat)\n",
    "chord_hidden2 = Dense(n_chords, activation='relu')(chord_hidden1)\n",
    "chord_dense = Dense(n_chords, activation='softmax')(chord_hidden2)\n",
    "\n",
    "model = Model([melody_input, chords_input], chord_dense)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce Early-Stopping and Save-Best-Performance callbacks\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min')\n",
    "filepath = \"../models/label-Mel1-Cho1-FC3_150ep.h5\"\n",
    "bp = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266784 samples, validate on 33348 samples\n",
      "Epoch 1/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.9421 - acc: 0.3378Epoch 00000: val_acc improved from -inf to 0.41664, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 2.9419 - acc: 0.3378 - val_loss: 2.5316 - val_acc: 0.4166\n",
      "Epoch 2/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.3803 - acc: 0.4390Epoch 00001: val_acc improved from 0.41664 to 0.46680, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 2.3802 - acc: 0.4390 - val_loss: 2.2999 - val_acc: 0.4668\n",
      "Epoch 3/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.1812 - acc: 0.4763Epoch 00002: val_acc improved from 0.46680 to 0.48914, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 2.1812 - acc: 0.4763 - val_loss: 2.1885 - val_acc: 0.4891\n",
      "Epoch 4/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 2.0376 - acc: 0.5084Epoch 00003: val_acc improved from 0.48914 to 0.51226, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 2.0375 - acc: 0.5084 - val_loss: 2.0866 - val_acc: 0.5123\n",
      "Epoch 5/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.9104 - acc: 0.5366Epoch 00004: val_acc improved from 0.51226 to 0.53628, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.9104 - acc: 0.5366 - val_loss: 1.9886 - val_acc: 0.5363\n",
      "Epoch 6/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.7994 - acc: 0.5621Epoch 00005: val_acc improved from 0.53628 to 0.55359, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.7994 - acc: 0.5621 - val_loss: 1.9246 - val_acc: 0.5536\n",
      "Epoch 7/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.7012 - acc: 0.5854Epoch 00006: val_acc improved from 0.55359 to 0.57032, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.7012 - acc: 0.5853 - val_loss: 1.8515 - val_acc: 0.5703\n",
      "Epoch 8/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.6145 - acc: 0.6053Epoch 00007: val_acc improved from 0.57032 to 0.58240, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.6145 - acc: 0.6053 - val_loss: 1.7984 - val_acc: 0.5824\n",
      "Epoch 9/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.5389 - acc: 0.6227Epoch 00008: val_acc improved from 0.58240 to 0.59344, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.5389 - acc: 0.6227 - val_loss: 1.7513 - val_acc: 0.5934\n",
      "Epoch 10/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.4731 - acc: 0.6382Epoch 00009: val_acc improved from 0.59344 to 0.60345, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.4731 - acc: 0.6382 - val_loss: 1.7160 - val_acc: 0.6035\n",
      "Epoch 11/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.4138 - acc: 0.6509Epoch 00010: val_acc improved from 0.60345 to 0.60996, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.4138 - acc: 0.6509 - val_loss: 1.6764 - val_acc: 0.6100\n",
      "Epoch 12/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.3642 - acc: 0.6621Epoch 00011: val_acc improved from 0.60996 to 0.62070, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.3641 - acc: 0.6621 - val_loss: 1.6486 - val_acc: 0.6207\n",
      "Epoch 13/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.3208 - acc: 0.6714Epoch 00012: val_acc improved from 0.62070 to 0.62786, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.3208 - acc: 0.6714 - val_loss: 1.6307 - val_acc: 0.6279\n",
      "Epoch 14/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2816 - acc: 0.6798Epoch 00013: val_acc improved from 0.62786 to 0.63176, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.2816 - acc: 0.6798 - val_loss: 1.6107 - val_acc: 0.6318\n",
      "Epoch 15/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2473 - acc: 0.6881Epoch 00014: val_acc improved from 0.63176 to 0.63740, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.2473 - acc: 0.6881 - val_loss: 1.5889 - val_acc: 0.6374\n",
      "Epoch 16/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.2172 - acc: 0.6942Epoch 00015: val_acc improved from 0.63740 to 0.64214, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.2173 - acc: 0.6942 - val_loss: 1.5761 - val_acc: 0.6421\n",
      "Epoch 17/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1905 - acc: 0.6992Epoch 00016: val_acc improved from 0.64214 to 0.64439, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.1906 - acc: 0.6992 - val_loss: 1.5488 - val_acc: 0.6444\n",
      "Epoch 18/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1757 - acc: 0.7026Epoch 00017: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 1.1756 - acc: 0.7026 - val_loss: 1.5626 - val_acc: 0.6434\n",
      "Epoch 19/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1483 - acc: 0.7081Epoch 00018: val_acc improved from 0.64439 to 0.65245, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.1483 - acc: 0.7081 - val_loss: 1.5251 - val_acc: 0.6525\n",
      "Epoch 20/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1267 - acc: 0.7126Epoch 00019: val_acc improved from 0.65245 to 0.65455, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.1267 - acc: 0.7126 - val_loss: 1.5272 - val_acc: 0.6546\n",
      "Epoch 21/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.1092 - acc: 0.7158Epoch 00020: val_acc improved from 0.65455 to 0.65599, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.1092 - acc: 0.7158 - val_loss: 1.5134 - val_acc: 0.6560\n",
      "Epoch 22/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0924 - acc: 0.7201Epoch 00021: val_acc improved from 0.65599 to 0.66586, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.0925 - acc: 0.7201 - val_loss: 1.4879 - val_acc: 0.6659\n",
      "Epoch 23/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0771 - acc: 0.7230Epoch 00022: val_acc improved from 0.66586 to 0.66826, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.0771 - acc: 0.7230 - val_loss: 1.4933 - val_acc: 0.6683\n",
      "Epoch 24/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0629 - acc: 0.7250Epoch 00023: val_acc improved from 0.66826 to 0.67146, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 1.0629 - acc: 0.7250 - val_loss: 1.4772 - val_acc: 0.6715\n",
      "Epoch 25/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0523 - acc: 0.7272Epoch 00024: val_acc improved from 0.67146 to 0.67380, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.0523 - acc: 0.7272 - val_loss: 1.4697 - val_acc: 0.6738\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0404 - acc: 0.7299Epoch 00025: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 1.0404 - acc: 0.7299 - val_loss: 1.4725 - val_acc: 0.6722\n",
      "Epoch 27/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0306 - acc: 0.7319Epoch 00026: val_acc improved from 0.67380 to 0.67911, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 1.0306 - acc: 0.7319 - val_loss: 1.4643 - val_acc: 0.6791\n",
      "Epoch 28/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0207 - acc: 0.7336Epoch 00027: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 1.0208 - acc: 0.7336 - val_loss: 1.4609 - val_acc: 0.6786\n",
      "Epoch 29/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0135 - acc: 0.7353Epoch 00028: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 1.0135 - acc: 0.7353 - val_loss: 1.4490 - val_acc: 0.6779\n",
      "Epoch 30/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.7367Epoch 00029: val_acc improved from 0.67911 to 0.68121, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 1.0058 - acc: 0.7367 - val_loss: 1.4496 - val_acc: 0.6812\n",
      "Epoch 31/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.7386Epoch 00030: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9965 - acc: 0.7386 - val_loss: 1.4501 - val_acc: 0.6788\n",
      "Epoch 32/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9927 - acc: 0.7397Epoch 00031: val_acc improved from 0.68121 to 0.68352, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 436s - loss: 0.9927 - acc: 0.7397 - val_loss: 1.4496 - val_acc: 0.6835\n",
      "Epoch 33/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9841 - acc: 0.7411Epoch 00032: val_acc improved from 0.68352 to 0.68583, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9841 - acc: 0.7412 - val_loss: 1.4533 - val_acc: 0.6858\n",
      "Epoch 34/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.7424Epoch 00033: val_acc improved from 0.68583 to 0.68661, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9815 - acc: 0.7424 - val_loss: 1.4453 - val_acc: 0.6866\n",
      "Epoch 35/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9761 - acc: 0.7423Epoch 00034: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9761 - acc: 0.7424 - val_loss: 1.4475 - val_acc: 0.6863\n",
      "Epoch 36/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.7438Epoch 00035: val_acc improved from 0.68661 to 0.68664, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9717 - acc: 0.7438 - val_loss: 1.4511 - val_acc: 0.6866\n",
      "Epoch 37/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9667 - acc: 0.7439Epoch 00036: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9667 - acc: 0.7439 - val_loss: 1.4542 - val_acc: 0.6863\n",
      "Epoch 38/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9624 - acc: 0.7460Epoch 00037: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9624 - acc: 0.7460 - val_loss: 1.4502 - val_acc: 0.6834\n",
      "Epoch 39/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9595 - acc: 0.7461Epoch 00038: val_acc improved from 0.68664 to 0.68961, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9595 - acc: 0.7462 - val_loss: 1.4359 - val_acc: 0.6896\n",
      "Epoch 40/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9557 - acc: 0.7467Epoch 00039: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9557 - acc: 0.7467 - val_loss: 1.4469 - val_acc: 0.6871\n",
      "Epoch 41/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9513 - acc: 0.7474Epoch 00040: val_acc improved from 0.68961 to 0.69602, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 0.9513 - acc: 0.7474 - val_loss: 1.4257 - val_acc: 0.6960\n",
      "Epoch 42/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9481 - acc: 0.7487Epoch 00041: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9480 - acc: 0.7487 - val_loss: 1.4378 - val_acc: 0.6942\n",
      "Epoch 43/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9465 - acc: 0.7486Epoch 00042: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9465 - acc: 0.7486 - val_loss: 1.4377 - val_acc: 0.6942\n",
      "Epoch 44/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9426 - acc: 0.7497Epoch 00043: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9427 - acc: 0.7497 - val_loss: 1.4272 - val_acc: 0.6957\n",
      "Epoch 45/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9402 - acc: 0.7501Epoch 00044: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9402 - acc: 0.7500 - val_loss: 1.4469 - val_acc: 0.6902\n",
      "Epoch 46/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9375 - acc: 0.7503Epoch 00045: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9375 - acc: 0.7503 - val_loss: 1.4348 - val_acc: 0.6954\n",
      "Epoch 47/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7512Epoch 00046: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9355 - acc: 0.7512 - val_loss: 1.4477 - val_acc: 0.6932\n",
      "Epoch 48/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9334 - acc: 0.7522Epoch 00047: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9334 - acc: 0.7522 - val_loss: 1.4310 - val_acc: 0.6953\n",
      "Epoch 49/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9311 - acc: 0.7521Epoch 00048: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9311 - acc: 0.7521 - val_loss: 1.4374 - val_acc: 0.6908\n",
      "Epoch 50/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9292 - acc: 0.7524Epoch 00049: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9292 - acc: 0.7524 - val_loss: 1.4329 - val_acc: 0.6951\n",
      "Epoch 51/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9264 - acc: 0.7532Epoch 00050: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9264 - acc: 0.7532 - val_loss: 1.4347 - val_acc: 0.6955\n",
      "Epoch 52/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.7529Epoch 00051: val_acc improved from 0.69602 to 0.69767, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9255 - acc: 0.7529 - val_loss: 1.4352 - val_acc: 0.6977\n",
      "Epoch 53/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.7533Epoch 00052: val_acc improved from 0.69767 to 0.70130, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 434s - loss: 0.9238 - acc: 0.7533 - val_loss: 1.4281 - val_acc: 0.7013\n",
      "Epoch 54/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9208 - acc: 0.7546Epoch 00053: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9208 - acc: 0.7546 - val_loss: 1.4321 - val_acc: 0.6976\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7537Epoch 00054: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9205 - acc: 0.7537 - val_loss: 1.4414 - val_acc: 0.6935\n",
      "Epoch 56/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9180 - acc: 0.7548Epoch 00055: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9180 - acc: 0.7548 - val_loss: 1.4283 - val_acc: 0.6945\n",
      "Epoch 57/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9175 - acc: 0.7548Epoch 00056: val_acc improved from 0.70130 to 0.70175, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9175 - acc: 0.7548 - val_loss: 1.4307 - val_acc: 0.7018\n",
      "Epoch 58/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.7551Epoch 00057: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9149 - acc: 0.7551 - val_loss: 1.4283 - val_acc: 0.6992\n",
      "Epoch 59/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9139 - acc: 0.7555Epoch 00058: val_acc improved from 0.70175 to 0.70439, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.9139 - acc: 0.7555 - val_loss: 1.4265 - val_acc: 0.7044\n",
      "Epoch 60/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9130 - acc: 0.7556Epoch 00059: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9130 - acc: 0.7557 - val_loss: 1.4271 - val_acc: 0.7033\n",
      "Epoch 61/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9111 - acc: 0.7558Epoch 00060: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9111 - acc: 0.7557 - val_loss: 1.4339 - val_acc: 0.6962\n",
      "Epoch 62/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9093 - acc: 0.7564Epoch 00061: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9093 - acc: 0.7564 - val_loss: 1.4299 - val_acc: 0.6956\n",
      "Epoch 63/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9098 - acc: 0.7563Epoch 00062: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9098 - acc: 0.7563 - val_loss: 1.4286 - val_acc: 0.7041\n",
      "Epoch 64/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.7545Epoch 00063: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.9170 - acc: 0.7545 - val_loss: 1.4332 - val_acc: 0.7025\n",
      "Epoch 65/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9083 - acc: 0.7556Epoch 00064: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9083 - acc: 0.7556 - val_loss: 1.4373 - val_acc: 0.7031\n",
      "Epoch 66/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9055 - acc: 0.7567Epoch 00065: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.9055 - acc: 0.7567 - val_loss: 1.4373 - val_acc: 0.6993\n",
      "Epoch 67/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9054 - acc: 0.7563Epoch 00066: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9054 - acc: 0.7563 - val_loss: 1.4323 - val_acc: 0.7011\n",
      "Epoch 68/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9057 - acc: 0.7567Epoch 00067: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9056 - acc: 0.7567 - val_loss: 1.4341 - val_acc: 0.7011\n",
      "Epoch 69/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7577Epoch 00068: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9022 - acc: 0.7577 - val_loss: 1.4284 - val_acc: 0.7022\n",
      "Epoch 70/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7572Epoch 00069: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.9034 - acc: 0.7572 - val_loss: 1.4191 - val_acc: 0.7018\n",
      "Epoch 71/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7581Epoch 00070: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9008 - acc: 0.7581 - val_loss: 1.4314 - val_acc: 0.7018\n",
      "Epoch 72/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9003 - acc: 0.7575Epoch 00071: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.9003 - acc: 0.7575 - val_loss: 1.4407 - val_acc: 0.7000\n",
      "Epoch 73/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8989 - acc: 0.7585Epoch 00072: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8989 - acc: 0.7585 - val_loss: 1.4331 - val_acc: 0.7023\n",
      "Epoch 74/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8982 - acc: 0.7579Epoch 00073: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8982 - acc: 0.7579 - val_loss: 1.4369 - val_acc: 0.6997\n",
      "Epoch 75/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8979 - acc: 0.7581Epoch 00074: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8979 - acc: 0.7581 - val_loss: 1.4294 - val_acc: 0.7021\n",
      "Epoch 76/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8972 - acc: 0.7583Epoch 00075: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8972 - acc: 0.7583 - val_loss: 1.4433 - val_acc: 0.7006\n",
      "Epoch 77/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7590Epoch 00076: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8963 - acc: 0.7590 - val_loss: 1.4405 - val_acc: 0.7007\n",
      "Epoch 78/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8945 - acc: 0.7594Epoch 00077: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8944 - acc: 0.7594 - val_loss: 1.4241 - val_acc: 0.7038\n",
      "Epoch 79/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8957 - acc: 0.7589Epoch 00078: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8957 - acc: 0.7589 - val_loss: 1.4364 - val_acc: 0.7019\n",
      "Epoch 80/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8933 - acc: 0.7591Epoch 00079: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8933 - acc: 0.7591 - val_loss: 1.4258 - val_acc: 0.7037\n",
      "Epoch 81/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8938 - acc: 0.7596Epoch 00080: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8938 - acc: 0.7596 - val_loss: 1.4297 - val_acc: 0.7010\n",
      "Epoch 82/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8934 - acc: 0.7590Epoch 00081: val_acc did not improve\n",
      "266784/266784 [==============================] - 432s - loss: 0.8935 - acc: 0.7590 - val_loss: 1.4368 - val_acc: 0.7007\n",
      "Epoch 83/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8927 - acc: 0.7588Epoch 00082: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8926 - acc: 0.7588 - val_loss: 1.4269 - val_acc: 0.7033\n",
      "Epoch 84/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7594Epoch 00083: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8914 - acc: 0.7594 - val_loss: 1.4415 - val_acc: 0.7007\n",
      "Epoch 85/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7598Epoch 00084: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8906 - acc: 0.7598 - val_loss: 1.4406 - val_acc: 0.7024\n",
      "Epoch 86/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8900 - acc: 0.7595Epoch 00085: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266784/266784 [==============================] - 434s - loss: 0.8899 - acc: 0.7595 - val_loss: 1.4364 - val_acc: 0.7034\n",
      "Epoch 87/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8904 - acc: 0.7595Epoch 00086: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8903 - acc: 0.7595 - val_loss: 1.4339 - val_acc: 0.6997\n",
      "Epoch 88/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.7593Epoch 00087: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8895 - acc: 0.7593 - val_loss: 1.4379 - val_acc: 0.7039\n",
      "Epoch 89/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8884 - acc: 0.7600Epoch 00088: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8885 - acc: 0.7599 - val_loss: 1.4358 - val_acc: 0.7015\n",
      "Epoch 90/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8893 - acc: 0.7598Epoch 00089: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8893 - acc: 0.7598 - val_loss: 1.4253 - val_acc: 0.7033\n",
      "Epoch 91/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.7600Epoch 00090: val_acc improved from 0.70439 to 0.70589, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.8876 - acc: 0.7600 - val_loss: 1.4267 - val_acc: 0.7059\n",
      "Epoch 92/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8865 - acc: 0.7606Epoch 00091: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8866 - acc: 0.7605 - val_loss: 1.4334 - val_acc: 0.7019\n",
      "Epoch 93/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8869 - acc: 0.7596Epoch 00092: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8869 - acc: 0.7596 - val_loss: 1.4312 - val_acc: 0.7039\n",
      "Epoch 94/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7605Epoch 00093: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8861 - acc: 0.7605 - val_loss: 1.4252 - val_acc: 0.7053\n",
      "Epoch 95/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7602Epoch 00094: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8848 - acc: 0.7602 - val_loss: 1.4402 - val_acc: 0.7013\n",
      "Epoch 96/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7604Epoch 00095: val_acc improved from 0.70589 to 0.70658, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 432s - loss: 0.8847 - acc: 0.7604 - val_loss: 1.4363 - val_acc: 0.7066\n",
      "Epoch 97/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8842 - acc: 0.7605Epoch 00096: val_acc did not improve\n",
      "266784/266784 [==============================] - 431s - loss: 0.8842 - acc: 0.7604 - val_loss: 1.4354 - val_acc: 0.7059\n",
      "Epoch 98/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7606Epoch 00097: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8850 - acc: 0.7606 - val_loss: 1.4363 - val_acc: 0.7056\n",
      "Epoch 99/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8827 - acc: 0.7606Epoch 00098: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8827 - acc: 0.7606 - val_loss: 1.4449 - val_acc: 0.7015\n",
      "Epoch 100/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.7609Epoch 00099: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8829 - acc: 0.7609 - val_loss: 1.4344 - val_acc: 0.7061\n",
      "Epoch 101/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8830 - acc: 0.7611Epoch 00100: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8830 - acc: 0.7611 - val_loss: 1.4440 - val_acc: 0.7004\n",
      "Epoch 102/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8827 - acc: 0.7609Epoch 00101: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8827 - acc: 0.7609 - val_loss: 1.4370 - val_acc: 0.7034\n",
      "Epoch 103/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8813 - acc: 0.7611Epoch 00102: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8813 - acc: 0.7611 - val_loss: 1.4347 - val_acc: 0.7038\n",
      "Epoch 104/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8805 - acc: 0.7615Epoch 00103: val_acc improved from 0.70658 to 0.70793, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 433s - loss: 0.8805 - acc: 0.7615 - val_loss: 1.4315 - val_acc: 0.7079\n",
      "Epoch 105/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7611Epoch 00104: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8805 - acc: 0.7611 - val_loss: 1.4437 - val_acc: 0.7062\n",
      "Epoch 106/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.9039 - acc: 0.7565Epoch 00105: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.9039 - acc: 0.7565 - val_loss: 1.4530 - val_acc: 0.6997\n",
      "Epoch 107/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8925 - acc: 0.7589Epoch 00106: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8926 - acc: 0.7589 - val_loss: 1.4504 - val_acc: 0.7004\n",
      "Epoch 108/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8854 - acc: 0.7608Epoch 00107: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8854 - acc: 0.7608 - val_loss: 1.4447 - val_acc: 0.7022\n",
      "Epoch 109/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7613Epoch 00108: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8811 - acc: 0.7613 - val_loss: 1.4470 - val_acc: 0.7025\n",
      "Epoch 110/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.7614Epoch 00109: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8792 - acc: 0.7614 - val_loss: 1.4404 - val_acc: 0.7053\n",
      "Epoch 111/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8794 - acc: 0.7616Epoch 00110: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8794 - acc: 0.7616 - val_loss: 1.4532 - val_acc: 0.7024\n",
      "Epoch 112/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8798 - acc: 0.7612Epoch 00111: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8798 - acc: 0.7612 - val_loss: 1.4434 - val_acc: 0.7024\n",
      "Epoch 113/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7615Epoch 00112: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8779 - acc: 0.7615 - val_loss: 1.4431 - val_acc: 0.7037\n",
      "Epoch 114/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7619Epoch 00113: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8786 - acc: 0.7619 - val_loss: 1.4442 - val_acc: 0.7071\n",
      "Epoch 115/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7616Epoch 00114: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8764 - acc: 0.7617 - val_loss: 1.4397 - val_acc: 0.7059\n",
      "Epoch 116/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7613Epoch 00115: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8773 - acc: 0.7613 - val_loss: 1.4421 - val_acc: 0.7026\n",
      "Epoch 117/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.7623Epoch 00116: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266784/266784 [==============================] - 435s - loss: 0.8762 - acc: 0.7623 - val_loss: 1.4468 - val_acc: 0.7066\n",
      "Epoch 118/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.7620Epoch 00117: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8762 - acc: 0.7620 - val_loss: 1.4413 - val_acc: 0.7020\n",
      "Epoch 119/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8759 - acc: 0.7620Epoch 00118: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8759 - acc: 0.7620 - val_loss: 1.4485 - val_acc: 0.7003\n",
      "Epoch 120/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.7620Epoch 00119: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8751 - acc: 0.7620 - val_loss: 1.4378 - val_acc: 0.7057\n",
      "Epoch 121/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.7620Epoch 00120: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8751 - acc: 0.7620 - val_loss: 1.4380 - val_acc: 0.7037\n",
      "Epoch 122/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8752 - acc: 0.7618Epoch 00121: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8752 - acc: 0.7618 - val_loss: 1.4437 - val_acc: 0.7012\n",
      "Epoch 123/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7622Epoch 00122: val_acc improved from 0.70793 to 0.70853, saving model to ../models/label-Mel1-Cho1-FC3_150ep.h5\n",
      "266784/266784 [==============================] - 435s - loss: 0.8742 - acc: 0.7622 - val_loss: 1.4478 - val_acc: 0.7085\n",
      "Epoch 124/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8740 - acc: 0.7628Epoch 00123: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8740 - acc: 0.7628 - val_loss: 1.4537 - val_acc: 0.7045\n",
      "Epoch 125/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8746 - acc: 0.7623Epoch 00124: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8746 - acc: 0.7623 - val_loss: 1.4477 - val_acc: 0.7021\n",
      "Epoch 126/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8726 - acc: 0.7629Epoch 00125: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8726 - acc: 0.7629 - val_loss: 1.4387 - val_acc: 0.7076\n",
      "Epoch 127/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8734 - acc: 0.7623Epoch 00126: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8734 - acc: 0.7623 - val_loss: 1.4420 - val_acc: 0.7058\n",
      "Epoch 128/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8980 - acc: 0.7574Epoch 00127: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8981 - acc: 0.7574 - val_loss: 1.4967 - val_acc: 0.6924\n",
      "Epoch 129/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8900 - acc: 0.7590Epoch 00128: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8901 - acc: 0.7590 - val_loss: 1.4521 - val_acc: 0.7058\n",
      "Epoch 130/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.7603Epoch 00129: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8828 - acc: 0.7603 - val_loss: 1.4553 - val_acc: 0.7021\n",
      "Epoch 131/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8809 - acc: 0.7612Epoch 00130: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8809 - acc: 0.7612 - val_loss: 1.4625 - val_acc: 0.7048\n",
      "Epoch 132/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8872 - acc: 0.7593Epoch 00131: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8872 - acc: 0.7593 - val_loss: 1.4673 - val_acc: 0.7041\n",
      "Epoch 133/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7617Epoch 00132: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8786 - acc: 0.7617 - val_loss: 1.4684 - val_acc: 0.7052\n",
      "Epoch 134/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7615Epoch 00133: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8766 - acc: 0.7614 - val_loss: 1.4854 - val_acc: 0.7011\n",
      "Epoch 135/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8738 - acc: 0.7632Epoch 00134: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8737 - acc: 0.7632 - val_loss: 1.4738 - val_acc: 0.7018\n",
      "Epoch 136/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7631Epoch 00135: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8736 - acc: 0.7631 - val_loss: 1.4671 - val_acc: 0.7047\n",
      "Epoch 137/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7635Epoch 00136: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8736 - acc: 0.7635 - val_loss: 1.4722 - val_acc: 0.7032\n",
      "Epoch 138/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.7625Epoch 00137: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8762 - acc: 0.7625 - val_loss: 1.4638 - val_acc: 0.7033\n",
      "Epoch 139/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8724 - acc: 0.7632Epoch 00138: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8725 - acc: 0.7632 - val_loss: 1.4780 - val_acc: 0.7016\n",
      "Epoch 140/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.7621Epoch 00139: val_acc did not improve\n",
      "266784/266784 [==============================] - 434s - loss: 0.8758 - acc: 0.7621 - val_loss: 1.4674 - val_acc: 0.7018\n",
      "Epoch 141/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8802 - acc: 0.7614Epoch 00140: val_acc did not improve\n",
      "266784/266784 [==============================] - 433s - loss: 0.8802 - acc: 0.7614 - val_loss: 1.4798 - val_acc: 0.7008\n",
      "Epoch 142/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7616Epoch 00141: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8779 - acc: 0.7616 - val_loss: 1.4784 - val_acc: 0.7028\n",
      "Epoch 143/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8741 - acc: 0.7627Epoch 00142: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8742 - acc: 0.7627 - val_loss: 1.4693 - val_acc: 0.7038\n",
      "Epoch 144/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8734 - acc: 0.7627Epoch 00143: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8733 - acc: 0.7627 - val_loss: 1.4653 - val_acc: 0.7059\n",
      "Epoch 145/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8746 - acc: 0.7627Epoch 00144: val_acc did not improve\n",
      "266784/266784 [==============================] - 435s - loss: 0.8747 - acc: 0.7627 - val_loss: 1.4712 - val_acc: 0.7038\n",
      "Epoch 146/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8766 - acc: 0.7617Epoch 00145: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8767 - acc: 0.7617 - val_loss: 1.4613 - val_acc: 0.7044\n",
      "Epoch 147/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8706 - acc: 0.7630Epoch 00146: val_acc did not improve\n",
      "266784/266784 [==============================] - 437s - loss: 0.8706 - acc: 0.7630 - val_loss: 1.4661 - val_acc: 0.7050\n",
      "Epoch 148/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8691 - acc: 0.7635Epoch 00147: val_acc did not improve\n",
      "266784/266784 [==============================] - 437s - loss: 0.8691 - acc: 0.7635 - val_loss: 1.4554 - val_acc: 0.7052\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8666 - acc: 0.7639Epoch 00148: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8667 - acc: 0.7639 - val_loss: 1.4590 - val_acc: 0.7033\n",
      "Epoch 150/150\n",
      "266752/266784 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7626Epoch 00149: val_acc did not improve\n",
      "266784/266784 [==============================] - 436s - loss: 0.8716 - acc: 0.7626 - val_loss: 1.4624 - val_acc: 0.7041\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit([X_melody_train, X_chords_train], Y_train, epochs=epochs, validation_data=([X_melody_valid, X_chords_valid], Y_valid,), batch_size=batch_size, callbacks=[bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55c7cb6438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFkCAYAAADWs8tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkkbe/wPVjVvl2OTIoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55c7cb6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model learning\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Training history of root model\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best performance model\n",
    "model = load_model(\"../models/label-Mel1-Cho1-FC3_150ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord label prediction: 0.7021\n",
      "Kappa score of combined chord label prediction: 0.6940\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of labels\n",
    "\n",
    "# Predict chords from each test sample melody\n",
    "Y_pred = model.predict([X_melody_test, X_chords_test])\n",
    "\n",
    "# Compute accuracy and kappa score\n",
    "print(\"Categorical accuracy of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Kappa score of combined chord label prediction: {0:.4f}\".format(harmoutil.compute_kappa_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord pitch prediction: 0.8745\n",
      "TP: 105352 TN: 244595 FP: 25455 FN: 24774\n",
      "Kappa score of combined chord pitch prediction: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of pitches\n",
    "\n",
    "def label_to_pitch_tensors(predictions):\n",
    "    predicted_chords = [int_to_chord[np.argmax(ch)] for ch in predictions]\n",
    "    pitch_chords = [harmoutil.chord_to_notes(label) for label in predicted_chords]\n",
    "    \n",
    "    Y_pitches = np.zeros((predictions.shape[0], 12), dtype='float32')\n",
    "    for i, chord_pitches in enumerate(pitch_chords):\n",
    "        for j, pitch_presence in enumerate(chord_pitches):\n",
    "            Y_pitches[i, j] = pitch_presence\n",
    "\n",
    "    return Y_pitches\n",
    "    \n",
    "    \n",
    "Y_pred_pitch = label_to_pitch_tensors(Y_pred)\n",
    "Y_test_pitch = label_to_pitch_tensors(Y_test)\n",
    "\n",
    "print(\"Categorical accuracy of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_accuracy_score(Y_test_pitch, Y_pred_pitch)))\n",
    "print(\"Kappa score of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_kappa_score(Y_test_pitch, Y_pred_pitch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"F-score: {0:.4f}\".format(harmoutil.compute_binary_fscore(Y_test_pitch, Y_pred_pitch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
