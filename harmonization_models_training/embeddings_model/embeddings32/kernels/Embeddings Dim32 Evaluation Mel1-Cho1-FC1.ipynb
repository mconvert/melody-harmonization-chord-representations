{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, GRU, concatenate\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Custom library for the project\n",
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "import harmoutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'harmoutil' from '../../src/harmoutil.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Remove when done with kernel\n",
    "import importlib\n",
    "importlib.reload(harmoutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = harmoutil.load_pickled_data(\"../../data/refined_data.pkl\") # lists of (chord label, melody seqs) by sections\n",
    "augmented_data = harmoutil.transpose_and_augment_data(raw_data)\n",
    "data = [harmoutil.to_sevenths(section) for section in augmented_data]\n",
    "data = [harmoutil.melody_to_octave_range(section) for section in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chords: 334344 | Sample chord: E6\n",
      "Number of melody notes in the data: 2209944 | Sample melody note: 4\n",
      "Unique notes: [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Unique notes: ['A', 'A+', 'A+7', 'A+j7', 'A-', 'A-6', 'A-7', 'A-j7', 'A6', 'A7', 'Ab', 'Ab+', 'Ab+7', 'Ab+j7', 'Ab-', 'Ab-6', 'Ab-7', 'Ab-j7', 'Ab6', 'Ab7', 'Abj7', 'Abm7b5', 'Abo', 'Abo7', 'Absus', 'Absus7', 'Aj7', 'Am7b5', 'Ao', 'Ao7', 'Asus', 'Asus7', 'B', 'B+', 'B+7', 'B+j7', 'B-', 'B-6', 'B-7', 'B-j7', 'B6', 'B7', 'Bb', 'Bb+', 'Bb+7', 'Bb+j7', 'Bb-', 'Bb-6', 'Bb-7', 'Bb-j7', 'Bb6', 'Bb7', 'Bbj7', 'Bbm7b5', 'Bbo', 'Bbo7', 'Bbsus', 'Bbsus7', 'Bj7', 'Bm7b5', 'Bo', 'Bo7', 'Bsus', 'Bsus7', 'C', 'C+', 'C+7', 'C+j7', 'C-', 'C-6', 'C-7', 'C-j7', 'C6', 'C7', 'Cj7', 'Cm7b5', 'Co', 'Co7', 'Csus', 'Csus7', 'D', 'D+', 'D+7', 'D+j7', 'D-', 'D-6', 'D-7', 'D-j7', 'D6', 'D7', 'Db', 'Db+', 'Db+7', 'Db+j7', 'Db-', 'Db-6', 'Db-7', 'Db-j7', 'Db6', 'Db7', 'Dbj7', 'Dbm7b5', 'Dbo', 'Dbo7', 'Dbsus', 'Dbsus7', 'Dj7', 'Dm7b5', 'Do', 'Do7', 'Dsus', 'Dsus7', 'E', 'E+', 'E+7', 'E+j7', 'E-', 'E-6', 'E-7', 'E-j7', 'E6', 'E7', 'Eb', 'Eb+', 'Eb+7', 'Eb+j7', 'Eb-', 'Eb-6', 'Eb-7', 'Eb-j7', 'Eb6', 'Eb7', 'Ebj7', 'Ebm7b5', 'Ebo', 'Ebo7', 'Ebsus', 'Ebsus7', 'Ej7', 'Em7b5', 'Eo', 'Eo7', 'Esus', 'Esus7', 'F', 'F+', 'F+7', 'F+j7', 'F-', 'F-6', 'F-7', 'F-j7', 'F6', 'F7', 'Fj7', 'Fm7b5', 'Fo', 'Fo7', 'Fsus', 'Fsus7', 'G', 'G+', 'G+7', 'G+j7', 'G-', 'G-6', 'G-7', 'G-j7', 'G6', 'G7', 'Gb', 'Gb+', 'Gb+7', 'Gb+j7', 'Gb-', 'Gb-6', 'Gb-7', 'Gb-j7', 'Gb6', 'Gb7', 'Gbj7', 'Gbm7b5', 'Gbo', 'Gbo7', 'Gbsus', 'Gbsus7', 'Gj7', 'Gm7b5', 'Go', 'Go7', 'Gsus', 'Gsus7', 'NC']\n"
     ]
    }
   ],
   "source": [
    "# Isolate relevant data\n",
    "def get_notes_by_chord(beats):\n",
    "    return [note for beat in beats for note in beat]\n",
    "\n",
    "def get_chords_by_section(section):\n",
    "    return [chord_info[0] for chord_info in section]\n",
    "\n",
    "# def check_if_augmented_major(section):\n",
    "#     section_chords = get_chords_by_section(section)\n",
    "#     for ch in section_chords:\n",
    "#         if \"+j7\" in ch:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "# data = [section for section in data if not check_if_augmented_major(section)]\n",
    "# print(\"---Remove sections with augmented major chord---\")\n",
    "# print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections_bf_augmaj7 = [get_chords_by_section(section) for section in data]\n",
    "chords = [chord_info[0] for section in data for chord_info in section]\n",
    "unique_chords = sorted(list(set(chords)))\n",
    "\n",
    "notes_by_chords_bf_augmaj7 = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "notes = [note for chord_notes in notes_by_chords_bf_augmaj7 for note in chord_notes]\n",
    "unique_notes = sorted(list(set(notes)))\n",
    "\n",
    "# print(sum([len(section) for section in chords_by_sections]))\n",
    "# print(\"Number of sections: {} | Sample section chords: {}\".format(len(chords_by_sections), chords_by_sections[0]))\n",
    "print(\"Number of chords: {} | Sample chord: {}\".format(len(chords), chords[0]))\n",
    "# print(\"Number of melodies {} | Sample melody: {}\".format(len(notes_by_chords), notes_by_chords[0]))\n",
    "print(\"Number of melody notes in the data: {} | Sample melody note: {}\".format(len(notes), notes[0]))\n",
    "print(\"Unique notes: {}\".format(unique_notes))\n",
    "print(\"Unique notes: {}\".format(unique_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody note to integer mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, '<pad>': 13, -1: 12}\n",
      "\n",
      "Integer to melody note mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: -1, 13: '<pad>'}\n",
      "\n",
      "Chord label to integer mapping:\n",
      " {'Csus7': 79, 'Cj7': 74, 'Bb': 42, 'Gbo': 182, 'Dm7b5': 107, '<bos>': 193, 'Eo7': 141, 'Abo7': 23, 'A6': 8, 'B-6': 37, 'Am7b5': 27, 'G': 160, 'C6': 72, 'D': 80, 'Bsus': 62, 'Dsus7': 111, 'Eb+j7': 125, 'Gbj7': 180, 'Db+7': 92, 'C-j7': 71, 'F-7': 150, 'Ab': 10, 'Csus': 78, 'Fsus7': 159, 'Go7': 189, 'Gb': 170, 'Dbsus7': 105, 'E-6': 117, 'E7': 121, 'Gb-6': 175, 'Bb+7': 44, 'Co7': 77, 'Asus': 30, 'F+j7': 147, 'Ab+j7': 13, 'E-': 116, 'Eb+': 123, 'Bbsus7': 57, 'Eb-7': 128, 'Ab-': 14, 'B-j7': 39, 'D-7': 86, 'Gsus': 190, 'Absus': 24, 'Ab-7': 16, 'Dsus': 110, 'Bbo': 54, 'Gb+7': 172, 'Ebo': 134, 'Eb-': 126, 'Bb-j7': 49, 'Ebsus': 136, 'Gj7': 186, 'NC': 192, 'F6': 152, 'E6': 120, 'C-': 68, 'A-7': 6, 'Gb6': 178, 'E+': 113, 'Db-6': 95, 'Dbm7b5': 101, 'Fsus': 158, 'Eb6': 130, 'B-7': 38, 'Fo7': 157, 'D6': 88, 'E-7': 118, 'A+': 1, 'A+7': 2, 'C+j7': 67, 'G+': 161, 'Eb+7': 124, 'Dbsus': 104, 'Db-': 94, 'Bo7': 61, 'Ej7': 138, 'Ab+': 11, 'Bbm7b5': 53, 'E+7': 114, 'A7': 9, 'G-j7': 167, 'Bsus7': 63, 'F-j7': 151, 'Db+j7': 93, 'D-6': 85, 'E': 112, 'Bb-7': 48, 'Bb+j7': 45, 'Abj7': 20, 'Co': 76, 'G+7': 162, 'G6': 168, 'Fj7': 154, 'Gb7': 179, 'Em7b5': 139, 'Asus7': 31, 'Eb-6': 127, 'Gsus7': 191, 'Bm7b5': 59, 'Ab-6': 15, 'Eo': 140, 'Absus7': 25, 'Dj7': 106, 'Ab+7': 12, 'D+7': 82, 'Ao7': 29, 'F-': 148, 'Gbsus': 184, 'Gbo7': 183, 'Bb6': 50, 'B+': 33, 'C-7': 70, 'Db6': 98, 'C+7': 66, 'Dbo7': 103, 'Ebm7b5': 133, 'Ab7': 19, 'C-6': 69, 'Ebo7': 135, 'A-j7': 7, 'F+': 145, 'Bj7': 58, 'E+j7': 115, 'F-6': 149, 'Gb+j7': 173, 'B': 32, 'G-6': 165, 'F': 144, 'Do7': 109, 'Bo': 60, 'Db': 90, 'F+7': 146, 'D-': 84, 'D-j7': 87, 'D+': 81, 'Gb-j7': 177, 'Cm7b5': 75, 'B6': 40, 'Fm7b5': 155, 'Db-j7': 97, 'Go': 188, 'C7': 73, 'G7': 169, 'Db7': 99, 'Bbo7': 55, 'Ebj7': 132, 'Gbm7b5': 181, 'Gm7b5': 187, '<eos>': 194, 'C+': 65, 'Bb7': 51, 'B+j7': 35, 'Gbsus7': 185, 'D7': 89, 'A-6': 5, 'Eb-j7': 129, 'Abm7b5': 21, 'Gb-': 174, 'F7': 153, 'Ab6': 18, 'Bb-6': 47, 'Db+': 91, 'Do': 108, 'E-j7': 119, 'G-7': 166, 'Dbj7': 100, 'Gb+': 171, 'Eb7': 131, 'Bb+': 43, 'Bb-': 46, 'A-': 4, 'A': 0, 'Abo': 22, 'Aj7': 26, 'Gb-7': 176, 'B-': 36, 'Ebsus7': 137, 'B+7': 34, 'Db-7': 96, 'Esus7': 143, 'D+j7': 83, 'Ab-j7': 17, 'Esus': 142, 'A+j7': 3, 'G+j7': 163, 'B7': 41, 'Fo': 156, 'Bbsus': 56, 'Ao': 28, 'Dbo': 102, 'G-': 164, 'C': 64, 'Bbj7': 52, 'Eb': 122}\n",
      "\n",
      "Integer to chord label mapping:\n",
      " {0: 'A', 1: 'A+', 2: 'A+7', 3: 'A+j7', 4: 'A-', 5: 'A-6', 6: 'A-7', 7: 'A-j7', 8: 'A6', 9: 'A7', 10: 'Ab', 11: 'Ab+', 12: 'Ab+7', 13: 'Ab+j7', 14: 'Ab-', 15: 'Ab-6', 16: 'Ab-7', 17: 'Ab-j7', 18: 'Ab6', 19: 'Ab7', 20: 'Abj7', 21: 'Abm7b5', 22: 'Abo', 23: 'Abo7', 24: 'Absus', 25: 'Absus7', 26: 'Aj7', 27: 'Am7b5', 28: 'Ao', 29: 'Ao7', 30: 'Asus', 31: 'Asus7', 32: 'B', 33: 'B+', 34: 'B+7', 35: 'B+j7', 36: 'B-', 37: 'B-6', 38: 'B-7', 39: 'B-j7', 40: 'B6', 41: 'B7', 42: 'Bb', 43: 'Bb+', 44: 'Bb+7', 45: 'Bb+j7', 46: 'Bb-', 47: 'Bb-6', 48: 'Bb-7', 49: 'Bb-j7', 50: 'Bb6', 51: 'Bb7', 52: 'Bbj7', 53: 'Bbm7b5', 54: 'Bbo', 55: 'Bbo7', 56: 'Bbsus', 57: 'Bbsus7', 58: 'Bj7', 59: 'Bm7b5', 60: 'Bo', 61: 'Bo7', 62: 'Bsus', 63: 'Bsus7', 64: 'C', 65: 'C+', 66: 'C+7', 67: 'C+j7', 68: 'C-', 69: 'C-6', 70: 'C-7', 71: 'C-j7', 72: 'C6', 73: 'C7', 74: 'Cj7', 75: 'Cm7b5', 76: 'Co', 77: 'Co7', 78: 'Csus', 79: 'Csus7', 80: 'D', 81: 'D+', 82: 'D+7', 83: 'D+j7', 84: 'D-', 85: 'D-6', 86: 'D-7', 87: 'D-j7', 88: 'D6', 89: 'D7', 90: 'Db', 91: 'Db+', 92: 'Db+7', 93: 'Db+j7', 94: 'Db-', 95: 'Db-6', 96: 'Db-7', 97: 'Db-j7', 98: 'Db6', 99: 'Db7', 100: 'Dbj7', 101: 'Dbm7b5', 102: 'Dbo', 103: 'Dbo7', 104: 'Dbsus', 105: 'Dbsus7', 106: 'Dj7', 107: 'Dm7b5', 108: 'Do', 109: 'Do7', 110: 'Dsus', 111: 'Dsus7', 112: 'E', 113: 'E+', 114: 'E+7', 115: 'E+j7', 116: 'E-', 117: 'E-6', 118: 'E-7', 119: 'E-j7', 120: 'E6', 121: 'E7', 122: 'Eb', 123: 'Eb+', 124: 'Eb+7', 125: 'Eb+j7', 126: 'Eb-', 127: 'Eb-6', 128: 'Eb-7', 129: 'Eb-j7', 130: 'Eb6', 131: 'Eb7', 132: 'Ebj7', 133: 'Ebm7b5', 134: 'Ebo', 135: 'Ebo7', 136: 'Ebsus', 137: 'Ebsus7', 138: 'Ej7', 139: 'Em7b5', 140: 'Eo', 141: 'Eo7', 142: 'Esus', 143: 'Esus7', 144: 'F', 145: 'F+', 146: 'F+7', 147: 'F+j7', 148: 'F-', 149: 'F-6', 150: 'F-7', 151: 'F-j7', 152: 'F6', 153: 'F7', 154: 'Fj7', 155: 'Fm7b5', 156: 'Fo', 157: 'Fo7', 158: 'Fsus', 159: 'Fsus7', 160: 'G', 161: 'G+', 162: 'G+7', 163: 'G+j7', 164: 'G-', 165: 'G-6', 166: 'G-7', 167: 'G-j7', 168: 'G6', 169: 'G7', 170: 'Gb', 171: 'Gb+', 172: 'Gb+7', 173: 'Gb+j7', 174: 'Gb-', 175: 'Gb-6', 176: 'Gb-7', 177: 'Gb-j7', 178: 'Gb6', 179: 'Gb7', 180: 'Gbj7', 181: 'Gbm7b5', 182: 'Gbo', 183: 'Gbo7', 184: 'Gbsus', 185: 'Gbsus7', 186: 'Gj7', 187: 'Gm7b5', 188: 'Go', 189: 'Go7', 190: 'Gsus', 191: 'Gsus7', 192: 'NC', 193: '<bos>', 194: '<eos>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create categorical data mappings\n",
    "note_to_int = dict([(c, i) for i, c in enumerate(unique_notes[1:])])\n",
    "note_to_int[-1] = len(note_to_int)\n",
    "note_to_int['<pad>'] = len(note_to_int)\n",
    "\n",
    "int_to_note = dict([(k, v) for v, k in note_to_int.items()])\n",
    "\n",
    "chord_to_int = dict([(c, i) for i, c in enumerate(unique_chords)])\n",
    "chord_to_int['<bos>'] = len(chord_to_int)\n",
    "chord_to_int['<eos>'] = len(chord_to_int)\n",
    "\n",
    "int_to_chord = dict([(k, v) for v, k in chord_to_int.items()])\n",
    "\n",
    "print(\"Melody note to integer mapping:\\n {}\\n\".format(note_to_int))\n",
    "print(\"Integer to melody note mapping:\\n {}\\n\".format(int_to_note))\n",
    "print(\"Chord label to integer mapping:\\n {}\\n\".format(chord_to_int))\n",
    "print(\"Integer to chord label mapping:\\n {}\\n\".format(int_to_chord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 334344\n",
      "Number of distinct melody notes: 14\n",
      "Number of distinct chord labels: 195\n",
      "Maximum length of melody sequences for one chord: 135\n",
      "Number of past chords given as input: 7\n"
     ]
    }
   ],
   "source": [
    "# Define numerical variables\n",
    "\n",
    "n_samples = len(chords)\n",
    "n_chords = len(chord_to_int)\n",
    "n_notes = len(note_to_int)\n",
    "max_mel_len = max([len(mel) for mel in notes_by_chords_bf_augmaj7])\n",
    "chord_context_len = 7\n",
    "\n",
    "# print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Number of distinct melody notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct chord labels: {}\".format(n_chords))\n",
    "print(\"Maximum length of melody sequences for one chord: {}\".format(max_mel_len))\n",
    "print(\"Number of past chords given as input: {}\".format(chord_context_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4], [4]]\n"
     ]
    }
   ],
   "source": [
    "mel_by_sections = [mel for section in data for ch, mel in section]\n",
    "print(mel_by_sections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Remove sections with augmented major chord---\n",
      "Number of sections: 28836\n",
      "\n",
      "Sample input melody sequence: [8, 3, 6, '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample input chord sequence: ['<bos>', '<bos>', 'E6', 'Db7', 'Gb-7', 'B7', 'E']\n",
      "\n",
      "Sample target chord: Db-7\n",
      "\n",
      "Input melody: 333480, Input chords: 333480, Target chords: 333480\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensor data\n",
    "\n",
    "\n",
    "def check_if_augmented_major(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"+j7\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "data = [section for section in data if not check_if_augmented_major(section)]\n",
    "print(\"---Remove sections with augmented major chord---\")\n",
    "print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in data]\n",
    "# chords = [chord_info[0] for section in data for chord_info in section]\n",
    "# unique_chords = sorted(list(set(chords)))\n",
    "\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "# notes = [note for chord_notes in notes_by_chords for note in chord_notes]\n",
    "# unique_notes = sorted(list(set(notes)))\n",
    "\n",
    "\n",
    "\n",
    "def pad_melody(melody, max_len):\n",
    "    return melody + (max_len-len(melody))*['<pad>']\n",
    "\n",
    "def build_input_chord_sequences(chord_seq, context_len):\n",
    "    padded_sequence = context_len*['<bos>'] + chord_seq\n",
    "    formatted_sequences = [padded_sequence[i:i+context_len+1] for i in range(len(chord_seq))]\n",
    "    return formatted_sequences\n",
    "\n",
    "# Melody\n",
    "input_melody_data = [pad_melody(melody, max_mel_len) for melody in notes_by_chords]\n",
    "print(\"Sample input melody sequence: {}\\n\".format(input_melody_data[5]))\n",
    "\n",
    "# Chords\n",
    "formatted_chords_data = []\n",
    "for section_chords in chords_by_sections:\n",
    "    formatted_chords_data += build_input_chord_sequences(section_chords, chord_context_len)\n",
    "    \n",
    "input_chords_data = [ch[:-1] for ch in formatted_chords_data]\n",
    "target_chords_data = [ch[-1] for ch in formatted_chords_data]\n",
    "print(\"Sample input chord sequence: {}\\n\".format(input_chords_data[5]))\n",
    "print(\"Sample target chord: {}\\n\".format(target_chords_data[5]))\n",
    "\n",
    "print(\"Input melody: {}, Input chords: {}, Target chords: {}\".format(len(input_melody_data), len(input_chords_data), len(target_chords_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding vectors\n",
    "\n",
    "num_dim = 32\n",
    "num_ch = 192\n",
    "num_notes = 12\n",
    "\n",
    "# Define embedding training model and load weights\n",
    "input_layer = Input(shape=(num_ch,)) \n",
    "embeddings_layer = Dense(num_dim, activation='linear', name=\"embeddings\")(input_layer)\n",
    "root_output_layer = Dense(num_notes, activation='softmax')(embeddings_layer)\n",
    "interval_output_layer = Dense(num_notes, activation='sigmoid')(embeddings_layer)\n",
    "pitch_output_layer = Dense(num_notes, activation='sigmoid')(embeddings_layer)\n",
    "melody_output_layer = Dense(num_notes, activation='relu')(embeddings_layer)\n",
    "embeddings_model = Model(input_layer, [root_output_layer, interval_output_layer, pitch_output_layer, melody_output_layer])\n",
    "\n",
    "embeddings_model.load_weights(\"../Skipgram & WJD/weights/combined_weights_dim32.h5\")\n",
    "\n",
    "X_chords_embeddings = embeddings_model.layers[1].get_weights()[0]\n",
    "print(X_chords_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embedding vector for each chord: 32\n"
     ]
    }
   ],
   "source": [
    "# Build tensors\n",
    "\n",
    "n_dimensions = X_chords_embeddings.shape[1]\n",
    "print(\"Size of embedding vector for each chord: {}\".format(n_dimensions))\n",
    "\n",
    "X_melody = np.zeros((n_samples, max_mel_len, n_notes), dtype='float32')\n",
    "X_chords = np.zeros((n_samples, chord_context_len, n_dimensions), dtype='float32')\n",
    "Y_chord = np.zeros((n_samples, n_chords), dtype='float32')\n",
    "\n",
    "for i, (input_mel, input_ch, target_ch) in enumerate(zip(input_melody_data, input_chords_data, target_chords_data)):\n",
    "    Y_chord[i, chord_to_int[target_ch]] = 1\n",
    "    for j, chord in enumerate(input_ch):\n",
    "#         X_chords[i, j, chord_to_int[chord]] = 1\n",
    "        chord_index = chord_to_int[chord]\n",
    "        if (chord_index < num_ch):\n",
    "            X_chords[i, j, :] = X_chords_embeddings[chord_index, :]\n",
    "    \n",
    "    for j, note in enumerate(input_mel):\n",
    "        X_melody[i, j, note_to_int[note]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334344, 135, 14)\n",
      "(334344, 7, 32)\n",
      "(334344, 195)\n"
     ]
    }
   ],
   "source": [
    "print(X_melody.shape)\n",
    "print(X_chords.shape)\n",
    "print(Y_chord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into 80%-10%-10% train-valid-test sets\n",
    "seed = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "\n",
    "for train_index, aux_index in sss.split(X_chords, Y_chord):\n",
    "    X_melody_train, X_melody_aux = X_melody[train_index], X_melody[aux_index]\n",
    "    X_chords_train, X_chords_aux = X_chords[train_index], X_chords[aux_index]\n",
    "    Y_chord_train, Y_chord_aux = Y_chord[train_index], Y_chord[aux_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "\n",
    "for valid_index, test_index in sss.split(X_chords_aux, Y_chord_aux):\n",
    "    X_melody_valid, X_melody_test = X_melody[valid_index], X_melody[test_index]\n",
    "    X_chords_valid, X_chords_test = X_chords[valid_index], X_chords[test_index]\n",
    "    Y_chord_valid, Y_chord_test = Y_chord[valid_index], Y_chord[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/maxime/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 135, 14)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 7, 32)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           54912       input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 128)           61824       input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           gru_1[0][0]                      \n",
      "                                                                   gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 195)           50115       concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 166,851\n",
      "Trainable params: 166,851\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 276.00 264.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 272,-260 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140561326396920 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140561326396920</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 125,-255.5 125,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-233.8\">input_3: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140561326397200 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140561326397200</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-146.5 30.5,-182.5 114.5,-182.5 114.5,-146.5 30.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72.5\" y=\"-160.8\">gru_1: GRU</text>\n",
       "</g>\n",
       "<!-- 140561326396920&#45;&gt;140561326397200 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140561326396920-&gt;140561326397200</title>\n",
       "<path d=\"M64.9207,-219.313C66.0508,-211.289 67.4229,-201.547 68.6874,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.1726,-192.919 70.1016,-182.529 65.241,-191.943 72.1726,-192.919\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140561326397088 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140561326397088</title>\n",
       "<polygon fill=\"none\" points=\"143,-219.5 143,-255.5 268,-255.5 268,-219.5 143,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-233.8\">input_4: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140561326396528 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140561326396528</title>\n",
       "<polygon fill=\"none\" points=\"152.5,-146.5 152.5,-182.5 236.5,-182.5 236.5,-146.5 152.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-160.8\">gru_2: GRU</text>\n",
       "</g>\n",
       "<!-- 140561326397088&#45;&gt;140561326396528 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140561326397088-&gt;140561326396528</title>\n",
       "<path d=\"M202.837,-219.313C201.594,-211.289 200.085,-201.547 198.694,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.128,-191.875 197.138,-182.529 195.211,-192.947 202.128,-191.875\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140561326396024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140561326396024</title>\n",
       "<polygon fill=\"none\" points=\"49.5,-73.5 49.5,-109.5 217.5,-109.5 217.5,-73.5 49.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-87.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140561326397200&#45;&gt;140561326396024 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140561326397200-&gt;140561326396024</title>\n",
       "<path d=\"M87.2664,-146.313C94.7591,-137.592 103.996,-126.84 112.239,-117.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"115.008,-119.395 118.87,-109.529 109.698,-114.833 115.008,-119.395\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140561326396528&#45;&gt;140561326396024 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140561326396528-&gt;140561326396024</title>\n",
       "<path d=\"M179.734,-146.313C172.241,-137.592 163.004,-126.84 154.761,-117.246\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"157.302,-114.833 148.13,-109.529 151.992,-119.395 157.302,-114.833\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140560731597792 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140560731597792</title>\n",
       "<polygon fill=\"none\" points=\"82.5,-0.5 82.5,-36.5 184.5,-36.5 184.5,-0.5 82.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-14.8\">dense_9: Dense</text>\n",
       "</g>\n",
       "<!-- 140561326396024&#45;&gt;140560731597792 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140561326396024-&gt;140560731597792</title>\n",
       "<path d=\"M133.5,-73.3129C133.5,-65.2895 133.5,-55.5475 133.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-46.5288 133.5,-36.5288 130,-46.5289 137,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define neual net architecture\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "melody_input = Input(shape=(max_mel_len, n_notes))\n",
    "melody_gru = GRU(latent_dim)(melody_input)\n",
    "\n",
    "chords_input = Input(shape=(chord_context_len, n_dimensions))\n",
    "chords_gru = GRU(latent_dim)(chords_input)\n",
    "\n",
    "concat = concatenate([melody_gru, chords_gru])\n",
    "\n",
    "chord_dense = Dense(n_chords, activation='softmax')(concat)\n",
    "\n",
    "model = Model([melody_input, chords_input], chord_dense)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce Best-Performance callbacks\n",
    "# es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min')\n",
    "filepath = \"models/embeddings32-Mel1-Cho1-FC1_150ep.h5\"\n",
    "bp = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 267475 samples, validate on 33434 samples\n",
      "Epoch 1/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.8944 - acc: 0.3496Epoch 00000: val_acc improved from -inf to 0.41634, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 430s - loss: 2.8940 - acc: 0.3496 - val_loss: 2.5612 - val_acc: 0.4163\n",
      "Epoch 2/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.3802 - acc: 0.4445Epoch 00001: val_acc improved from 0.41634 to 0.45666, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 2.3802 - acc: 0.4445 - val_loss: 2.3411 - val_acc: 0.4567\n",
      "Epoch 3/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.1791 - acc: 0.4831Epoch 00002: val_acc improved from 0.45666 to 0.48995, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 2.1789 - acc: 0.4831 - val_loss: 2.2037 - val_acc: 0.4900\n",
      "Epoch 4/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.0366 - acc: 0.5148Epoch 00003: val_acc improved from 0.48995 to 0.51962, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 2.0367 - acc: 0.5147 - val_loss: 2.1163 - val_acc: 0.5196\n",
      "Epoch 5/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.9260 - acc: 0.5409Epoch 00004: val_acc improved from 0.51962 to 0.53111, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.9260 - acc: 0.5409 - val_loss: 2.0547 - val_acc: 0.5311\n",
      "Epoch 6/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.8362 - acc: 0.5622Epoch 00005: val_acc improved from 0.53111 to 0.54977, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.8360 - acc: 0.5622 - val_loss: 1.9890 - val_acc: 0.5498\n",
      "Epoch 7/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.7637 - acc: 0.5804Epoch 00006: val_acc improved from 0.54977 to 0.56209, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 1.7637 - acc: 0.5803 - val_loss: 1.9540 - val_acc: 0.5621\n",
      "Epoch 8/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.7021 - acc: 0.5949Epoch 00007: val_acc improved from 0.56209 to 0.56628, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 432s - loss: 1.7021 - acc: 0.5949 - val_loss: 1.9163 - val_acc: 0.5663\n",
      "Epoch 9/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.6483 - acc: 0.6074Epoch 00008: val_acc improved from 0.56628 to 0.57343, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 1.6483 - acc: 0.6074 - val_loss: 1.8892 - val_acc: 0.5734\n",
      "Epoch 10/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.6028 - acc: 0.6186Epoch 00009: val_acc improved from 0.57343 to 0.58007, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.6028 - acc: 0.6186 - val_loss: 1.8656 - val_acc: 0.5801\n",
      "Epoch 11/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.5629 - acc: 0.6276Epoch 00010: val_acc improved from 0.58007 to 0.58300, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.5629 - acc: 0.6276 - val_loss: 1.8461 - val_acc: 0.5830\n",
      "Epoch 12/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.5281 - acc: 0.6353Epoch 00011: val_acc improved from 0.58300 to 0.59203, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.5282 - acc: 0.6353 - val_loss: 1.8206 - val_acc: 0.5920\n",
      "Epoch 13/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4976 - acc: 0.6424Epoch 00012: val_acc improved from 0.59203 to 0.59251, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.4976 - acc: 0.6424 - val_loss: 1.8112 - val_acc: 0.5925\n",
      "Epoch 14/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4718 - acc: 0.6480Epoch 00013: val_acc improved from 0.59251 to 0.59706, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.4716 - acc: 0.6481 - val_loss: 1.7926 - val_acc: 0.5971\n",
      "Epoch 15/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4478 - acc: 0.6537Epoch 00014: val_acc improved from 0.59706 to 0.60292, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.4478 - acc: 0.6537 - val_loss: 1.7775 - val_acc: 0.6029\n",
      "Epoch 16/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4258 - acc: 0.6583Epoch 00015: val_acc improved from 0.60292 to 0.60480, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.4257 - acc: 0.6583 - val_loss: 1.7748 - val_acc: 0.6048\n",
      "Epoch 17/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4079 - acc: 0.6621Epoch 00016: val_acc improved from 0.60480 to 0.61085, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.4079 - acc: 0.6621 - val_loss: 1.7650 - val_acc: 0.6108\n",
      "Epoch 18/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3901 - acc: 0.6654Epoch 00017: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.3901 - acc: 0.6654 - val_loss: 1.7541 - val_acc: 0.6101\n",
      "Epoch 19/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3734 - acc: 0.6697Epoch 00018: val_acc improved from 0.61085 to 0.61587, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.3733 - acc: 0.6698 - val_loss: 1.7443 - val_acc: 0.6159\n",
      "Epoch 20/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3591 - acc: 0.6731Epoch 00019: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 1.3592 - acc: 0.6731 - val_loss: 1.7351 - val_acc: 0.6157\n",
      "Epoch 21/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3454 - acc: 0.6756Epoch 00020: val_acc improved from 0.61587 to 0.62000, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.3456 - acc: 0.6755 - val_loss: 1.7346 - val_acc: 0.6200\n",
      "Epoch 22/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3330 - acc: 0.6779Epoch 00021: val_acc improved from 0.62000 to 0.62164, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.3330 - acc: 0.6779 - val_loss: 1.7215 - val_acc: 0.6216\n",
      "Epoch 23/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3214 - acc: 0.6801Epoch 00022: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.3213 - acc: 0.6801 - val_loss: 1.7220 - val_acc: 0.6204\n",
      "Epoch 24/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3112 - acc: 0.6817Epoch 00023: val_acc improved from 0.62164 to 0.62332, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.3113 - acc: 0.6817 - val_loss: 1.7120 - val_acc: 0.6233\n",
      "Epoch 25/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3019 - acc: 0.6837Epoch 00024: val_acc improved from 0.62332 to 0.62640, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 1.3019 - acc: 0.6837 - val_loss: 1.7043 - val_acc: 0.6264\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2919 - acc: 0.6859Epoch 00025: val_acc improved from 0.62640 to 0.62721, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.2920 - acc: 0.6859 - val_loss: 1.7038 - val_acc: 0.6272\n",
      "Epoch 27/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2832 - acc: 0.6876Epoch 00026: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 1.2831 - acc: 0.6876 - val_loss: 1.6989 - val_acc: 0.6262\n",
      "Epoch 28/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2759 - acc: 0.6887Epoch 00027: val_acc improved from 0.62721 to 0.62798, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.2759 - acc: 0.6887 - val_loss: 1.6959 - val_acc: 0.6280\n",
      "Epoch 29/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2680 - acc: 0.6904Epoch 00028: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 1.2680 - acc: 0.6904 - val_loss: 1.6869 - val_acc: 0.6274\n",
      "Epoch 30/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2612 - acc: 0.6920Epoch 00029: val_acc improved from 0.62798 to 0.63136, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.2611 - acc: 0.6920 - val_loss: 1.6884 - val_acc: 0.6314\n",
      "Epoch 31/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2538 - acc: 0.6932Epoch 00030: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.2539 - acc: 0.6931 - val_loss: 1.6881 - val_acc: 0.6293\n",
      "Epoch 32/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2478 - acc: 0.6940Epoch 00031: val_acc improved from 0.63136 to 0.63307, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.2479 - acc: 0.6940 - val_loss: 1.6847 - val_acc: 0.6331\n",
      "Epoch 33/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2418 - acc: 0.6951Epoch 00032: val_acc improved from 0.63307 to 0.63489, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.2418 - acc: 0.6951 - val_loss: 1.6757 - val_acc: 0.6349\n",
      "Epoch 34/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2352 - acc: 0.6964Epoch 00033: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.2353 - acc: 0.6963 - val_loss: 1.6783 - val_acc: 0.6340\n",
      "Epoch 35/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2308 - acc: 0.6973Epoch 00034: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.2308 - acc: 0.6973 - val_loss: 1.6724 - val_acc: 0.6341\n",
      "Epoch 36/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2256 - acc: 0.6982Epoch 00035: val_acc improved from 0.63489 to 0.63687, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.2256 - acc: 0.6982 - val_loss: 1.6716 - val_acc: 0.6369\n",
      "Epoch 37/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2210 - acc: 0.6991Epoch 00036: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.2211 - acc: 0.6990 - val_loss: 1.6772 - val_acc: 0.6365\n",
      "Epoch 38/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2159 - acc: 0.7005Epoch 00037: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.2161 - acc: 0.7005 - val_loss: 1.6774 - val_acc: 0.6356\n",
      "Epoch 39/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2122 - acc: 0.7013Epoch 00038: val_acc improved from 0.63687 to 0.64016, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.2123 - acc: 0.7013 - val_loss: 1.6684 - val_acc: 0.6402\n",
      "Epoch 40/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2144 - acc: 0.7005Epoch 00039: val_acc improved from 0.64016 to 0.64126, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.2142 - acc: 0.7006 - val_loss: 1.6642 - val_acc: 0.6413\n",
      "Epoch 41/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2119 - acc: 0.7001Epoch 00040: val_acc improved from 0.64126 to 0.64333, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.2119 - acc: 0.7001 - val_loss: 1.6599 - val_acc: 0.6433\n",
      "Epoch 42/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1981 - acc: 0.7032Epoch 00041: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.1980 - acc: 0.7032 - val_loss: 1.6610 - val_acc: 0.6373\n",
      "Epoch 43/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1904 - acc: 0.7042Epoch 00042: val_acc improved from 0.64333 to 0.64357, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.1905 - acc: 0.7041 - val_loss: 1.6466 - val_acc: 0.6436\n",
      "Epoch 44/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1825 - acc: 0.7058Epoch 00043: val_acc improved from 0.64357 to 0.64381, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.1825 - acc: 0.7058 - val_loss: 1.6436 - val_acc: 0.6438\n",
      "Epoch 45/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1753 - acc: 0.7078Epoch 00044: val_acc improved from 0.64381 to 0.64422, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.1753 - acc: 0.7078 - val_loss: 1.6394 - val_acc: 0.6442\n",
      "Epoch 46/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1700 - acc: 0.7086Epoch 00045: val_acc improved from 0.64422 to 0.64428, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.1700 - acc: 0.7086 - val_loss: 1.6467 - val_acc: 0.6443\n",
      "Epoch 47/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1641 - acc: 0.7097Epoch 00046: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.1641 - acc: 0.7097 - val_loss: 1.6533 - val_acc: 0.6428\n",
      "Epoch 48/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1725 - acc: 0.7076Epoch 00047: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.1724 - acc: 0.7077 - val_loss: 1.6497 - val_acc: 0.6430\n",
      "Epoch 49/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1617 - acc: 0.7092Epoch 00048: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 1.1618 - acc: 0.7092 - val_loss: 1.6563 - val_acc: 0.6410\n",
      "Epoch 50/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1571 - acc: 0.7100Epoch 00049: val_acc improved from 0.64428 to 0.64533, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.1573 - acc: 0.7099 - val_loss: 1.6486 - val_acc: 0.6453\n",
      "Epoch 51/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1501 - acc: 0.7116Epoch 00050: val_acc improved from 0.64533 to 0.64802, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.1501 - acc: 0.7116 - val_loss: 1.6407 - val_acc: 0.6480\n",
      "Epoch 52/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1402 - acc: 0.7131Epoch 00051: val_acc improved from 0.64802 to 0.64997, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.1401 - acc: 0.7131 - val_loss: 1.6283 - val_acc: 0.6500\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1308 - acc: 0.7152Epoch 00052: val_acc improved from 0.64997 to 0.65000, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 1.1308 - acc: 0.7152 - val_loss: 1.6235 - val_acc: 0.6500\n",
      "Epoch 54/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1217 - acc: 0.7163Epoch 00053: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.1219 - acc: 0.7163 - val_loss: 1.6421 - val_acc: 0.6467\n",
      "Epoch 55/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1342 - acc: 0.7137Epoch 00054: val_acc improved from 0.65000 to 0.65221, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.1344 - acc: 0.7137 - val_loss: 1.6210 - val_acc: 0.6522\n",
      "Epoch 56/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1224 - acc: 0.7161Epoch 00055: val_acc improved from 0.65221 to 0.65248, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.1226 - acc: 0.7161 - val_loss: 1.6075 - val_acc: 0.6525\n",
      "Epoch 57/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1166 - acc: 0.7165Epoch 00056: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.1163 - acc: 0.7166 - val_loss: 1.6121 - val_acc: 0.6507\n",
      "Epoch 58/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1117 - acc: 0.7175Epoch 00057: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.1116 - acc: 0.7175 - val_loss: 1.6067 - val_acc: 0.6519\n",
      "Epoch 59/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1077 - acc: 0.7186Epoch 00058: val_acc improved from 0.65248 to 0.65565, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.1075 - acc: 0.7186 - val_loss: 1.6026 - val_acc: 0.6556\n",
      "Epoch 60/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1042 - acc: 0.7199Epoch 00059: val_acc improved from 0.65565 to 0.65592, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.1044 - acc: 0.7199 - val_loss: 1.5971 - val_acc: 0.6559\n",
      "Epoch 61/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1006 - acc: 0.7198Epoch 00060: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 1.1006 - acc: 0.7198 - val_loss: 1.6011 - val_acc: 0.6511\n",
      "Epoch 62/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0977 - acc: 0.7201Epoch 00061: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.0977 - acc: 0.7201 - val_loss: 1.6084 - val_acc: 0.6521\n",
      "Epoch 63/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0941 - acc: 0.7209Epoch 00062: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.0942 - acc: 0.7209 - val_loss: 1.6019 - val_acc: 0.6512\n",
      "Epoch 64/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0925 - acc: 0.7207Epoch 00063: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.0926 - acc: 0.7207 - val_loss: 1.5930 - val_acc: 0.6555\n",
      "Epoch 65/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0894 - acc: 0.7215Epoch 00064: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 1.0895 - acc: 0.7214 - val_loss: 1.5928 - val_acc: 0.6533\n",
      "Epoch 66/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.7223Epoch 00065: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.0901 - acc: 0.7223 - val_loss: 1.6071 - val_acc: 0.6541\n",
      "Epoch 67/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.7223Epoch 00066: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.0883 - acc: 0.7222 - val_loss: 1.6042 - val_acc: 0.6549\n",
      "Epoch 68/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0828 - acc: 0.7234Epoch 00067: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.0827 - acc: 0.7234 - val_loss: 1.5962 - val_acc: 0.6552\n",
      "Epoch 69/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0841 - acc: 0.7230Epoch 00068: val_acc improved from 0.65592 to 0.65673, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 1.0839 - acc: 0.7231 - val_loss: 1.5974 - val_acc: 0.6567\n",
      "Epoch 70/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0827 - acc: 0.7225Epoch 00069: val_acc improved from 0.65673 to 0.65694, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.0827 - acc: 0.7225 - val_loss: 1.6006 - val_acc: 0.6569\n",
      "Epoch 71/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0778 - acc: 0.7239Epoch 00070: val_acc improved from 0.65694 to 0.65969, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.0779 - acc: 0.7238 - val_loss: 1.5895 - val_acc: 0.6597\n",
      "Epoch 72/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0767 - acc: 0.7233Epoch 00071: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.0766 - acc: 0.7233 - val_loss: 1.5944 - val_acc: 0.6552\n",
      "Epoch 73/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0771 - acc: 0.7241Epoch 00072: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 1.0772 - acc: 0.7240 - val_loss: 1.5932 - val_acc: 0.6585\n",
      "Epoch 74/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0732 - acc: 0.7243Epoch 00073: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 1.0731 - acc: 0.7243 - val_loss: 1.5888 - val_acc: 0.6563\n",
      "Epoch 75/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.7254Epoch 00074: val_acc did not improve\n",
      "267475/267475 [==============================] - 431s - loss: 1.0657 - acc: 0.7254 - val_loss: 1.5884 - val_acc: 0.6557\n",
      "Epoch 76/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0557 - acc: 0.7277Epoch 00075: val_acc improved from 0.65969 to 0.66056, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 1.0557 - acc: 0.7277 - val_loss: 1.5689 - val_acc: 0.6606\n",
      "Epoch 77/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0466 - acc: 0.7296Epoch 00076: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 1.0467 - acc: 0.7295 - val_loss: 1.5626 - val_acc: 0.6597\n",
      "Epoch 78/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0398 - acc: 0.7305Epoch 00077: val_acc improved from 0.66056 to 0.66160, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 1.0396 - acc: 0.7305 - val_loss: 1.5549 - val_acc: 0.6616\n",
      "Epoch 79/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0284 - acc: 0.7324Epoch 00078: val_acc improved from 0.66160 to 0.67010, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 1.0284 - acc: 0.7324 - val_loss: 1.5344 - val_acc: 0.6701\n",
      "Epoch 80/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0200 - acc: 0.7336Epoch 00079: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 1.0201 - acc: 0.7336 - val_loss: 1.5350 - val_acc: 0.6665\n",
      "Epoch 81/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0117 - acc: 0.7353Epoch 00080: val_acc improved from 0.67010 to 0.67219, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267475/267475 [==============================] - 432s - loss: 1.0118 - acc: 0.7353 - val_loss: 1.5248 - val_acc: 0.6722\n",
      "Epoch 82/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0028 - acc: 0.7374Epoch 00081: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 1.0029 - acc: 0.7374 - val_loss: 1.5205 - val_acc: 0.6670\n",
      "Epoch 83/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9932 - acc: 0.7386Epoch 00082: val_acc improved from 0.67219 to 0.67467, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 0.9931 - acc: 0.7386 - val_loss: 1.4929 - val_acc: 0.6747\n",
      "Epoch 84/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.7406Epoch 00083: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.9846 - acc: 0.7406 - val_loss: 1.4948 - val_acc: 0.6742\n",
      "Epoch 85/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9779 - acc: 0.7412Epoch 00084: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.9778 - acc: 0.7413 - val_loss: 1.5159 - val_acc: 0.6705\n",
      "Epoch 86/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9870 - acc: 0.7394Epoch 00085: val_acc improved from 0.67467 to 0.67958, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.9869 - acc: 0.7394 - val_loss: 1.4746 - val_acc: 0.6796\n",
      "Epoch 87/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9692 - acc: 0.7433Epoch 00086: val_acc improved from 0.67958 to 0.68015, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 436s - loss: 0.9692 - acc: 0.7432 - val_loss: 1.4796 - val_acc: 0.6801\n",
      "Epoch 88/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9606 - acc: 0.7449Epoch 00087: val_acc improved from 0.68015 to 0.68062, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 0.9605 - acc: 0.7449 - val_loss: 1.4673 - val_acc: 0.6806\n",
      "Epoch 89/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.7469Epoch 00088: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.9545 - acc: 0.7469 - val_loss: 1.4623 - val_acc: 0.6787\n",
      "Epoch 90/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9465 - acc: 0.7481Epoch 00089: val_acc improved from 0.68062 to 0.68242, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.9468 - acc: 0.7480 - val_loss: 1.4612 - val_acc: 0.6824\n",
      "Epoch 91/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9410 - acc: 0.7496Epoch 00090: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 0.9411 - acc: 0.7496 - val_loss: 1.4645 - val_acc: 0.6808\n",
      "Epoch 92/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9375 - acc: 0.7501Epoch 00091: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 0.9376 - acc: 0.7501 - val_loss: 1.4615 - val_acc: 0.6822\n",
      "Epoch 93/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.7519Epoch 00092: val_acc improved from 0.68242 to 0.68526, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 0.9295 - acc: 0.7519 - val_loss: 1.4475 - val_acc: 0.6853\n",
      "Epoch 94/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.7545Epoch 00093: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.9225 - acc: 0.7545 - val_loss: 1.4481 - val_acc: 0.6826\n",
      "Epoch 95/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9261 - acc: 0.7530Epoch 00094: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.9261 - acc: 0.7530 - val_loss: 1.4534 - val_acc: 0.6806\n",
      "Epoch 96/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9172 - acc: 0.7548Epoch 00095: val_acc improved from 0.68526 to 0.68652, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.9171 - acc: 0.7548 - val_loss: 1.4389 - val_acc: 0.6865\n",
      "Epoch 97/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9068 - acc: 0.7565Epoch 00096: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.9067 - acc: 0.7565 - val_loss: 1.4429 - val_acc: 0.6861\n",
      "Epoch 98/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.7583Epoch 00097: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.9018 - acc: 0.7583 - val_loss: 1.4392 - val_acc: 0.6865\n",
      "Epoch 99/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7595Epoch 00098: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8962 - acc: 0.7595 - val_loss: 1.4534 - val_acc: 0.6844\n",
      "Epoch 100/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7608Epoch 00099: val_acc improved from 0.68652 to 0.68822, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.8905 - acc: 0.7608 - val_loss: 1.4361 - val_acc: 0.6882\n",
      "Epoch 101/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8868 - acc: 0.7616Epoch 00100: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8867 - acc: 0.7616 - val_loss: 1.4431 - val_acc: 0.6862\n",
      "Epoch 102/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8805 - acc: 0.7629Epoch 00101: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8805 - acc: 0.7629 - val_loss: 1.4400 - val_acc: 0.6849\n",
      "Epoch 103/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.7415Epoch 00102: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 0.9645 - acc: 0.7415 - val_loss: 1.4992 - val_acc: 0.6746\n",
      "Epoch 104/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9526 - acc: 0.7457Epoch 00103: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 0.9527 - acc: 0.7457 - val_loss: 1.4852 - val_acc: 0.6794\n",
      "Epoch 105/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9416 - acc: 0.7485Epoch 00104: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.9416 - acc: 0.7485 - val_loss: 1.4834 - val_acc: 0.6805\n",
      "Epoch 106/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9299 - acc: 0.7510Epoch 00105: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.9299 - acc: 0.7510 - val_loss: 1.4678 - val_acc: 0.6819\n",
      "Epoch 107/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9171 - acc: 0.7542Epoch 00106: val_acc did not improve\n",
      "267475/267475 [==============================] - 428s - loss: 0.9170 - acc: 0.7542 - val_loss: 1.4644 - val_acc: 0.6830\n",
      "Epoch 108/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9061 - acc: 0.7565Epoch 00107: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.9061 - acc: 0.7565 - val_loss: 1.4580 - val_acc: 0.6853\n",
      "Epoch 109/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8983 - acc: 0.7582Epoch 00108: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8984 - acc: 0.7582 - val_loss: 1.4680 - val_acc: 0.6788\n",
      "Epoch 110/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8899 - acc: 0.7603Epoch 00109: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8902 - acc: 0.7602 - val_loss: 1.4475 - val_acc: 0.6869\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7635Epoch 00110: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8783 - acc: 0.7635 - val_loss: 1.4432 - val_acc: 0.6877\n",
      "Epoch 112/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8884 - acc: 0.7604Epoch 00111: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8884 - acc: 0.7604 - val_loss: 1.4465 - val_acc: 0.6876\n",
      "Epoch 113/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8749 - acc: 0.7634Epoch 00112: val_acc improved from 0.68822 to 0.68846, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 429s - loss: 0.8750 - acc: 0.7635 - val_loss: 1.4423 - val_acc: 0.6885\n",
      "Epoch 114/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8614 - acc: 0.7671Epoch 00113: val_acc improved from 0.68846 to 0.69064, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.8614 - acc: 0.7671 - val_loss: 1.4484 - val_acc: 0.6906\n",
      "Epoch 115/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8661 - acc: 0.7659Epoch 00114: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8661 - acc: 0.7659 - val_loss: 1.4434 - val_acc: 0.6901\n",
      "Epoch 116/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7675Epoch 00115: val_acc did not improve\n",
      "267475/267475 [==============================] - 436s - loss: 0.8595 - acc: 0.7675 - val_loss: 1.4456 - val_acc: 0.6828\n",
      "Epoch 117/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7678Epoch 00116: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8572 - acc: 0.7678 - val_loss: 1.5106 - val_acc: 0.6742\n",
      "Epoch 118/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8793 - acc: 0.7621Epoch 00117: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8792 - acc: 0.7621 - val_loss: 1.4440 - val_acc: 0.6886\n",
      "Epoch 119/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8538 - acc: 0.7692Epoch 00118: val_acc improved from 0.69064 to 0.69265, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 0.8537 - acc: 0.7692 - val_loss: 1.4367 - val_acc: 0.6926\n",
      "Epoch 120/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8463 - acc: 0.7701Epoch 00119: val_acc improved from 0.69265 to 0.69429, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 435s - loss: 0.8465 - acc: 0.7701 - val_loss: 1.4328 - val_acc: 0.6943\n",
      "Epoch 121/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.7717Epoch 00120: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8400 - acc: 0.7717 - val_loss: 1.4444 - val_acc: 0.6907\n",
      "Epoch 122/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.7737Epoch 00121: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8336 - acc: 0.7736 - val_loss: 1.4402 - val_acc: 0.6887\n",
      "Epoch 123/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8402 - acc: 0.7719Epoch 00122: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8401 - acc: 0.7719 - val_loss: 1.4320 - val_acc: 0.6941\n",
      "Epoch 124/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7714Epoch 00123: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8397 - acc: 0.7715 - val_loss: 1.4595 - val_acc: 0.6866\n",
      "Epoch 125/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8410 - acc: 0.7716Epoch 00124: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8410 - acc: 0.7716 - val_loss: 1.4730 - val_acc: 0.6806\n",
      "Epoch 126/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8570 - acc: 0.7671Epoch 00125: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8569 - acc: 0.7671 - val_loss: 1.4517 - val_acc: 0.6904\n",
      "Epoch 127/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8314 - acc: 0.7738Epoch 00126: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8314 - acc: 0.7738 - val_loss: 1.4431 - val_acc: 0.6927\n",
      "Epoch 128/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8234 - acc: 0.7766Epoch 00127: val_acc improved from 0.69429 to 0.69492, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 434s - loss: 0.8235 - acc: 0.7766 - val_loss: 1.4352 - val_acc: 0.6949\n",
      "Epoch 129/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8193 - acc: 0.7772Epoch 00128: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8195 - acc: 0.7772 - val_loss: 1.4583 - val_acc: 0.6911\n",
      "Epoch 130/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8134 - acc: 0.7785Epoch 00129: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8135 - acc: 0.7785 - val_loss: 1.4398 - val_acc: 0.6928\n",
      "Epoch 131/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8082 - acc: 0.7792Epoch 00130: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 0.8081 - acc: 0.7792 - val_loss: 1.4313 - val_acc: 0.6943\n",
      "Epoch 132/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7816Epoch 00131: val_acc improved from 0.69492 to 0.69597, saving model to models/embeddings32-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 433s - loss: 0.8028 - acc: 0.7816 - val_loss: 1.4322 - val_acc: 0.6960\n",
      "Epoch 133/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8098 - acc: 0.7787Epoch 00132: val_acc did not improve\n",
      "267475/267475 [==============================] - 434s - loss: 0.8099 - acc: 0.7787 - val_loss: 1.4394 - val_acc: 0.6944\n",
      "Epoch 134/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8012 - acc: 0.7817Epoch 00133: val_acc did not improve\n",
      "267475/267475 [==============================] - 433s - loss: 0.8013 - acc: 0.7816 - val_loss: 1.4527 - val_acc: 0.6868\n",
      "Epoch 135/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8035 - acc: 0.7797Epoch 00134: val_acc did not improve\n",
      "267475/267475 [==============================] - 432s - loss: 0.8035 - acc: 0.7796 - val_loss: 1.4398 - val_acc: 0.6953\n",
      "Epoch 136/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8095 - acc: 0.7794Epoch 00135: val_acc did not improve\n",
      "267475/267475 [==============================] - 431s - loss: 0.8097 - acc: 0.7794 - val_loss: 1.5168 - val_acc: 0.6754\n",
      "Epoch 137/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9020 - acc: 0.7542Epoch 00136: val_acc did not improve\n",
      "267475/267475 [==============================] - 431s - loss: 0.9020 - acc: 0.7542 - val_loss: 1.4994 - val_acc: 0.6804\n",
      "Epoch 138/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8808 - acc: 0.7604Epoch 00137: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8808 - acc: 0.7604 - val_loss: 1.4800 - val_acc: 0.6838\n",
      "Epoch 139/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8746 - acc: 0.7613Epoch 00138: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8747 - acc: 0.7613 - val_loss: 1.4849 - val_acc: 0.6840\n",
      "Epoch 140/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8697 - acc: 0.7637Epoch 00139: val_acc did not improve\n",
      "267475/267475 [==============================] - 431s - loss: 0.8698 - acc: 0.7637 - val_loss: 1.4758 - val_acc: 0.6847\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8655 - acc: 0.7640Epoch 00140: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8654 - acc: 0.7641 - val_loss: 1.4653 - val_acc: 0.6900\n",
      "Epoch 142/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8639 - acc: 0.7649Epoch 00141: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8639 - acc: 0.7649 - val_loss: 1.4718 - val_acc: 0.6860\n",
      "Epoch 143/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.7660Epoch 00142: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8615 - acc: 0.7660 - val_loss: 1.4768 - val_acc: 0.6862\n",
      "Epoch 144/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7660Epoch 00143: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8596 - acc: 0.7660 - val_loss: 1.4759 - val_acc: 0.6855\n",
      "Epoch 145/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7671Epoch 00144: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8581 - acc: 0.7671 - val_loss: 1.4675 - val_acc: 0.6883\n",
      "Epoch 146/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8570 - acc: 0.7671Epoch 00145: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8570 - acc: 0.7671 - val_loss: 1.4588 - val_acc: 0.6906\n",
      "Epoch 147/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8544 - acc: 0.7672Epoch 00146: val_acc did not improve\n",
      "267475/267475 [==============================] - 429s - loss: 0.8545 - acc: 0.7672 - val_loss: 1.4675 - val_acc: 0.6869\n",
      "Epoch 148/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8522 - acc: 0.7682Epoch 00147: val_acc did not improve\n",
      "267475/267475 [==============================] - 430s - loss: 0.8522 - acc: 0.7682 - val_loss: 1.4613 - val_acc: 0.6909\n",
      "Epoch 149/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8517 - acc: 0.7674Epoch 00148: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8517 - acc: 0.7674 - val_loss: 1.4696 - val_acc: 0.6876\n",
      "Epoch 150/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8482 - acc: 0.7686Epoch 00149: val_acc did not improve\n",
      "267475/267475 [==============================] - 435s - loss: 0.8482 - acc: 0.7686 - val_loss: 1.4691 - val_acc: 0.6888\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit([X_melody_train, X_chords_train], Y_chord_train, epochs=epochs, validation_data=([X_melody_valid, X_chords_valid], Y_chord_valid), batch_size=batch_size, callbacks=[bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd768a77c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFkCAYAAADWs8tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd768a204a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model learning\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Training history of root model\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best performance model\n",
    "best_model = load_model(\"models/embeddings32-Mel1-Cho1-FC1_150ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33435, 135, 14)\n",
      "(33435, 7, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X_melody_test.shape)\n",
    "print(X_chords_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord prediction: 0.6886\n",
      "Kappa score of combined chord prediction: 0.6809\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of labels\n",
    "\n",
    "# Predict chords from each test sample melody\n",
    "Y_chord_pred = model.predict([X_melody_test, X_chords_test])\n",
    "\n",
    "# Compute accuracy and kappa score \n",
    "print(\"Categorical accuracy of combined chord prediction: {0:.4f}\".format(harmoutil.compute_accuracy_score(Y_chord_test, Y_chord_pred)))\n",
    "print(\"Kappa score of combined chord prediction: {0:.4f}\".format(harmoutil.compute_kappa_score(Y_chord_test, Y_chord_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord pitch prediction: 0.8890\n",
      "TP: 108343 TN: 248358 FP: 22409 FN: 22110\n",
      "Kappa score of combined chord pitch prediction: 0.7473\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of pitches\n",
    "\n",
    "def label_to_pitch_tensors(predictions):\n",
    "    predicted_chords = [int_to_chord[np.argmax(ch)] for ch in predictions]\n",
    "    pitch_chords = [harmoutil.chord_to_notes(label) for label in predicted_chords]\n",
    "    \n",
    "    Y_pitches = np.zeros((predictions.shape[0], 12), dtype='float32')\n",
    "    for i, chord_pitches in enumerate(pitch_chords):\n",
    "        for j, pitch_presence in enumerate(chord_pitches):\n",
    "            Y_pitches[i, j] = pitch_presence\n",
    "\n",
    "    return Y_pitches\n",
    "\n",
    "Y_pitch_pred = label_to_pitch_tensors(Y_chord_pred)\n",
    "Y_pitch_test = label_to_pitch_tensors(Y_chord_test)\n",
    "\n",
    "print(\"Categorical accuracy of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_accuracy_score(Y_pitch_test, Y_pitch_pred)))\n",
    "print(\"Kappa score of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_kappa_score(Y_pitch_test, Y_pitch_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
