{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, GRU, concatenate\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Custom library for the project\n",
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "import harmoutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'harmoutil' from '../../src/harmoutil.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Remove when done with kernel\n",
    "import importlib\n",
    "importlib.reload(harmoutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = harmoutil.load_pickled_data(\"../../data/refined_data.pkl\") # lists of (chord label, melody seqs) by sections\n",
    "augmented_data = harmoutil.transpose_and_augment_data(raw_data)\n",
    "data = [harmoutil.to_sevenths(section) for section in augmented_data]\n",
    "data = [harmoutil.melody_to_octave_range(section) for section in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chords: 334344 | Sample chord: E6\n",
      "Number of melody notes in the data: 2209944 | Sample melody note: 4\n",
      "Unique notes: [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Unique notes: ['A', 'A+', 'A+7', 'A+j7', 'A-', 'A-6', 'A-7', 'A-j7', 'A6', 'A7', 'Ab', 'Ab+', 'Ab+7', 'Ab+j7', 'Ab-', 'Ab-6', 'Ab-7', 'Ab-j7', 'Ab6', 'Ab7', 'Abj7', 'Abm7b5', 'Abo', 'Abo7', 'Absus', 'Absus7', 'Aj7', 'Am7b5', 'Ao', 'Ao7', 'Asus', 'Asus7', 'B', 'B+', 'B+7', 'B+j7', 'B-', 'B-6', 'B-7', 'B-j7', 'B6', 'B7', 'Bb', 'Bb+', 'Bb+7', 'Bb+j7', 'Bb-', 'Bb-6', 'Bb-7', 'Bb-j7', 'Bb6', 'Bb7', 'Bbj7', 'Bbm7b5', 'Bbo', 'Bbo7', 'Bbsus', 'Bbsus7', 'Bj7', 'Bm7b5', 'Bo', 'Bo7', 'Bsus', 'Bsus7', 'C', 'C+', 'C+7', 'C+j7', 'C-', 'C-6', 'C-7', 'C-j7', 'C6', 'C7', 'Cj7', 'Cm7b5', 'Co', 'Co7', 'Csus', 'Csus7', 'D', 'D+', 'D+7', 'D+j7', 'D-', 'D-6', 'D-7', 'D-j7', 'D6', 'D7', 'Db', 'Db+', 'Db+7', 'Db+j7', 'Db-', 'Db-6', 'Db-7', 'Db-j7', 'Db6', 'Db7', 'Dbj7', 'Dbm7b5', 'Dbo', 'Dbo7', 'Dbsus', 'Dbsus7', 'Dj7', 'Dm7b5', 'Do', 'Do7', 'Dsus', 'Dsus7', 'E', 'E+', 'E+7', 'E+j7', 'E-', 'E-6', 'E-7', 'E-j7', 'E6', 'E7', 'Eb', 'Eb+', 'Eb+7', 'Eb+j7', 'Eb-', 'Eb-6', 'Eb-7', 'Eb-j7', 'Eb6', 'Eb7', 'Ebj7', 'Ebm7b5', 'Ebo', 'Ebo7', 'Ebsus', 'Ebsus7', 'Ej7', 'Em7b5', 'Eo', 'Eo7', 'Esus', 'Esus7', 'F', 'F+', 'F+7', 'F+j7', 'F-', 'F-6', 'F-7', 'F-j7', 'F6', 'F7', 'Fj7', 'Fm7b5', 'Fo', 'Fo7', 'Fsus', 'Fsus7', 'G', 'G+', 'G+7', 'G+j7', 'G-', 'G-6', 'G-7', 'G-j7', 'G6', 'G7', 'Gb', 'Gb+', 'Gb+7', 'Gb+j7', 'Gb-', 'Gb-6', 'Gb-7', 'Gb-j7', 'Gb6', 'Gb7', 'Gbj7', 'Gbm7b5', 'Gbo', 'Gbo7', 'Gbsus', 'Gbsus7', 'Gj7', 'Gm7b5', 'Go', 'Go7', 'Gsus', 'Gsus7', 'NC']\n"
     ]
    }
   ],
   "source": [
    "# Isolate relevant data\n",
    "def get_notes_by_chord(beats):\n",
    "    return [note for beat in beats for note in beat]\n",
    "\n",
    "def get_chords_by_section(section):\n",
    "    return [chord_info[0] for chord_info in section]\n",
    "\n",
    "# def check_if_augmented_major(section):\n",
    "#     section_chords = get_chords_by_section(section)\n",
    "#     for ch in section_chords:\n",
    "#         if \"+j7\" in ch:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "# data = [section for section in data if not check_if_augmented_major(section)]\n",
    "# print(\"---Remove sections with augmented major chord---\")\n",
    "# print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections_bf_augmaj7 = [get_chords_by_section(section) for section in data]\n",
    "chords = [chord_info[0] for section in data for chord_info in section]\n",
    "unique_chords = sorted(list(set(chords)))\n",
    "\n",
    "notes_by_chords_bf_augmaj7 = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "notes = [note for chord_notes in notes_by_chords_bf_augmaj7 for note in chord_notes]\n",
    "unique_notes = sorted(list(set(notes)))\n",
    "\n",
    "# print(sum([len(section) for section in chords_by_sections]))\n",
    "# print(\"Number of sections: {} | Sample section chords: {}\".format(len(chords_by_sections), chords_by_sections[0]))\n",
    "print(\"Number of chords: {} | Sample chord: {}\".format(len(chords), chords[0]))\n",
    "# print(\"Number of melodies {} | Sample melody: {}\".format(len(notes_by_chords), notes_by_chords[0]))\n",
    "print(\"Number of melody notes in the data: {} | Sample melody note: {}\".format(len(notes), notes[0]))\n",
    "print(\"Unique notes: {}\".format(unique_notes))\n",
    "print(\"Unique notes: {}\".format(unique_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody note to integer mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, '<pad>': 13, -1: 12}\n",
      "\n",
      "Integer to melody note mapping:\n",
      " {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: -1, 13: '<pad>'}\n",
      "\n",
      "Chord label to integer mapping:\n",
      " {'G6': 168, 'Ebj7': 132, 'F-6': 149, 'Fj7': 154, 'Ebm7b5': 133, 'Go': 188, 'G-6': 165, 'G-j7': 167, 'F7': 153, 'Ab-7': 16, 'G+7': 162, 'Db7': 99, 'Ebo': 134, 'A-': 4, 'Cm7b5': 75, 'Eb+': 123, 'D+j7': 83, 'Bsus7': 63, 'Ab+': 11, 'Ab6': 18, 'E7': 121, 'Absus': 24, 'Abm7b5': 21, 'Dbsus7': 105, 'A7': 9, 'Ao7': 29, 'Bb-7': 48, 'E-7': 118, 'Eb6': 130, 'Ab-6': 15, 'D7': 89, 'G': 160, 'Bo': 60, 'Eo': 140, 'B-7': 38, 'Ab-': 14, 'Db-6': 95, 'C-7': 70, 'Bbm7b5': 53, 'Gb-': 174, 'Fsus': 158, 'F-': 148, 'Ao': 28, 'Esus7': 143, 'B+j7': 35, 'Ej7': 138, 'Gb-6': 175, 'Ab-j7': 17, 'Db-': 94, 'F6': 152, 'Gb': 170, 'D': 80, 'Db-7': 96, 'B6': 40, 'Bb+j7': 45, 'Gb+7': 172, 'NC': 192, 'B': 32, 'Bbo': 54, 'Abj7': 20, 'Dj7': 106, 'Gb6': 178, 'A+': 1, 'Eb-6': 127, 'Bb7': 51, 'D-6': 85, 'F': 144, 'Dbm7b5': 101, 'G-7': 166, 'Cj7': 74, 'E-6': 117, 'Gj7': 186, 'Bbo7': 55, 'Bb-j7': 49, 'Eb-': 126, 'Do': 108, 'C6': 72, '<bos>': 193, 'Fsus7': 159, 'Gb+': 171, 'E+j7': 115, 'Co7': 77, 'Gbo': 182, 'G+j7': 163, 'Gm7b5': 187, 'Dbo7': 103, 'Em7b5': 139, 'E-': 116, 'D+': 81, 'C': 64, 'Gbj7': 180, 'Bbsus': 56, 'Eb-7': 128, 'A+j7': 3, 'Ab+j7': 13, 'Dsus7': 111, 'F+7': 146, 'Gb-j7': 177, 'Fo': 156, 'Db+j7': 93, 'Bb': 42, 'D+7': 82, 'D-': 84, 'Gbm7b5': 181, 'D-7': 86, 'Db6': 98, 'E+': 113, 'Abo': 22, '<eos>': 194, 'Gb7': 179, 'C-': 68, 'A+7': 2, 'B-': 36, 'Bbsus7': 57, 'Asus7': 31, 'C7': 73, 'Ab': 10, 'B-j7': 39, 'Go7': 189, 'E-j7': 119, 'Db+': 91, 'Db+7': 92, 'Gb+j7': 173, 'Gbo7': 183, 'Co': 76, 'Gsus7': 191, 'F-j7': 151, 'F+': 145, 'Dm7b5': 107, 'E': 112, 'Ab7': 19, 'Absus7': 25, 'E+7': 114, 'Bsus': 62, 'A6': 8, 'Gbsus7': 185, 'C+7': 66, 'Bbj7': 52, 'Do7': 109, 'Esus': 142, 'C-6': 69, 'Bb+7': 44, 'Am7b5': 27, 'B+': 33, 'Gsus': 190, 'Bo7': 61, 'B7': 41, 'Csus': 78, 'Bm7b5': 59, 'G7': 169, 'C-j7': 71, 'G+': 161, 'B-6': 37, 'A': 0, 'Dsus': 110, 'Aj7': 26, 'F-7': 150, 'F+j7': 147, 'A-j7': 7, 'C+': 65, 'Dbj7': 100, 'Dbsus': 104, 'D-j7': 87, 'Bb+': 43, 'Eb': 122, 'D6': 88, 'Gbsus': 184, 'Dbo': 102, 'Bb6': 50, 'Abo7': 23, 'Eb+7': 124, 'A-7': 6, 'Eb+j7': 125, 'Eb7': 131, 'Ebsus': 136, 'Fm7b5': 155, 'Eo7': 141, 'Db': 90, 'B+7': 34, 'Bb-': 46, 'Asus': 30, 'G-': 164, 'Eb-j7': 129, 'Ebo7': 135, 'C+j7': 67, 'Csus7': 79, 'Ab+7': 12, 'Bj7': 58, 'Bb-6': 47, 'Fo7': 157, 'E6': 120, 'A-6': 5, 'Gb-7': 176, 'Db-j7': 97, 'Ebsus7': 137}\n",
      "\n",
      "Integer to chord label mapping:\n",
      " {0: 'A', 1: 'A+', 2: 'A+7', 3: 'A+j7', 4: 'A-', 5: 'A-6', 6: 'A-7', 7: 'A-j7', 8: 'A6', 9: 'A7', 10: 'Ab', 11: 'Ab+', 12: 'Ab+7', 13: 'Ab+j7', 14: 'Ab-', 15: 'Ab-6', 16: 'Ab-7', 17: 'Ab-j7', 18: 'Ab6', 19: 'Ab7', 20: 'Abj7', 21: 'Abm7b5', 22: 'Abo', 23: 'Abo7', 24: 'Absus', 25: 'Absus7', 26: 'Aj7', 27: 'Am7b5', 28: 'Ao', 29: 'Ao7', 30: 'Asus', 31: 'Asus7', 32: 'B', 33: 'B+', 34: 'B+7', 35: 'B+j7', 36: 'B-', 37: 'B-6', 38: 'B-7', 39: 'B-j7', 40: 'B6', 41: 'B7', 42: 'Bb', 43: 'Bb+', 44: 'Bb+7', 45: 'Bb+j7', 46: 'Bb-', 47: 'Bb-6', 48: 'Bb-7', 49: 'Bb-j7', 50: 'Bb6', 51: 'Bb7', 52: 'Bbj7', 53: 'Bbm7b5', 54: 'Bbo', 55: 'Bbo7', 56: 'Bbsus', 57: 'Bbsus7', 58: 'Bj7', 59: 'Bm7b5', 60: 'Bo', 61: 'Bo7', 62: 'Bsus', 63: 'Bsus7', 64: 'C', 65: 'C+', 66: 'C+7', 67: 'C+j7', 68: 'C-', 69: 'C-6', 70: 'C-7', 71: 'C-j7', 72: 'C6', 73: 'C7', 74: 'Cj7', 75: 'Cm7b5', 76: 'Co', 77: 'Co7', 78: 'Csus', 79: 'Csus7', 80: 'D', 81: 'D+', 82: 'D+7', 83: 'D+j7', 84: 'D-', 85: 'D-6', 86: 'D-7', 87: 'D-j7', 88: 'D6', 89: 'D7', 90: 'Db', 91: 'Db+', 92: 'Db+7', 93: 'Db+j7', 94: 'Db-', 95: 'Db-6', 96: 'Db-7', 97: 'Db-j7', 98: 'Db6', 99: 'Db7', 100: 'Dbj7', 101: 'Dbm7b5', 102: 'Dbo', 103: 'Dbo7', 104: 'Dbsus', 105: 'Dbsus7', 106: 'Dj7', 107: 'Dm7b5', 108: 'Do', 109: 'Do7', 110: 'Dsus', 111: 'Dsus7', 112: 'E', 113: 'E+', 114: 'E+7', 115: 'E+j7', 116: 'E-', 117: 'E-6', 118: 'E-7', 119: 'E-j7', 120: 'E6', 121: 'E7', 122: 'Eb', 123: 'Eb+', 124: 'Eb+7', 125: 'Eb+j7', 126: 'Eb-', 127: 'Eb-6', 128: 'Eb-7', 129: 'Eb-j7', 130: 'Eb6', 131: 'Eb7', 132: 'Ebj7', 133: 'Ebm7b5', 134: 'Ebo', 135: 'Ebo7', 136: 'Ebsus', 137: 'Ebsus7', 138: 'Ej7', 139: 'Em7b5', 140: 'Eo', 141: 'Eo7', 142: 'Esus', 143: 'Esus7', 144: 'F', 145: 'F+', 146: 'F+7', 147: 'F+j7', 148: 'F-', 149: 'F-6', 150: 'F-7', 151: 'F-j7', 152: 'F6', 153: 'F7', 154: 'Fj7', 155: 'Fm7b5', 156: 'Fo', 157: 'Fo7', 158: 'Fsus', 159: 'Fsus7', 160: 'G', 161: 'G+', 162: 'G+7', 163: 'G+j7', 164: 'G-', 165: 'G-6', 166: 'G-7', 167: 'G-j7', 168: 'G6', 169: 'G7', 170: 'Gb', 171: 'Gb+', 172: 'Gb+7', 173: 'Gb+j7', 174: 'Gb-', 175: 'Gb-6', 176: 'Gb-7', 177: 'Gb-j7', 178: 'Gb6', 179: 'Gb7', 180: 'Gbj7', 181: 'Gbm7b5', 182: 'Gbo', 183: 'Gbo7', 184: 'Gbsus', 185: 'Gbsus7', 186: 'Gj7', 187: 'Gm7b5', 188: 'Go', 189: 'Go7', 190: 'Gsus', 191: 'Gsus7', 192: 'NC', 193: '<bos>', 194: '<eos>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create categorical data mappings\n",
    "note_to_int = dict([(c, i) for i, c in enumerate(unique_notes[1:])])\n",
    "note_to_int[-1] = len(note_to_int)\n",
    "note_to_int['<pad>'] = len(note_to_int)\n",
    "\n",
    "int_to_note = dict([(k, v) for v, k in note_to_int.items()])\n",
    "\n",
    "chord_to_int = dict([(c, i) for i, c in enumerate(unique_chords)])\n",
    "chord_to_int['<bos>'] = len(chord_to_int)\n",
    "chord_to_int['<eos>'] = len(chord_to_int)\n",
    "\n",
    "int_to_chord = dict([(k, v) for v, k in chord_to_int.items()])\n",
    "\n",
    "print(\"Melody note to integer mapping:\\n {}\\n\".format(note_to_int))\n",
    "print(\"Integer to melody note mapping:\\n {}\\n\".format(int_to_note))\n",
    "print(\"Chord label to integer mapping:\\n {}\\n\".format(chord_to_int))\n",
    "print(\"Integer to chord label mapping:\\n {}\\n\".format(int_to_chord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 334344\n",
      "Number of distinct melody notes: 14\n",
      "Number of distinct chord labels: 195\n",
      "Maximum length of melody sequences for one chord: 135\n",
      "Number of past chords given as input: 7\n"
     ]
    }
   ],
   "source": [
    "# Define numerical variables\n",
    "\n",
    "n_samples = len(chords)\n",
    "n_chords = len(chord_to_int)\n",
    "n_notes = len(note_to_int)\n",
    "max_mel_len = max([len(mel) for mel in notes_by_chords_bf_augmaj7])\n",
    "chord_context_len = 7\n",
    "\n",
    "# print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Total number of samples: {}\".format(n_samples))\n",
    "print(\"Number of distinct melody notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct chord labels: {}\".format(n_chords))\n",
    "print(\"Maximum length of melody sequences for one chord: {}\".format(max_mel_len))\n",
    "print(\"Number of past chords given as input: {}\".format(chord_context_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4], [4]]\n"
     ]
    }
   ],
   "source": [
    "mel_by_sections = [mel for section in data for ch, mel in section]\n",
    "print(mel_by_sections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Remove sections with augmented major chord---\n",
      "Number of sections: 28836\n",
      "\n",
      "Sample input melody sequence: [8, 3, 6, '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample input chord sequence: ['<bos>', '<bos>', 'E6', 'Db7', 'Gb-7', 'B7', 'E']\n",
      "\n",
      "Sample target chord: Db-7\n",
      "\n",
      "Input melody: 333480, Input chords: 333480, Target chords: 333480\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensor data\n",
    "\n",
    "\n",
    "def check_if_augmented_major(section):\n",
    "    section_chords = get_chords_by_section(section)\n",
    "    for ch in section_chords:\n",
    "        if \"+j7\" in ch:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Remove sections that involve augmented major chords (since not enough data to even allow StratifiedShuffleSplit)\n",
    "data = [section for section in data if not check_if_augmented_major(section)]\n",
    "print(\"---Remove sections with augmented major chord---\")\n",
    "print(\"Number of sections: {}\\n\".format(len(data)))\n",
    "\n",
    "chords_by_sections = [get_chords_by_section(section) for section in data]\n",
    "# chords = [chord_info[0] for section in data for chord_info in section]\n",
    "# unique_chords = sorted(list(set(chords)))\n",
    "\n",
    "notes_by_chords = [get_notes_by_chord(chord_info[1]) for section in data for chord_info in section]\n",
    "# notes = [note for chord_notes in notes_by_chords for note in chord_notes]\n",
    "# unique_notes = sorted(list(set(notes)))\n",
    "\n",
    "\n",
    "\n",
    "def pad_melody(melody, max_len):\n",
    "    return melody + (max_len-len(melody))*['<pad>']\n",
    "\n",
    "def build_input_chord_sequences(chord_seq, context_len):\n",
    "    padded_sequence = context_len*['<bos>'] + chord_seq\n",
    "    formatted_sequences = [padded_sequence[i:i+context_len+1] for i in range(len(chord_seq))]\n",
    "    return formatted_sequences\n",
    "\n",
    "# Melody\n",
    "input_melody_data = [pad_melody(melody, max_mel_len) for melody in notes_by_chords]\n",
    "print(\"Sample input melody sequence: {}\\n\".format(input_melody_data[5]))\n",
    "\n",
    "# Chords\n",
    "formatted_chords_data = []\n",
    "for section_chords in chords_by_sections:\n",
    "    formatted_chords_data += build_input_chord_sequences(section_chords, chord_context_len)\n",
    "    \n",
    "input_chords_data = [ch[:-1] for ch in formatted_chords_data]\n",
    "target_chords_data = [ch[-1] for ch in formatted_chords_data]\n",
    "print(\"Sample input chord sequence: {}\\n\".format(input_chords_data[5]))\n",
    "print(\"Sample target chord: {}\\n\".format(target_chords_data[5]))\n",
    "\n",
    "print(\"Input melody: {}, Input chords: {}, Target chords: {}\".format(len(input_melody_data), len(input_chords_data), len(target_chords_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding vectors\n",
    "\n",
    "num_dim = 8\n",
    "num_ch = 192\n",
    "num_notes = 12\n",
    "\n",
    "# Define embedding training model and load weights\n",
    "input_layer = Input(shape=(num_ch,)) \n",
    "embeddings_layer = Dense(num_dim, activation='linear', name=\"embeddings\")(input_layer)\n",
    "root_output_layer = Dense(num_notes, activation='softmax')(embeddings_layer)\n",
    "interval_output_layer = Dense(num_notes, activation='sigmoid')(embeddings_layer)\n",
    "pitch_output_layer = Dense(num_notes, activation='sigmoid')(embeddings_layer)\n",
    "melody_output_layer = Dense(num_notes, activation='relu')(embeddings_layer)\n",
    "embeddings_model = Model(input_layer, [root_output_layer, interval_output_layer, pitch_output_layer, melody_output_layer])\n",
    "\n",
    "embeddings_model.load_weights(\"../Skipgram & WJD/weights/combined_weights_dim8.h5\")\n",
    "\n",
    "X_chords_embeddings = embeddings_model.layers[1].get_weights()[0]\n",
    "print(X_chords_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embedding vector for each chord: 8\n"
     ]
    }
   ],
   "source": [
    "# Build tensors\n",
    "\n",
    "n_dimensions = X_chords_embeddings.shape[1]\n",
    "print(\"Size of embedding vector for each chord: {}\".format(n_dimensions))\n",
    "\n",
    "X_melody = np.zeros((n_samples, max_mel_len, n_notes), dtype='float32')\n",
    "X_chords = np.zeros((n_samples, chord_context_len, n_dimensions), dtype='float32')\n",
    "Y_chord = np.zeros((n_samples, n_chords), dtype='float32')\n",
    "\n",
    "for i, (input_mel, input_ch, target_ch) in enumerate(zip(input_melody_data, input_chords_data, target_chords_data)):\n",
    "    Y_chord[i, chord_to_int[target_ch]] = 1\n",
    "    for j, chord in enumerate(input_ch):\n",
    "#         X_chords[i, j, chord_to_int[chord]] = 1\n",
    "        chord_index = chord_to_int[chord]\n",
    "        if (chord_index < num_ch):\n",
    "            X_chords[i, j, :] = X_chords_embeddings[chord_index, :]\n",
    "    \n",
    "    for j, note in enumerate(input_mel):\n",
    "        X_melody[i, j, note_to_int[note]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334344, 135, 14)\n",
      "(334344, 7, 8)\n",
      "(334344, 195)\n"
     ]
    }
   ],
   "source": [
    "print(X_melody.shape)\n",
    "print(X_chords.shape)\n",
    "print(Y_chord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into 80%-10%-10% train-valid-test sets\n",
    "seed = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "\n",
    "for train_index, aux_index in sss.split(X_chords, Y_chord):\n",
    "    X_melody_train, X_melody_aux = X_melody[train_index], X_melody[aux_index]\n",
    "    X_chords_train, X_chords_aux = X_chords[train_index], X_chords[aux_index]\n",
    "    Y_chord_train, Y_chord_aux = Y_chord[train_index], Y_chord[aux_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "\n",
    "for valid_index, test_index in sss.split(X_chords_aux, Y_chord_aux):\n",
    "    X_melody_valid, X_melody_test = X_melody[valid_index], X_melody[test_index]\n",
    "    X_chords_valid, X_chords_test = X_chords[valid_index], X_chords[test_index]\n",
    "    Y_chord_valid, Y_chord_test = Y_chord[valid_index], Y_chord[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 135, 14)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 7, 8)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           54912       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 128)           52608       input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           gru_1[0][0]                      \n",
      "                                                                   gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 195)           50115       concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 157,635\n",
      "Trainable params: 157,635\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define neual net architecture\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "melody_input = Input(shape=(max_mel_len, n_notes))\n",
    "melody_gru = GRU(latent_dim)(melody_input)\n",
    "\n",
    "chords_input = Input(shape=(chord_context_len, n_dimensions))\n",
    "chords_gru = GRU(latent_dim)(chords_input)\n",
    "\n",
    "concat = concatenate([melody_gru, chords_gru])\n",
    "\n",
    "chord_dense = Dense(n_chords, activation='softmax')(concat)\n",
    "\n",
    "model = Model([melody_input, chords_input], chord_dense)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce Best-Performance callbacks\n",
    "# es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min')\n",
    "filepath = \"models/embeddings8-Mel1-Cho1-FC1_150ep.h5\"\n",
    "bp = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 267475 samples, validate on 33434 samples\n",
      "Epoch 1/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 3.0001 - acc: 0.3298Epoch 00000: val_acc improved from -inf to 0.40157, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 179s - loss: 2.9998 - acc: 0.3299 - val_loss: 2.6297 - val_acc: 0.4016\n",
      "Epoch 2/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.4218 - acc: 0.4366Epoch 00001: val_acc improved from 0.40157 to 0.47081, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 178s - loss: 2.4217 - acc: 0.4366 - val_loss: 2.2714 - val_acc: 0.4708\n",
      "Epoch 3/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 2.0879 - acc: 0.4967Epoch 00002: val_acc improved from 0.47081 to 0.51804, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 2.0878 - acc: 0.4967 - val_loss: 2.0548 - val_acc: 0.5180\n",
      "Epoch 4/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.8966 - acc: 0.5383Epoch 00003: val_acc improved from 0.51804 to 0.54774, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.8966 - acc: 0.5383 - val_loss: 1.9165 - val_acc: 0.5477\n",
      "Epoch 5/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.7610 - acc: 0.5681Epoch 00004: val_acc improved from 0.54774 to 0.57113, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.7610 - acc: 0.5680 - val_loss: 1.8250 - val_acc: 0.5711\n",
      "Epoch 6/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.6659 - acc: 0.5895Epoch 00005: val_acc improved from 0.57113 to 0.58444, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.6659 - acc: 0.5895 - val_loss: 1.7698 - val_acc: 0.5844\n",
      "Epoch 7/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.5909 - acc: 0.6079Epoch 00006: val_acc improved from 0.58444 to 0.59018, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.5907 - acc: 0.6079 - val_loss: 1.7344 - val_acc: 0.5902\n",
      "Epoch 8/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.5284 - acc: 0.6222Epoch 00007: val_acc improved from 0.59018 to 0.60178, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.5285 - acc: 0.6222 - val_loss: 1.6986 - val_acc: 0.6018\n",
      "Epoch 9/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4754 - acc: 0.6347Epoch 00008: val_acc improved from 0.60178 to 0.60513, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.4754 - acc: 0.6347 - val_loss: 1.6756 - val_acc: 0.6051\n",
      "Epoch 10/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.4281 - acc: 0.6458Epoch 00009: val_acc improved from 0.60513 to 0.61351, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.4281 - acc: 0.6457 - val_loss: 1.6517 - val_acc: 0.6135\n",
      "Epoch 11/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3874 - acc: 0.6558Epoch 00010: val_acc improved from 0.61351 to 0.61569, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.3874 - acc: 0.6558 - val_loss: 1.6333 - val_acc: 0.6157\n",
      "Epoch 12/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3522 - acc: 0.6641Epoch 00011: val_acc improved from 0.61569 to 0.62033, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.3522 - acc: 0.6641 - val_loss: 1.6175 - val_acc: 0.6203\n",
      "Epoch 13/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.3225 - acc: 0.6708Epoch 00012: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 1.3225 - acc: 0.6708 - val_loss: 1.6084 - val_acc: 0.6193\n",
      "Epoch 14/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2928 - acc: 0.6775Epoch 00013: val_acc improved from 0.62033 to 0.62876, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.2927 - acc: 0.6774 - val_loss: 1.5946 - val_acc: 0.6288\n",
      "Epoch 15/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2698 - acc: 0.6826Epoch 00014: val_acc improved from 0.62876 to 0.63065, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.2697 - acc: 0.6827 - val_loss: 1.5794 - val_acc: 0.6306\n",
      "Epoch 16/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2427 - acc: 0.6887Epoch 00015: val_acc improved from 0.63065 to 0.63486, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 1.2427 - acc: 0.6887 - val_loss: 1.5644 - val_acc: 0.6349\n",
      "Epoch 17/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2236 - acc: 0.6936Epoch 00016: val_acc improved from 0.63486 to 0.63914, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.2237 - acc: 0.6936 - val_loss: 1.5562 - val_acc: 0.6391\n",
      "Epoch 18/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.2009 - acc: 0.6981Epoch 00017: val_acc improved from 0.63914 to 0.63992, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 1.2009 - acc: 0.6982 - val_loss: 1.5469 - val_acc: 0.6399\n",
      "Epoch 19/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1835 - acc: 0.7023Epoch 00018: val_acc improved from 0.63992 to 0.64183, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 1.1835 - acc: 0.7022 - val_loss: 1.5437 - val_acc: 0.6418\n",
      "Epoch 20/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1673 - acc: 0.7061Epoch 00019: val_acc improved from 0.64183 to 0.64354, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 178s - loss: 1.1674 - acc: 0.7061 - val_loss: 1.5396 - val_acc: 0.6435\n",
      "Epoch 21/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1507 - acc: 0.7093Epoch 00020: val_acc improved from 0.64354 to 0.64620, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 1.1507 - acc: 0.7092 - val_loss: 1.5350 - val_acc: 0.6462\n",
      "Epoch 22/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1393 - acc: 0.7115Epoch 00021: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 1.1394 - acc: 0.7115 - val_loss: 1.5394 - val_acc: 0.6418\n",
      "Epoch 23/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1298 - acc: 0.7128Epoch 00022: val_acc improved from 0.64620 to 0.65209, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 1.1300 - acc: 0.7127 - val_loss: 1.5179 - val_acc: 0.6521\n",
      "Epoch 24/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1124 - acc: 0.7175Epoch 00023: val_acc improved from 0.65209 to 0.65400, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 178s - loss: 1.1124 - acc: 0.7175 - val_loss: 1.5187 - val_acc: 0.6540\n",
      "Epoch 25/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1008 - acc: 0.7197Epoch 00024: val_acc improved from 0.65400 to 0.65433, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.1008 - acc: 0.7197 - val_loss: 1.5127 - val_acc: 0.6543\n",
      "Epoch 26/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.1002 - acc: 0.7203Epoch 00025: val_acc improved from 0.65433 to 0.65466, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 174s - loss: 1.1001 - acc: 0.7203 - val_loss: 1.5158 - val_acc: 0.6547\n",
      "Epoch 27/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0864 - acc: 0.7236Epoch 00026: val_acc improved from 0.65466 to 0.65852, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 174s - loss: 1.0865 - acc: 0.7236 - val_loss: 1.5041 - val_acc: 0.6585\n",
      "Epoch 28/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.7263Epoch 00027: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 1.0734 - acc: 0.7263 - val_loss: 1.5073 - val_acc: 0.6558\n",
      "Epoch 29/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0647 - acc: 0.7281Epoch 00028: val_acc improved from 0.65852 to 0.65906, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.0647 - acc: 0.7281 - val_loss: 1.4996 - val_acc: 0.6591\n",
      "Epoch 30/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0598 - acc: 0.7292Epoch 00029: val_acc improved from 0.65906 to 0.66082, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 174s - loss: 1.0598 - acc: 0.7292 - val_loss: 1.4904 - val_acc: 0.6608\n",
      "Epoch 31/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0537 - acc: 0.7310Epoch 00030: val_acc improved from 0.66082 to 0.66229, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.0537 - acc: 0.7310 - val_loss: 1.4926 - val_acc: 0.6623\n",
      "Epoch 32/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0428 - acc: 0.7331Epoch 00031: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 1.0428 - acc: 0.7332 - val_loss: 1.4912 - val_acc: 0.6605\n",
      "Epoch 33/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0335 - acc: 0.7356Epoch 00032: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 1.0335 - acc: 0.7356 - val_loss: 1.4936 - val_acc: 0.6609\n",
      "Epoch 34/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0237 - acc: 0.7372Epoch 00033: val_acc improved from 0.66229 to 0.66298, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.0237 - acc: 0.7372 - val_loss: 1.4899 - val_acc: 0.6630\n",
      "Epoch 35/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0151 - acc: 0.7398Epoch 00034: val_acc improved from 0.66298 to 0.66549, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 1.0152 - acc: 0.7398 - val_loss: 1.4863 - val_acc: 0.6655\n",
      "Epoch 36/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0116 - acc: 0.7404Epoch 00035: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 1.0116 - acc: 0.7404 - val_loss: 1.4869 - val_acc: 0.6654\n",
      "Epoch 37/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0041 - acc: 0.7415Epoch 00036: val_acc improved from 0.66549 to 0.66917, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 1.0041 - acc: 0.7415 - val_loss: 1.4761 - val_acc: 0.6692\n",
      "Epoch 38/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.7397Epoch 00037: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 1.0079 - acc: 0.7396 - val_loss: 1.4891 - val_acc: 0.6651\n",
      "Epoch 39/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9967 - acc: 0.7423Epoch 00038: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9968 - acc: 0.7423 - val_loss: 1.4810 - val_acc: 0.6687\n",
      "Epoch 40/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9893 - acc: 0.7446Epoch 00039: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9894 - acc: 0.7446 - val_loss: 1.4826 - val_acc: 0.6678\n",
      "Epoch 41/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9839 - acc: 0.7458Epoch 00040: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9839 - acc: 0.7458 - val_loss: 1.4898 - val_acc: 0.6659\n",
      "Epoch 42/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9789 - acc: 0.7468Epoch 00041: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9788 - acc: 0.7468 - val_loss: 1.4755 - val_acc: 0.6689\n",
      "Epoch 43/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9725 - acc: 0.7484Epoch 00042: val_acc improved from 0.66917 to 0.66977, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 0.9725 - acc: 0.7483 - val_loss: 1.4792 - val_acc: 0.6698\n",
      "Epoch 44/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.7436Epoch 00043: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9885 - acc: 0.7435 - val_loss: 1.4862 - val_acc: 0.6659\n",
      "Epoch 45/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9687 - acc: 0.7490Epoch 00044: val_acc improved from 0.66977 to 0.67222, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 0.9688 - acc: 0.7490 - val_loss: 1.4743 - val_acc: 0.6722\n",
      "Epoch 46/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9579 - acc: 0.7517Epoch 00045: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9580 - acc: 0.7517 - val_loss: 1.4862 - val_acc: 0.6705\n",
      "Epoch 47/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.7530Epoch 00046: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9528 - acc: 0.7530 - val_loss: 1.4845 - val_acc: 0.6689\n",
      "Epoch 48/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.7499Epoch 00047: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9617 - acc: 0.7499 - val_loss: 1.4794 - val_acc: 0.6691\n",
      "Epoch 49/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.7468Epoch 00048: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9721 - acc: 0.7468 - val_loss: 1.4750 - val_acc: 0.6720\n",
      "Epoch 50/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9533 - acc: 0.7514Epoch 00049: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9535 - acc: 0.7514 - val_loss: 1.4859 - val_acc: 0.6705\n",
      "Epoch 51/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.7539Epoch 00050: val_acc improved from 0.67222 to 0.67288, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 0.9439 - acc: 0.7539 - val_loss: 1.4772 - val_acc: 0.6729\n",
      "Epoch 52/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7559Epoch 00051: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9354 - acc: 0.7560 - val_loss: 1.4754 - val_acc: 0.6718\n",
      "Epoch 53/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9651 - acc: 0.7479Epoch 00052: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9651 - acc: 0.7479 - val_loss: 1.4791 - val_acc: 0.6720\n",
      "Epoch 54/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9357 - acc: 0.7558Epoch 00053: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267475/267475 [==============================] - 177s - loss: 0.9357 - acc: 0.7558 - val_loss: 1.4759 - val_acc: 0.6716\n",
      "Epoch 55/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9594 - acc: 0.7500Epoch 00054: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9594 - acc: 0.7500 - val_loss: 1.5198 - val_acc: 0.6612\n",
      "Epoch 56/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9635 - acc: 0.7479Epoch 00055: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9636 - acc: 0.7479 - val_loss: 1.4857 - val_acc: 0.6720\n",
      "Epoch 57/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9369 - acc: 0.7549Epoch 00056: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9370 - acc: 0.7549 - val_loss: 1.4820 - val_acc: 0.6709\n",
      "Epoch 58/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9292 - acc: 0.7572Epoch 00057: val_acc improved from 0.67288 to 0.67339, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 0.9292 - acc: 0.7572 - val_loss: 1.4767 - val_acc: 0.6734\n",
      "Epoch 59/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9377 - acc: 0.7548Epoch 00058: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9377 - acc: 0.7548 - val_loss: 1.4691 - val_acc: 0.6725\n",
      "Epoch 60/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9289 - acc: 0.7559Epoch 00059: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9290 - acc: 0.7559 - val_loss: 1.4845 - val_acc: 0.6721\n",
      "Epoch 61/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9215 - acc: 0.7579Epoch 00060: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.9215 - acc: 0.7579 - val_loss: 1.4818 - val_acc: 0.6712\n",
      "Epoch 62/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9203 - acc: 0.7575Epoch 00061: val_acc did not improve\n",
      "267475/267475 [==============================] - 178s - loss: 0.9202 - acc: 0.7576 - val_loss: 1.4734 - val_acc: 0.6721\n",
      "Epoch 63/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9273 - acc: 0.7564Epoch 00062: val_acc improved from 0.67339 to 0.67473, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 177s - loss: 0.9275 - acc: 0.7564 - val_loss: 1.4750 - val_acc: 0.6747\n",
      "Epoch 64/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.7564Epoch 00063: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.9241 - acc: 0.7564 - val_loss: 1.4780 - val_acc: 0.6739\n",
      "Epoch 65/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9369 - acc: 0.7539Epoch 00064: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.9368 - acc: 0.7540 - val_loss: 1.4909 - val_acc: 0.6728\n",
      "Epoch 66/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9383 - acc: 0.7536Epoch 00065: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.9383 - acc: 0.7536 - val_loss: 1.5005 - val_acc: 0.6684\n",
      "Epoch 67/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9397 - acc: 0.7526Epoch 00066: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.9398 - acc: 0.7526 - val_loss: 1.4861 - val_acc: 0.6719\n",
      "Epoch 68/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9191 - acc: 0.7590Epoch 00067: val_acc improved from 0.67473 to 0.67485, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.9191 - acc: 0.7590 - val_loss: 1.4803 - val_acc: 0.6749\n",
      "Epoch 69/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9064 - acc: 0.7618Epoch 00068: val_acc improved from 0.67485 to 0.67724, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.9065 - acc: 0.7618 - val_loss: 1.4754 - val_acc: 0.6772\n",
      "Epoch 70/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8983 - acc: 0.7643Epoch 00069: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8984 - acc: 0.7643 - val_loss: 1.4754 - val_acc: 0.6752\n",
      "Epoch 71/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8992 - acc: 0.7634Epoch 00070: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8991 - acc: 0.7634 - val_loss: 1.4738 - val_acc: 0.6765\n",
      "Epoch 72/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8971 - acc: 0.7633Epoch 00071: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8970 - acc: 0.7633 - val_loss: 1.4900 - val_acc: 0.6712\n",
      "Epoch 73/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.7577Epoch 00072: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.9150 - acc: 0.7576 - val_loss: 1.4821 - val_acc: 0.6716\n",
      "Epoch 74/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9079 - acc: 0.7600Epoch 00073: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.9079 - acc: 0.7600 - val_loss: 1.4762 - val_acc: 0.6748\n",
      "Epoch 75/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8883 - acc: 0.7649Epoch 00074: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8882 - acc: 0.7650 - val_loss: 1.4780 - val_acc: 0.6750\n",
      "Epoch 76/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8936 - acc: 0.7635Epoch 00075: val_acc improved from 0.67724 to 0.68074, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 0.8936 - acc: 0.7635 - val_loss: 1.4753 - val_acc: 0.6807\n",
      "Epoch 77/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8970 - acc: 0.7621Epoch 00076: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8969 - acc: 0.7621 - val_loss: 1.4850 - val_acc: 0.6761\n",
      "Epoch 78/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7641Epoch 00077: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8924 - acc: 0.7640 - val_loss: 1.4680 - val_acc: 0.6791\n",
      "Epoch 79/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7663Epoch 00078: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8850 - acc: 0.7663 - val_loss: 1.4843 - val_acc: 0.6755\n",
      "Epoch 80/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.7670Epoch 00079: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8810 - acc: 0.7670 - val_loss: 1.4734 - val_acc: 0.6806\n",
      "Epoch 81/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8857 - acc: 0.7656Epoch 00080: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8856 - acc: 0.7656 - val_loss: 1.4766 - val_acc: 0.6780\n",
      "Epoch 82/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8808 - acc: 0.7662Epoch 00081: val_acc improved from 0.68074 to 0.68110, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 0.8809 - acc: 0.7662 - val_loss: 1.4630 - val_acc: 0.6811\n",
      "Epoch 83/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7671Epoch 00082: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8783 - acc: 0.7672 - val_loss: 1.4864 - val_acc: 0.6763\n",
      "Epoch 84/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8737 - acc: 0.7684Epoch 00083: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8738 - acc: 0.7683 - val_loss: 1.4856 - val_acc: 0.6768\n",
      "Epoch 85/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.7661Epoch 00084: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8793 - acc: 0.7661 - val_loss: 1.4773 - val_acc: 0.6779\n",
      "Epoch 86/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.7649Epoch 00085: val_acc improved from 0.68110 to 0.68296, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 0.8827 - acc: 0.7650 - val_loss: 1.4699 - val_acc: 0.6830\n",
      "Epoch 87/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8754 - acc: 0.7668Epoch 00086: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8754 - acc: 0.7668 - val_loss: 1.4810 - val_acc: 0.6762\n",
      "Epoch 88/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8668 - acc: 0.7697Epoch 00087: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8670 - acc: 0.7697 - val_loss: 1.4688 - val_acc: 0.6800\n",
      "Epoch 89/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7697Epoch 00088: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8660 - acc: 0.7696 - val_loss: 1.5294 - val_acc: 0.6677\n",
      "Epoch 90/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.9041 - acc: 0.7590Epoch 00089: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.9040 - acc: 0.7590 - val_loss: 1.4890 - val_acc: 0.6752\n",
      "Epoch 91/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8722 - acc: 0.7681Epoch 00090: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8722 - acc: 0.7681 - val_loss: 1.4710 - val_acc: 0.6804\n",
      "Epoch 92/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7704Epoch 00091: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8640 - acc: 0.7703 - val_loss: 1.4809 - val_acc: 0.6784\n",
      "Epoch 93/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8733 - acc: 0.7676Epoch 00092: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8734 - acc: 0.7675 - val_loss: 1.5028 - val_acc: 0.6734\n",
      "Epoch 94/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8756 - acc: 0.7663Epoch 00093: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8755 - acc: 0.7663 - val_loss: 1.4766 - val_acc: 0.6808\n",
      "Epoch 95/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8816 - acc: 0.7644Epoch 00094: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8815 - acc: 0.7644 - val_loss: 1.5048 - val_acc: 0.6724\n",
      "Epoch 96/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7670Epoch 00095: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8738 - acc: 0.7670 - val_loss: 1.4826 - val_acc: 0.6781\n",
      "Epoch 97/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8856 - acc: 0.7630Epoch 00096: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8855 - acc: 0.7631 - val_loss: 1.4786 - val_acc: 0.6788\n",
      "Epoch 98/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8636 - acc: 0.7692Epoch 00097: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8636 - acc: 0.7693 - val_loss: 1.4857 - val_acc: 0.6769\n",
      "Epoch 99/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7647Epoch 00098: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8814 - acc: 0.7647 - val_loss: 1.4868 - val_acc: 0.6798\n",
      "Epoch 100/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8620 - acc: 0.7691Epoch 00099: val_acc did not improve\n",
      "267475/267475 [==============================] - 177s - loss: 0.8620 - acc: 0.7691 - val_loss: 1.4814 - val_acc: 0.6775\n",
      "Epoch 101/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8700 - acc: 0.7674Epoch 00100: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8700 - acc: 0.7674 - val_loss: 1.4990 - val_acc: 0.6748\n",
      "Epoch 102/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8777 - acc: 0.7647Epoch 00101: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8778 - acc: 0.7647 - val_loss: 1.4857 - val_acc: 0.6789\n",
      "Epoch 103/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8590 - acc: 0.7708Epoch 00102: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8590 - acc: 0.7707 - val_loss: 1.4846 - val_acc: 0.6786\n",
      "Epoch 104/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8543 - acc: 0.7715Epoch 00103: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8543 - acc: 0.7715 - val_loss: 1.4846 - val_acc: 0.6810\n",
      "Epoch 105/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7693Epoch 00104: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8606 - acc: 0.7692 - val_loss: 1.4773 - val_acc: 0.6810\n",
      "Epoch 106/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8566 - acc: 0.7706Epoch 00105: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8565 - acc: 0.7706 - val_loss: 1.5078 - val_acc: 0.6743\n",
      "Epoch 107/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8869 - acc: 0.7623Epoch 00106: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8867 - acc: 0.7624 - val_loss: 1.4959 - val_acc: 0.6768\n",
      "Epoch 108/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8593 - acc: 0.7708Epoch 00107: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8592 - acc: 0.7708 - val_loss: 1.4772 - val_acc: 0.6821\n",
      "Epoch 109/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8541 - acc: 0.7716Epoch 00108: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8541 - acc: 0.7716 - val_loss: 1.4884 - val_acc: 0.6786\n",
      "Epoch 110/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8503 - acc: 0.7728Epoch 00109: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8504 - acc: 0.7727 - val_loss: 1.5045 - val_acc: 0.6768\n",
      "Epoch 111/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8610 - acc: 0.7685Epoch 00110: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8609 - acc: 0.7686 - val_loss: 1.4833 - val_acc: 0.6793\n",
      "Epoch 112/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8394 - acc: 0.7749Epoch 00111: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8394 - acc: 0.7749 - val_loss: 1.4872 - val_acc: 0.6806\n",
      "Epoch 113/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8616 - acc: 0.7676Epoch 00112: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8615 - acc: 0.7676 - val_loss: 1.4956 - val_acc: 0.6793\n",
      "Epoch 114/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7688Epoch 00113: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8596 - acc: 0.7688 - val_loss: 1.4864 - val_acc: 0.6809\n",
      "Epoch 115/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7697Epoch 00114: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8584 - acc: 0.7697 - val_loss: 1.4939 - val_acc: 0.6771\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8623 - acc: 0.7676Epoch 00115: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8622 - acc: 0.7677 - val_loss: 1.5244 - val_acc: 0.6731\n",
      "Epoch 117/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7628Epoch 00116: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8809 - acc: 0.7629 - val_loss: 1.5078 - val_acc: 0.6732\n",
      "Epoch 118/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.7664Epoch 00117: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8703 - acc: 0.7664 - val_loss: 1.4931 - val_acc: 0.6795\n",
      "Epoch 119/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8519 - acc: 0.7715Epoch 00118: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8519 - acc: 0.7715 - val_loss: 1.4869 - val_acc: 0.6808\n",
      "Epoch 120/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.7721Epoch 00119: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8460 - acc: 0.7722 - val_loss: 1.4815 - val_acc: 0.6806\n",
      "Epoch 121/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7740Epoch 00120: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8414 - acc: 0.7740 - val_loss: 1.4929 - val_acc: 0.6807\n",
      "Epoch 122/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8556 - acc: 0.7696Epoch 00121: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8556 - acc: 0.7696 - val_loss: 1.4890 - val_acc: 0.6822\n",
      "Epoch 123/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.7648Epoch 00122: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8721 - acc: 0.7648 - val_loss: 1.4997 - val_acc: 0.6764\n",
      "Epoch 124/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8630 - acc: 0.7684Epoch 00123: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8630 - acc: 0.7685 - val_loss: 1.4843 - val_acc: 0.6824\n",
      "Epoch 125/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8433 - acc: 0.7741Epoch 00124: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8434 - acc: 0.7741 - val_loss: 1.4873 - val_acc: 0.6795\n",
      "Epoch 126/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8368 - acc: 0.7749Epoch 00125: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8368 - acc: 0.7749 - val_loss: 1.4778 - val_acc: 0.6811\n",
      "Epoch 127/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8388 - acc: 0.7742Epoch 00126: val_acc improved from 0.68296 to 0.68353, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.8388 - acc: 0.7743 - val_loss: 1.4880 - val_acc: 0.6835\n",
      "Epoch 128/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8438 - acc: 0.7725Epoch 00127: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8439 - acc: 0.7725 - val_loss: 1.5202 - val_acc: 0.6732\n",
      "Epoch 129/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8583 - acc: 0.7682Epoch 00128: val_acc improved from 0.68353 to 0.68385, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 176s - loss: 0.8582 - acc: 0.7682 - val_loss: 1.4818 - val_acc: 0.6839\n",
      "Epoch 130/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8407 - acc: 0.7733Epoch 00129: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8406 - acc: 0.7733 - val_loss: 1.4859 - val_acc: 0.6804\n",
      "Epoch 131/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7737Epoch 00130: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8397 - acc: 0.7738 - val_loss: 1.4812 - val_acc: 0.6838\n",
      "Epoch 132/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8378 - acc: 0.7746Epoch 00131: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8380 - acc: 0.7746 - val_loss: 1.4994 - val_acc: 0.6803\n",
      "Epoch 133/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8366 - acc: 0.7747Epoch 00132: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8366 - acc: 0.7746 - val_loss: 1.4834 - val_acc: 0.6834\n",
      "Epoch 134/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8321 - acc: 0.7755Epoch 00133: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8322 - acc: 0.7755 - val_loss: 1.4886 - val_acc: 0.6835\n",
      "Epoch 135/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8345 - acc: 0.7752Epoch 00134: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8347 - acc: 0.7752 - val_loss: 1.4861 - val_acc: 0.6830\n",
      "Epoch 136/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8619 - acc: 0.7675Epoch 00135: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8618 - acc: 0.7675 - val_loss: 1.5134 - val_acc: 0.6769\n",
      "Epoch 137/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8671 - acc: 0.7651Epoch 00136: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8671 - acc: 0.7652 - val_loss: 1.5006 - val_acc: 0.6789\n",
      "Epoch 138/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8528 - acc: 0.7700Epoch 00137: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8528 - acc: 0.7700 - val_loss: 1.4940 - val_acc: 0.6792\n",
      "Epoch 139/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8428 - acc: 0.7727Epoch 00138: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8427 - acc: 0.7727 - val_loss: 1.4891 - val_acc: 0.6798\n",
      "Epoch 140/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8402 - acc: 0.7738Epoch 00139: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8401 - acc: 0.7738 - val_loss: 1.5006 - val_acc: 0.6791\n",
      "Epoch 141/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8382 - acc: 0.7739Epoch 00140: val_acc did not improve\n",
      "267475/267475 [==============================] - 176s - loss: 0.8383 - acc: 0.7739 - val_loss: 1.4834 - val_acc: 0.6833\n",
      "Epoch 142/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8307 - acc: 0.7751Epoch 00141: val_acc improved from 0.68385 to 0.68448, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.8307 - acc: 0.7751 - val_loss: 1.4849 - val_acc: 0.6845\n",
      "Epoch 143/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8253 - acc: 0.7770Epoch 00142: val_acc did not improve\n",
      "267475/267475 [==============================] - 174s - loss: 0.8252 - acc: 0.7770 - val_loss: 1.4887 - val_acc: 0.6834\n",
      "Epoch 144/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8217 - acc: 0.7781Epoch 00143: val_acc improved from 0.68448 to 0.68463, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.8217 - acc: 0.7780 - val_loss: 1.4832 - val_acc: 0.6846\n",
      "Epoch 145/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8222 - acc: 0.7778Epoch 00144: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8222 - acc: 0.7778 - val_loss: 1.4840 - val_acc: 0.6830\n",
      "Epoch 146/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7753Epoch 00145: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8301 - acc: 0.7753 - val_loss: 1.4803 - val_acc: 0.6824\n",
      "Epoch 147/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8182 - acc: 0.7790Epoch 00146: val_acc improved from 0.68463 to 0.68643, saving model to models/embeddings8-Mel1-Cho1-FC1_150ep.h5\n",
      "267475/267475 [==============================] - 175s - loss: 0.8181 - acc: 0.7790 - val_loss: 1.4777 - val_acc: 0.6864\n",
      "Epoch 148/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8184 - acc: 0.7788Epoch 00147: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8184 - acc: 0.7788 - val_loss: 1.4768 - val_acc: 0.6858\n",
      "Epoch 149/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7749Epoch 00148: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8314 - acc: 0.7750 - val_loss: 1.4923 - val_acc: 0.6809\n",
      "Epoch 150/150\n",
      "267264/267475 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7757Epoch 00149: val_acc did not improve\n",
      "267475/267475 [==============================] - 175s - loss: 0.8287 - acc: 0.7756 - val_loss: 1.4852 - val_acc: 0.6860\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit([X_melody_train, X_chords_train], Y_chord_train, epochs=epochs, validation_data=([X_melody_valid, X_chords_valid], Y_chord_valid), batch_size=batch_size, callbacks=[bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe06c1f4b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFkCAYAAADWs8tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoiIiIiIiIjEPTUwRERERERERCTu/Q+p+/CwqN7i3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe06c1ee320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model learning\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Training history of root model\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best performance model\n",
    "best_model = load_model(\"models/embeddings8-Mel1-Cho1-FC1_150ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33435, 135, 14)\n",
      "(33435, 7, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_melody_test.shape)\n",
    "print(X_chords_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord prediction: 0.6847\n",
      "Kappa score of combined chord prediction: 0.6767\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of labels\n",
    "\n",
    "# Predict chords from each test sample melody\n",
    "Y_chord_pred = model.predict([X_melody_test, X_chords_test])\n",
    "\n",
    "# Compute accuracy and kappa score \n",
    "print(\"Categorical accuracy of combined chord prediction: {0:.4f}\".format(harmoutil.compute_accuracy_score(Y_chord_test, Y_chord_pred)))\n",
    "print(\"Kappa score of combined chord prediction: {0:.4f}\".format(harmoutil.compute_kappa_score(Y_chord_test, Y_chord_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy of combined chord pitch prediction: 0.8878\n",
      "TP: 108083 TN: 248130 FP: 22637 FN: 22370\n",
      "Kappa score of combined chord pitch prediction: 0.7445\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions in terms of pitches\n",
    "\n",
    "def label_to_pitch_tensors(predictions):\n",
    "    predicted_chords = [int_to_chord[np.argmax(ch)] for ch in predictions]\n",
    "    pitch_chords = [harmoutil.chord_to_notes(label) for label in predicted_chords]\n",
    "    \n",
    "    Y_pitches = np.zeros((predictions.shape[0], 12), dtype='float32')\n",
    "    for i, chord_pitches in enumerate(pitch_chords):\n",
    "        for j, pitch_presence in enumerate(chord_pitches):\n",
    "            Y_pitches[i, j] = pitch_presence\n",
    "\n",
    "    return Y_pitches\n",
    "\n",
    "Y_pitch_pred = label_to_pitch_tensors(Y_chord_pred)\n",
    "Y_pitch_test = label_to_pitch_tensors(Y_chord_test)\n",
    "\n",
    "print(\"Categorical accuracy of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_accuracy_score(Y_pitch_test, Y_pitch_pred)))\n",
    "print(\"Kappa score of combined chord pitch prediction: {0:.4f}\".format(harmoutil.compute_multiclass_binary_kappa_score(Y_pitch_test, Y_pitch_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
